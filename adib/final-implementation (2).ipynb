{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6491929,"sourceType":"datasetVersion","datasetId":3321433}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install boruta category_encoders xgboost catboost\n!pip uninstall -y scikit-learn imbalanced-learn\n\n!pip install scikit-learn==1.4.2 imbalanced-learn==0.12.0\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, RobustScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV, RandomizedSearchCV\nfrom sklearn.feature_selection import mutual_info_classif, SelectKBest\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler, SMOTENC\nfrom imblearn.combine import SMOTETomek, SMOTEENN\nfrom imblearn.under_sampling import CondensedNearestNeighbour, TomekLinks, RandomUnderSampler\nfrom boruta import BorutaPy\nfrom keras.models import Model, Sequential\nfrom keras.layers import Input, Dense\nfrom keras.optimizers import Adam\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE, BorderlineSMOTE, SVMSMOTE, ADASYN\nfrom imblearn.under_sampling import RandomUnderSampler, ClusterCentroids, NearMiss\nfrom imblearn.under_sampling import TomekLinks\n\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE, BorderlineSMOTE, SVMSMOTE, ADASYN\nfrom imblearn.under_sampling import RandomUnderSampler, ClusterCentroids, NearMiss, TomekLinks\nfrom imblearn.combine import SMOTETomek, SMOTEENN","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:05:41.995247Z","iopub.execute_input":"2025-10-06T14:05:41.995616Z","iopub.status.idle":"2025-10-06T14:05:54.540640Z","shell.execute_reply.started":"2025-10-06T14:05:41.995593Z","shell.execute_reply":"2025-10-06T14:05:54.539374Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: boruta in /usr/local/lib/python3.11/dist-packages (0.4.3)\nRequirement already satisfied: category_encoders in /usr/local/lib/python3.11/dist-packages (2.7.0)\nRequirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.0.3)\nRequirement already satisfied: catboost in /usr/local/lib/python3.11/dist-packages (1.2.8)\nRequirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.11/dist-packages (from boruta) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.11/dist-packages (from boruta) (1.4.2)\nRequirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.11/dist-packages (from boruta) (1.15.3)\nRequirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (2.2.3)\nRequirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.0.1)\nRequirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (0.14.4)\nRequirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.21)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.7.2)\nRequirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10.4->boruta) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10.4->boruta) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10.4->boruta) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10.4->boruta) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10.4->boruta) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10.4->boruta) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.17.1->boruta) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.17.1->boruta) (3.6.0)\nRequirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.9.0->category_encoders) (25.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.2.1)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.0.9)\nRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (8.5.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.10.4->boruta) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.10.4->boruta) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.10.4->boruta) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.10.4->boruta) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.10.4->boruta) (2024.2.0)\nFound existing installation: scikit-learn 1.4.2\nUninstalling scikit-learn-1.4.2:\n  Successfully uninstalled scikit-learn-1.4.2\nFound existing installation: imbalanced-learn 0.12.0\nUninstalling imbalanced-learn-0.12.0:\n  Successfully uninstalled imbalanced-learn-0.12.0\nCollecting scikit-learn==1.4.2\n  Using cached scikit_learn-1.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nCollecting imbalanced-learn==0.12.0\n  Using cached imbalanced_learn-0.12.0-py3-none-any.whl.metadata (8.2 kB)\nRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2) (1.26.4)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2) (1.15.3)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.4.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.4.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.4.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.4.2) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.4.2) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.4.2) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.5->scikit-learn==1.4.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.5->scikit-learn==1.4.2) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.5->scikit-learn==1.4.2) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.5->scikit-learn==1.4.2) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.5->scikit-learn==1.4.2) (2024.2.0)\nUsing cached scikit_learn-1.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\nUsing cached imbalanced_learn-0.12.0-py3-none-any.whl (257 kB)\nInstalling collected packages: scikit-learn, imbalanced-learn\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed imbalanced-learn-0.12.0 scikit-learn-1.4.2\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# ==============================\n# Combined Feature Engineering Pipeline\n# ==============================\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n# ------------------------------\n# Step 1: Load dataset\n# ------------------------------\ndf = pd.read_csv(\"/kaggle/input/sleep-health-and-lifestyle-dataset/Sleep_health_and_lifestyle_dataset.csv\")\ndf.fillna(\"None\", inplace=True)\n\n\n\n\n\n# ------------------------------\n# Step 2: Basic preprocessing\n# ------------------------------\n# Split Blood Pressure\ndf[['Systolic BP', 'Diastolic BP']] = df['Blood Pressure'].str.split('/', expand=True).astype(int)\ndf.drop(['Person ID', 'Blood Pressure'], axis=1, inplace=True)\n\n# Group rare occupations\ndf['Occupation'] = df['Occupation'].replace(['Manager', 'Sales Representative', 'Scientist', 'Software Engineer'], 'Other')\n\n# Map BMI categories to numeric\ndf['BMI Category'] = df['BMI Category'].replace({'Normal': 22, 'Normal Weight': 22, 'Overweight': 27, 'Obese': 30})\n\n# Label encode categorical columns\nlabel_encoder = LabelEncoder()\nfor col in ['Gender', 'Sleep Disorder']:\n    df[col] = label_encoder.fit_transform(df[col])\n\n# One-hot encode Occupation\ndf = pd.get_dummies(df, columns=['Occupation'], drop_first=False)\n\n# ------------------------------\n# Step 3: Feature Engineering\n# ------------------------------\nepsilon = 1e-6\n\n# ---- Ratio & difference features ----\ndf['Sleep_Heart_ratio'] = df['Sleep Duration'] / (df['Heart Rate'] + epsilon)\ndf['Sleep_Steps_ratio'] = df['Sleep Duration'] / (df['Daily Steps'] + epsilon)\ndf['Stress_Activity_ratio'] = df['Stress Level'] / (df['Physical Activity Level'] + epsilon)\ndf['Heart_Sleep_diff'] = df['Heart Rate'] - df['Sleep Duration']\n\n\n\n# ---- Aggregation features ----\ndf['Total_Activity'] = df['Physical Activity Level'] + df['Daily Steps']\ndf['Stress_per_Activity'] = df['Stress Level'] / (df['Total_Activity'] + epsilon)\n\n# ---- Interaction features ----\ndf['Sleep_Stress_interaction'] = df['Sleep Duration'] * df['Stress Level']\ndf['BMI_Activity_interaction'] = df['BMI Category'] * df['Physical Activity Level']\n\n# ---- Categorical interaction features ----\ndf['Gender_Occupation'] = df['Gender'].astype(str) + \"_\" + df['Occupation_Other'].astype(str)\ndf = pd.get_dummies(df, columns=['Gender_Occupation'], drop_first=False)\n\n\n\ndf = pd.read_csv(\"/kaggle/input/sleep-health-and-lifestyle-dataset/Sleep_health_and_lifestyle_dataset.csv\")\ndf.fillna(\"None\", inplace=True)\n\n# Dividing Blood Pressure into Systolic and Diastolic BP\ndf[['Systolic BP', 'Diastolic BP']] = df['Blood Pressure'].str.split('/', expand=True).astype(int)\ndf.drop(['Person ID', 'Blood Pressure'], axis=1, inplace=True)\n\n# Labeling less number of careers as other\ndf['Occupation'] = df['Occupation'].replace(['Manager', 'Sales Representative', 'Scientist', 'Software Engineer'], 'Other')\n\n# Adding the average BMI for the range\ndf['BMI Category'] = df['BMI Category'].replace({'Normal':22, 'Normal Weight':22, 'Overweight':27, 'Obese':30})\n\n# Creating Interaction features\ndf['Stress_sleep_interaction'] = df['Stress Level'] / df['Quality of Sleep']\ndf['BMI_Activity'] = df['BMI Category'] * df['Physical Activity Level']\ndf['Sleep_Heart_ratio'] = df['Sleep Duration'] / df['Heart Rate']\ndf['Sleep_Steps_ratio'] = df['Sleep Duration'] / df['Daily Steps']\ndf['Sleep_Stress_ratio'] = df['Sleep Duration'] / df['Stress Level']\n\ndf = pd.get_dummies(df, columns=['Occupation'], drop_first=False)\n\nlabel_encoder = LabelEncoder()\ncolumns = ['Gender', 'Sleep Disorder']\nfor col in columns:\n  df[col] = label_encoder.fit_transform(df[col])\n\n\n'''\n\n\nnum_col = ['Age', 'Sleep Duration', 'Quality of Sleep', 'Physical Activity Level', 'Stress Level', 'Stress_sleep_interaction',\n          'Sleep_Heart_ratio', 'Sleep_Steps_ratio', 'Sleep_Stress_ratio', 'Heart Rate', 'Daily Steps',\n           'Systolic BP', 'Diastolic BP']\n\nQ1 = df[num_col].quantile(0.25)\nQ3 = df[num_col].quantile(0.75)\nIQR = Q3 - Q1\n# getting rid of the outliers\ndf = df[~((df[num_col] < (Q1 - 1.4 * IQR)) | (df[num_col] > (Q3 + 1.4 * IQR))).any(axis=1)]\n\nprint(df.shape)\n\n\n'''\n# ------------------------------\n# Step 4: Prepare features & target\n# ------------------------------\nX = df.drop('Sleep Disorder', axis=1)\ny = df['Sleep Disorder']\n\n# ------------------------------\n# Step 5: Train-test split\n# ------------------------------\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n'''\n# Get only the numeric columns\nnum_col = X_train.select_dtypes(include='number').columns\n\nQ1 = X_train[num_col].quantile(0.25)\nQ3 = X_train[num_col].quantile(0.75)\nIQR = Q3 - Q1\n\n# Remove outliers from X_train\nX_train = X_train[~((X_train[num_col] < (Q1 - 3 * IQR)) | (X_train[num_col] > (Q3 + 3* IQR))).any(axis=1)]\ny_train = y_train[X_train.index]\n'''\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:05:54.542716Z","iopub.execute_input":"2025-10-06T14:05:54.543031Z","iopub.status.idle":"2025-10-06T14:05:54.610451Z","shell.execute_reply.started":"2025-10-06T14:05:54.543005Z","shell.execute_reply":"2025-10-06T14:05:54.609653Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"\"\\n# Get only the numeric columns\\nnum_col = X_train.select_dtypes(include='number').columns\\n\\nQ1 = X_train[num_col].quantile(0.25)\\nQ3 = X_train[num_col].quantile(0.75)\\nIQR = Q3 - Q1\\n\\n# Remove outliers from X_train\\nX_train = X_train[~((X_train[num_col] < (Q1 - 3 * IQR)) | (X_train[num_col] > (Q3 + 3* IQR))).any(axis=1)]\\ny_train = y_train[X_train.index]\\n\""},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"'''\n\nimport numpy as np\nimport pandas as pd\n\n# Get only numeric columns\nnum_col = X_train.select_dtypes(include='number').columns\n\n# Compute IQR\nQ1 = X_train[num_col].quantile(0.25)\nQ3 = X_train[num_col].quantile(0.75)\nIQR = Q3 - Q1\n\n# Define bounds for outliers (e.g., 3*IQR)\nupper_bound = Q3 + 3 * IQR\nlower_bound = Q1 - 3 * IQR\n\n# Sample some rows (you can also target specific values/classes if needed)\nn_outliers_to_add = 100  # or int(0.05 * len(X_train)) for 5% more\nsampled_rows = X_train.sample(n=n_outliers_to_add, random_state=42)\n\n# Apply noise to numeric columns to push them into outlier space\noutliers = sampled_rows.copy()\nfor col in num_col:\n    direction = np.random.choice([-1, 1], size=n_outliers_to_add)\n    magnitude = np.random.uniform(3, 5, size=n_outliers_to_add)  # stronger than 3*IQR\n    outliers[col] += direction * magnitude * IQR[col]\n\n# Append to original data\nX_train_augmented = pd.concat([X_train, outliers], axis=0)\ny_train_augmented = pd.concat([y_train, y_train.loc[sampled_rows.index]], axis=0)\n\n\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:05:54.611494Z","iopub.execute_input":"2025-10-06T14:05:54.611831Z","iopub.status.idle":"2025-10-06T14:05:54.618140Z","shell.execute_reply.started":"2025-10-06T14:05:54.611810Z","shell.execute_reply":"2025-10-06T14:05:54.617361Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"\"\\n\\nimport numpy as np\\nimport pandas as pd\\n\\n# Get only numeric columns\\nnum_col = X_train.select_dtypes(include='number').columns\\n\\n# Compute IQR\\nQ1 = X_train[num_col].quantile(0.25)\\nQ3 = X_train[num_col].quantile(0.75)\\nIQR = Q3 - Q1\\n\\n# Define bounds for outliers (e.g., 3*IQR)\\nupper_bound = Q3 + 3 * IQR\\nlower_bound = Q1 - 3 * IQR\\n\\n# Sample some rows (you can also target specific values/classes if needed)\\nn_outliers_to_add = 100  # or int(0.05 * len(X_train)) for 5% more\\nsampled_rows = X_train.sample(n=n_outliers_to_add, random_state=42)\\n\\n# Apply noise to numeric columns to push them into outlier space\\noutliers = sampled_rows.copy()\\nfor col in num_col:\\n    direction = np.random.choice([-1, 1], size=n_outliers_to_add)\\n    magnitude = np.random.uniform(3, 5, size=n_outliers_to_add)  # stronger than 3*IQR\\n    outliers[col] += direction * magnitude * IQR[col]\\n\\n# Append to original data\\nX_train_augmented = pd.concat([X_train, outliers], axis=0)\\ny_train_augmented = pd.concat([y_train, y_train.loc[sampled_rows.index]], axis=0)\\n\\n\\n\""},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:05:54.620286Z","iopub.execute_input":"2025-10-06T14:05:54.620533Z","iopub.status.idle":"2025-10-06T14:05:54.636242Z","shell.execute_reply.started":"2025-10-06T14:05:54.620514Z","shell.execute_reply":"2025-10-06T14:05:54.635287Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"(374, 25)"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"# -----------------------------\n# RandomForest classifier (for Boruta feature selection)\n# -----------------------------\nscaler = MinMaxScaler()\nrfc = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Normalize data\nX_train_normalized = scaler.fit_transform(X_train)\nX_test_normalized = scaler.transform(X_test)\n\n# -----------------------------\n# Boruta Feature Selection\n# -----------------------------\nboruta_selector = BorutaPy(rfc, n_estimators='auto', verbose=0, random_state=42)\n\nX_train_boruta = boruta_selector.fit_transform(X_train_normalized, y_train)\nX_test_boruta = boruta_selector.transform(X_test_normalized)\n\n# -----------------------------\n# Autoencoder Architecture\n# -----------------------------\nn_features = X_train_boruta.shape[1]\ninput_layer = Input(shape=(n_features,))\n\n# Encoder\nencoded = Dense(32, activation='relu')(input_layer)\nbottleneck = Dense(16, activation='relu')(encoded)\n\n# Decoder\ndecoded = Dense(32, activation='relu')(bottleneck)\noutput_layer = Dense(n_features, activation='sigmoid')(decoded)\n\n# Full autoencoder model\nautoencoder = Model(inputs=input_layer, outputs=output_layer)\nautoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n\n# Train the autoencoder\nautoencoder.fit(X_train_boruta, X_train_boruta, epochs=10, batch_size=32, verbose=0)\n\n# Encoder model to extract bottleneck features\nencoder = Model(inputs=input_layer, outputs=bottleneck)\n\n# Transform data\nX_train_encoded = encoder.predict(X_train_boruta)\nX_test_encoded = encoder.predict(X_test_boruta)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:05:54.637144Z","iopub.execute_input":"2025-10-06T14:05:54.637406Z","iopub.status.idle":"2025-10-06T14:06:05.800544Z","shell.execute_reply.started":"2025-10-06T14:05:54.637378Z","shell.execute_reply":"2025-10-06T14:06:05.799822Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"scaler = RobustScaler()\nX_train_robust = scaler.fit_transform(X_train)\nX_test_robust = scaler.transform(X_test)\n\n# Feature selection using Mutual Information\nmi = SelectKBest(score_func=mutual_info_classif, k=5)\nX_train_mi = mi.fit_transform(X_train_robust, y_train)\nX_test_mi = mi.transform(X_test_robust)\n\n# Applying Linear Discriminant Analysis (LDA)\nlda = LinearDiscriminantAnalysis(n_components=2)\nX_train_lda = lda.fit_transform(X_train_mi, y_train)\nX_test_lda = lda.transform(X_test_mi)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:06:05.801807Z","iopub.execute_input":"2025-10-06T14:06:05.802086Z","iopub.status.idle":"2025-10-06T14:06:05.900902Z","shell.execute_reply.started":"2025-10-06T14:06:05.802053Z","shell.execute_reply":"2025-10-06T14:06:05.899960Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"from sklearn.preprocessing import PowerTransformer\nscaler = PowerTransformer(method='yeo-johnson')\nX_train_power = scaler.fit_transform(X_train)\nX_test_power = scaler.transform(X_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:06:05.901964Z","iopub.execute_input":"2025-10-06T14:06:05.902261Z","iopub.status.idle":"2025-10-06T14:06:05.978788Z","shell.execute_reply.started":"2025-10-06T14:06:05.902230Z","shell.execute_reply":"2025-10-06T14:06:05.977888Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"Boruta + SMOTE + Tomek\n\n\nPowerTransformer + SMOTE\n\n\nMI + SMOTE + Tomek\n\n\nLDA\n\n\nMI\n","metadata":{}},{"cell_type":"code","source":"\n# Boruta + SMOTE+Tomek\n\nsmotetomek_boruta = SMOTETomek(random_state=42)\nX_train_boruta_smotetomek, y_train_boruta_smotetomek = smotetomek_boruta.fit_resample(X_train_boruta, y_train)\nX_test_boruta_smotetomek = X_test_boruta.copy()\n\n\n# PowerTransformer + SMOTE\nsmote_power = SMOTE(random_state=42, k_neighbors=5)\nX_train_power_smote, y_train_power_smote = smote_power.fit_resample(X_train_power, y_train)\nX_test_power_smote = X_test_power.copy()\n\n\n# MI + SMOTE+Tomek\nsmotetomek_mi = SMOTETomek(random_state=42)\nX_train_mi_smotetomek, y_train_mi_smotetomek = smotetomek_mi.fit_resample(X_train_mi, y_train)\nX_test_mi_smotetomek = X_test_mi.copy()\n\n\n\n# MI\nmi = SelectKBest(score_func=mutual_info_classif, k=5)\nX_train_mi = mi.fit_transform(X_train_robust, y_train)\nX_test_mi = mi.transform(X_test_robust)\n\n# LDA\nlda = LinearDiscriminantAnalysis(n_components=2)\nX_train_lda = lda.fit_transform(X_train_mi, y_train)\nX_test_lda = lda.transform(X_test_mi)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:06:05.979789Z","iopub.execute_input":"2025-10-06T14:06:05.980837Z","iopub.status.idle":"2025-10-06T14:06:06.133372Z","shell.execute_reply.started":"2025-10-06T14:06:05.980812Z","shell.execute_reply":"2025-10-06T14:06:06.132550Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"# training individual model","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import (AdaBoostClassifier, RandomForestClassifier, \n                               ExtraTreesClassifier, GradientBoostingClassifier)\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import (accuracy_score, precision_score, recall_score, \n                              f1_score, roc_auc_score)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PowerTransformer, RobustScaler\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.over_sampling import SMOTE\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Assuming you have X_train, X_test, y_train, y_test already loaded\n# You need to have the base preprocessing done before running configurations\n\n# ==============================================================================\n# STEP 1: PREPARE ALL 5 CONFIGURATIONS\n# ==============================================================================\n\nprint(\"Preparing all configurations...\")\n\n# Assuming you have X_train_robust and X_test_robust ready (with RobustScaler applied)\n# If not, create them first:\n# scaler = RobustScaler()\n# X_train_robust = scaler.fit_transform(X_train)\n# X_test_robust = scaler.transform(X_test)\n\n# Configuration 1: Boruta + SMOTE+Tomek\n# Assuming X_train_boruta and X_test_boruta are already prepared from Boruta selection\nsmotetomek_boruta = SMOTETomek(random_state=42)\nX_train_boruta_smotetomek, y_train_boruta_smotetomek = smotetomek_boruta.fit_resample(X_train_boruta, y_train)\nX_test_boruta_smotetomek = X_test_boruta.copy()\ny_test_boruta_smotetomek = y_test.copy()\n\n# Configuration 2: PowerTransformer + SMOTE\n# Assuming X_train_power and X_test_power are already prepared\nsmote_power = SMOTE(random_state=42, k_neighbors=5)\nX_train_power_smote, y_train_power_smote = smote_power.fit_resample(X_train_power, y_train)\nX_test_power_smote = X_test_power.copy()\ny_test_power_smote = y_test.copy()\n\n# Configuration 3: MI + SMOTE+Tomek\n# First apply MI selection\nmi = SelectKBest(score_func=mutual_info_classif, k=5)\nX_train_mi = mi.fit_transform(X_train_robust, y_train)\nX_test_mi = mi.transform(X_test_robust)\n\nsmotetomek_mi = SMOTETomek(random_state=42)\nX_train_mi_smotetomek, y_train_mi_smotetomek = smotetomek_mi.fit_resample(X_train_mi, y_train)\nX_test_mi_smotetomek = X_test_mi.copy()\ny_test_mi_smotetomek = y_test.copy()\n\n# Configuration 4: MI only (without SMOTE)\n# Already created above: X_train_mi, X_test_mi\ny_train_mi = y_train.copy()\ny_test_mi = y_test.copy()\n\n# Configuration 5: LDA\nlda = LinearDiscriminantAnalysis(n_components=2)\nX_train_lda = lda.fit_transform(X_train_mi, y_train)\nX_test_lda = lda.transform(X_test_mi)\ny_train_lda = y_train.copy()\ny_test_lda = y_test.copy()\n\nprint(\"All configurations prepared!\\n\")\n\n# ==============================================================================\n# STEP 2: DEFINE CLASSIFIERS AND HELPER FUNCTIONS\n# ==============================================================================\n\n# Define all classifiers - CREATE NEW INSTANCES FOR EACH TRAINING\nfrom sklearn.linear_model import LogisticRegression\n\ndef get_classifiers():\n    return {\n        'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),\n        'MLPClassifier': MLPClassifier(random_state=42, max_iter=1000),\n        'SVC': SVC(random_state=42, probability=True),\n        'AdaBoostClassifier': AdaBoostClassifier(random_state=42, algorithm='SAMME'),\n        'RandomForestClassifier': RandomForestClassifier(random_state=42, n_estimators=100),\n        'ExtraTreesClassifier': ExtraTreesClassifier(random_state=42, n_estimators=100),\n        'GradientBoostingClassifier': GradientBoostingClassifier(random_state=42),\n        'XGBClassifier': XGBClassifier(random_state=42, eval_metric='logloss', use_label_encoder=False),\n        'LGBMClassifier': LGBMClassifier(random_state=42, verbose=-1),\n        'DecisionTreeClassifier': DecisionTreeClassifier(random_state=42)\n    }\n\n# Function to calculate metrics\ndef calculate_metrics(y_true, y_pred, y_pred_proba=None):\n    \"\"\"Calculate classification metrics\"\"\"\n    metrics = {\n        'Accuracy': accuracy_score(y_true, y_pred),\n        'Precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),\n        'Recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),\n        'F1-Score': f1_score(y_true, y_pred, average='weighted', zero_division=0)\n    }\n    \n    # Add ROC-AUC if probability predictions are available\n    if y_pred_proba is not None:\n        try:\n            if len(np.unique(y_true)) == 2:  # Binary classification\n                metrics['ROC-AUC'] = roc_auc_score(y_true, y_pred_proba[:, 1])\n            else:  # Multi-class\n                metrics['ROC-AUC'] = roc_auc_score(y_true, y_pred_proba, \n                                                    multi_class='ovr', average='weighted')\n        except Exception as e:\n            metrics['ROC-AUC'] = 'N/A'\n    else:\n        metrics['ROC-AUC'] = 'N/A'\n    \n    return metrics\n\n# Function to train and evaluate a model\ndef train_and_evaluate(model, X_train, y_train, X_test, y_test, model_name, config_name):\n    \"\"\"Train model and return metrics\"\"\"\n    try:\n        # Train the model\n        model.fit(X_train, y_train)\n        \n        # Make predictions\n        y_pred = model.predict(X_test)\n        \n        # Get probability predictions if available\n        try:\n            y_pred_proba = model.predict_proba(X_test)\n        except:\n            y_pred_proba = None\n        \n        # Calculate metrics\n        metrics = calculate_metrics(y_test, y_pred, y_pred_proba)\n        metrics['Model'] = model_name\n        metrics['Configuration'] = config_name\n        \n        return metrics\n    \n    except Exception as e:\n        print(f\"  ❌ Error: {str(e)}\")\n        return {\n            'Model': model_name,\n            'Configuration': config_name,\n            'Accuracy': 0.0,\n            'Precision': 0.0,\n            'Recall': 0.0,\n            'F1-Score': 0.0,\n            'ROC-AUC': 'Error'\n        }\n\n# ==============================================================================\n# STEP 3: TRAIN ALL MODELS ON ALL CONFIGURATIONS\n# ==============================================================================\n\n# Store all configurations\nconfigurations = [\n    ('Boruta + SMOTE+Tomek', X_train_boruta_smotetomek, y_train_boruta_smotetomek, \n     X_test_boruta_smotetomek, y_test_boruta_smotetomek),\n    ('PowerTransformer + SMOTE', X_train_power_smote, y_train_power_smote, \n     X_test_power_smote, y_test_power_smote),\n    ('MI + SMOTE+Tomek', X_train_mi_smotetomek, y_train_mi_smotetomek, \n     X_test_mi_smotetomek, y_test_mi_smotetomek),\n    ('MI', X_train_mi, y_train_mi, X_test_mi, y_test_mi),\n    ('LDA', X_train_lda, y_train_lda, X_test_lda, y_test_lda)\n]\n\n# Dictionary to store all results\nall_results = []\n\n# Train all models on all configurations\nfor config_name, X_tr, y_tr, X_te, y_te in configurations:\n    print(\"=\"*80)\n    print(f\"CONFIGURATION: {config_name}\")\n    print(\"=\"*80)\n    print(f\"Training shape: {X_tr.shape}, Test shape: {X_te.shape}\")\n    print()\n    \n    classifiers = get_classifiers()  # Get fresh instances\n    \n    for model_name, model in classifiers.items():\n        print(f\"Training {model_name}...\", end=' ')\n        \n        metrics = train_and_evaluate(\n            model, X_tr, y_tr, X_te, y_te, model_name, config_name\n        )\n        all_results.append(metrics)\n        \n        # Display metrics inline\n        print(f\"✓ Acc: {metrics['Accuracy']:.4f} | Prec: {metrics['Precision']:.4f} | \" +\n              f\"Rec: {metrics['Recall']:.4f} | F1: {metrics['F1-Score']:.4f} | \" +\n              f\"AUC: {metrics['ROC-AUC']}\")\n    \n    print()\n\n# ==============================================================================\n# STEP 4: DISPLAY ALL RESULTS\n# ==============================================================================\n\n# Create comprehensive results DataFrame\nresults_df = pd.DataFrame(all_results)\n\n# Display summary table sorted by accuracy\nprint(\"\\n\" + \"=\"*80)\nprint(\"📊 ALL RESULTS SORTED BY ACCURACY (HIGHEST TO LOWEST)\")\nprint(\"=\"*80)\nresults_sorted = results_df.sort_values('Accuracy', ascending=False).reset_index(drop=True)\nresults_sorted.index = results_sorted.index + 1  # Start index from 1\n\n# Format the display\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.float_format', '{:.4f}'.format)\n\nprint(results_sorted.to_string())\n\n# Top 10 models\nprint(\"\\n\" + \"=\"*80)\nprint(\"🏆 TOP 10 MODEL-CONFIGURATION COMBINATIONS\")\nprint(\"=\"*80)\nprint(results_sorted.head(10).to_string())\n\n# Best model per configuration\nprint(\"\\n\" + \"=\"*80)\nprint(\"🥇 BEST MODEL PER CONFIGURATION\")\nprint(\"=\"*80)\nfor config in configurations:\n    config_name = config[0]\n    config_results = results_df[results_df['Configuration'] == config_name]\n    best_model = config_results.loc[config_results['Accuracy'].idxmax()]\n    print(f\"\\n{config_name}:\")\n    print(f\"  Model:      {best_model['Model']}\")\n    print(f\"  Accuracy:   {best_model['Accuracy']:.4f}\")\n    print(f\"  Precision:  {best_model['Precision']:.4f}\")\n    print(f\"  Recall:     {best_model['Recall']:.4f}\")\n    print(f\"  F1-Score:   {best_model['F1-Score']:.4f}\")\n    print(f\"  ROC-AUC:    {best_model['ROC-AUC']}\")\n\n# Best configuration per model\nprint(\"\\n\" + \"=\"*80)\nprint(\"🎯 BEST CONFIGURATION PER MODEL\")\nprint(\"=\"*80)\nmodel_names = results_df['Model'].unique()\nfor model_name in sorted(model_names):\n    model_results = results_df[results_df['Model'] == model_name]\n    best_config = model_results.loc[model_results['Accuracy'].idxmax()]\n    print(f\"\\n{model_name}:\")\n    print(f\"  Configuration: {best_config['Configuration']}\")\n    print(f\"  Accuracy:      {best_config['Accuracy']:.4f}\")\n    print(f\"  F1-Score:      {best_config['F1-Score']:.4f}\")\n    print(f\"  ROC-AUC:       {best_config['ROC-AUC']}\")\n\n# Overall best model\nprint(\"\\n\" + \"=\"*80)\nprint(\"🌟 OVERALL BEST MODEL-CONFIGURATION COMBINATION\")\nprint(\"=\"*80)\nbest_overall = results_df.loc[results_df['Accuracy'].idxmax()]\nprint(f\"Model:         {best_overall['Model']}\")\nprint(f\"Configuration: {best_overall['Configuration']}\")\nprint(f\"Accuracy:      {best_overall['Accuracy']:.4f}\")\nprint(f\"Precision:     {best_overall['Precision']:.4f}\")\nprint(f\"Recall:        {best_overall['Recall']:.4f}\")\nprint(f\"F1-Score:      {best_overall['F1-Score']:.4f}\")\nprint(f\"ROC-AUC:       {best_overall['ROC-AUC']}\")\n\n# Create pivot tables\nprint(\"\\n\" + \"=\"*80)\nprint(\"📈 PIVOT TABLE: ACCURACY BY MODEL AND CONFIGURATION\")\nprint(\"=\"*80)\npivot_accuracy = results_df.pivot(index='Model', columns='Configuration', values='Accuracy')\nprint(pivot_accuracy.to_string())\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"📈 PIVOT TABLE: F1-SCORE BY MODEL AND CONFIGURATION\")\nprint(\"=\"*80)\npivot_f1 = results_df.pivot(index='Model', columns='Configuration', values='F1-Score')\nprint(pivot_f1.to_string())\n\n# Export results\nresults_df.to_csv('model_comparison_results.csv', index=False)\nprint(\"\\n\" + \"=\"*80)\nprint(\"💾 Results exported to 'model_comparison_results.csv'\")\nprint(\"=\"*80)\n\n# Summary statistics\nprint(\"\\n\" + \"=\"*80)\nprint(\"📊 SUMMARY STATISTICS\")\nprint(\"=\"*80)\nprint(f\"Total experiments run: {len(results_df)}\")\nprint(f\"Number of models: {len(model_names)}\")\nprint(f\"Number of configurations: {len(configurations)}\")\nprint(f\"\\nAccuracy Statistics:\")\nprint(f\"  Mean:    {results_df['Accuracy'].mean():.4f}\")\nprint(f\"  Median:  {results_df['Accuracy'].median():.4f}\")\nprint(f\"  Std Dev: {results_df['Accuracy'].std():.4f}\")\nprint(f\"  Min:     {results_df['Accuracy'].min():.4f}\")\nprint(f\"  Max:     {results_df['Accuracy'].max():.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:06:06.134498Z","iopub.execute_input":"2025-10-06T14:06:06.134803Z","iopub.status.idle":"2025-10-06T14:06:24.030266Z","shell.execute_reply.started":"2025-10-06T14:06:06.134744Z","shell.execute_reply":"2025-10-06T14:06:24.029305Z"}},"outputs":[{"name":"stdout","text":"Preparing all configurations...\nAll configurations prepared!\n\n================================================================================\nCONFIGURATION: Boruta + SMOTE+Tomek\n================================================================================\nTraining shape: (519, 10), Test shape: (75, 10)\n\nTraining LogisticRegression... ✓ Acc: 0.9467 | Prec: 0.9499 | Rec: 0.9467 | F1: 0.9476 | AUC: 0.9763151691877772\nTraining MLPClassifier... ✓ Acc: 0.9467 | Prec: 0.9573 | Rec: 0.9467 | F1: 0.9484 | AUC: 0.9865429803778628\nTraining SVC... ✓ Acc: 0.9333 | Prec: 0.9419 | Rec: 0.9333 | F1: 0.9341 | AUC: 0.9788162323066641\nTraining AdaBoostClassifier... ✓ Acc: 0.9067 | Prec: 0.9065 | Rec: 0.9067 | F1: 0.9062 | AUC: 0.9820314683190571\nTraining RandomForestClassifier... ✓ Acc: 0.9600 | Prec: 0.9606 | Rec: 0.9600 | F1: 0.9599 | AUC: 0.9941694915254238\nTraining ExtraTreesClassifier... ✓ Acc: 0.9333 | Prec: 0.9341 | Rec: 0.9333 | F1: 0.9333 | AUC: 0.9768079096045199\nTraining GradientBoostingClassifier... ✓ Acc: 0.9600 | Prec: 0.9606 | Rec: 0.9600 | F1: 0.9599 | AUC: 0.9824745762711863\nTraining XGBClassifier... ✓ Acc: 0.9333 | Prec: 0.9341 | Rec: 0.9333 | F1: 0.9333 | AUC: 0.9829566854990585\nTraining LGBMClassifier... ✓ Acc: 0.9333 | Prec: 0.9363 | Rec: 0.9333 | F1: 0.9338 | AUC: 0.9890169491525425\nTraining DecisionTreeClassifier... ✓ Acc: 0.9200 | Prec: 0.9245 | Rec: 0.9200 | F1: 0.9213 | AUC: 0.9385846546382358\n\n================================================================================\nCONFIGURATION: PowerTransformer + SMOTE\n================================================================================\nTraining shape: (525, 24), Test shape: (75, 24)\n\nTraining LogisticRegression... ✓ Acc: 0.9600 | Prec: 0.9663 | Rec: 0.9600 | F1: 0.9612 | AUC: 0.99370572869206\nTraining MLPClassifier... ✓ Acc: 0.9200 | Prec: 0.9245 | Rec: 0.9200 | F1: 0.9213 | AUC: 0.970078245550088\nTraining SVC... ✓ Acc: 0.9467 | Prec: 0.9520 | Rec: 0.9467 | F1: 0.9474 | AUC: 0.9886218334244579\nTraining AdaBoostClassifier... ✓ Acc: 0.9600 | Prec: 0.9663 | Rec: 0.9600 | F1: 0.9595 | AUC: 0.9952862523540491\nTraining RandomForestClassifier... ✓ Acc: 0.9600 | Prec: 0.9617 | Rec: 0.9600 | F1: 0.9606 | AUC: 0.9946214689265537\nTraining ExtraTreesClassifier... ✓ Acc: 0.9333 | Prec: 0.9403 | Rec: 0.9333 | F1: 0.9345 | AUC: 0.9665553125569528\nTraining GradientBoostingClassifier... ✓ Acc: 0.9467 | Prec: 0.9482 | Rec: 0.9467 | F1: 0.9472 | AUC: 0.9881016949152541\nTraining XGBClassifier... ✓ Acc: 0.9333 | Prec: 0.9337 | Rec: 0.9333 | F1: 0.9324 | AUC: 0.986162809063848\nTraining LGBMClassifier... ✓ Acc: 0.9333 | Prec: 0.9403 | Rec: 0.9333 | F1: 0.9345 | AUC: 0.9867645951035781\nTraining DecisionTreeClassifier... ✓ Acc: 0.8933 | Prec: 0.9074 | Rec: 0.8933 | F1: 0.8968 | AUC: 0.9193338193305389\n\n================================================================================\nCONFIGURATION: MI + SMOTE+Tomek\n================================================================================\nTraining shape: (523, 5), Test shape: (75, 5)\n\nTraining LogisticRegression... ✓ Acc: 0.8267 | Prec: 0.8754 | Rec: 0.8267 | F1: 0.8377 | AUC: 0.9641674260373003\nTraining MLPClassifier... ✓ Acc: 0.9733 | Prec: 0.9741 | Rec: 0.9733 | F1: 0.9735 | AUC: 0.9997777777777778\nTraining SVC... ✓ Acc: 0.9333 | Prec: 0.9419 | Rec: 0.9333 | F1: 0.9341 | AUC: 0.9895254237288136\nTraining AdaBoostClassifier... ✓ Acc: 0.9067 | Prec: 0.9178 | Rec: 0.9067 | F1: 0.9102 | AUC: 0.9898141060688901\nTraining RandomForestClassifier... ✓ Acc: 0.9733 | Prec: 0.9763 | Rec: 0.9733 | F1: 0.9732 | AUC: 0.9970922787193973\nTraining ExtraTreesClassifier... ✓ Acc: 0.9733 | Prec: 0.9763 | Rec: 0.9733 | F1: 0.9732 | AUC: 0.9876120527306969\nTraining GradientBoostingClassifier... ✓ Acc: 0.9733 | Prec: 0.9763 | Rec: 0.9733 | F1: 0.9732 | AUC: 0.9921883239171374\nTraining XGBClassifier... ✓ Acc: 0.9600 | Prec: 0.9608 | Rec: 0.9600 | F1: 0.9600 | AUC: 0.9930621468926554\nTraining LGBMClassifier... ✓ Acc: 0.9733 | Prec: 0.9763 | Rec: 0.9733 | F1: 0.9732 | AUC: 0.9975480225988701\nTraining DecisionTreeClassifier... ✓ Acc: 0.9467 | Prec: 0.9483 | Rec: 0.9467 | F1: 0.9472 | AUC: 0.9541753234918898\n\n================================================================================\nCONFIGURATION: MI\n================================================================================\nTraining shape: (299, 5), Test shape: (75, 5)\n\nTraining LogisticRegression... ✓ Acc: 0.9467 | Prec: 0.9467 | Rec: 0.9467 | F1: 0.9467 | AUC: 0.9717380475062268\nTraining MLPClassifier... ✓ Acc: 0.9733 | Prec: 0.9765 | Rec: 0.9733 | F1: 0.9733 | AUC: 0.9982033898305086\nTraining SVC... ✓ Acc: 0.9333 | Prec: 0.9358 | Rec: 0.9333 | F1: 0.9343 | AUC: 0.9946252354048963\nTraining AdaBoostClassifier... ✓ Acc: 0.9467 | Prec: 0.9491 | Rec: 0.9467 | F1: 0.9463 | AUC: 0.9964161958568739\nTraining RandomForestClassifier... ✓ Acc: 0.9733 | Prec: 0.9733 | Rec: 0.9733 | F1: 0.9733 | AUC: 0.9968625235404898\nTraining ExtraTreesClassifier... ✓ Acc: 0.9600 | Prec: 0.9606 | Rec: 0.9600 | F1: 0.9599 | AUC: 0.976467043314501\nTraining GradientBoostingClassifier... ✓ Acc: 0.9600 | Prec: 0.9606 | Rec: 0.9600 | F1: 0.9599 | AUC: 0.9870018832391714\nTraining XGBClassifier... ✓ Acc: 0.9467 | Prec: 0.9495 | Rec: 0.9467 | F1: 0.9466 | AUC: 0.9770885122410546\nTraining LGBMClassifier... ✓ Acc: 0.9733 | Prec: 0.9733 | Rec: 0.9733 | F1: 0.9733 | AUC: 0.9923653483992467\nTraining DecisionTreeClassifier... ✓ Acc: 0.9733 | Prec: 0.9733 | Rec: 0.9733 | F1: 0.9733 | AUC: 0.9778116760828626\n\n================================================================================\nCONFIGURATION: LDA\n================================================================================\nTraining shape: (299, 2), Test shape: (75, 2)\n\nTraining LogisticRegression... ✓ Acc: 0.8800 | Prec: 0.8747 | Rec: 0.8800 | F1: 0.8744 | AUC: 0.9733237348885245\nTraining MLPClassifier... ✓ Acc: 0.9333 | Prec: 0.9413 | Rec: 0.9333 | F1: 0.9354 | AUC: 0.9774576271186439\nTraining SVC... ✓ Acc: 0.9333 | Prec: 0.9413 | Rec: 0.9333 | F1: 0.9354 | AUC: 0.981093129214507\nTraining AdaBoostClassifier... ✓ Acc: 0.9467 | Prec: 0.9505 | Rec: 0.9467 | F1: 0.9476 | AUC: 0.989241054613936\nTraining RandomForestClassifier... ✓ Acc: 0.9733 | Prec: 0.9733 | Rec: 0.9733 | F1: 0.9733 | AUC: 0.9979811676082864\nTraining ExtraTreesClassifier... ✓ Acc: 0.9733 | Prec: 0.9733 | Rec: 0.9733 | F1: 0.9733 | AUC: 0.9768003766478344\nTraining GradientBoostingClassifier... ✓ Acc: 0.9600 | Prec: 0.9606 | Rec: 0.9600 | F1: 0.9599 | AUC: 0.9847344632768361\nTraining XGBClassifier... ✓ Acc: 0.9733 | Prec: 0.9733 | Rec: 0.9733 | F1: 0.9733 | AUC: 0.9883352165725048\nTraining LGBMClassifier... ✓ Acc: 0.9867 | Prec: 0.9875 | Rec: 0.9867 | F1: 0.9867 | AUC: 0.9993276836158193\nTraining DecisionTreeClassifier... ✓ Acc: 0.9733 | Prec: 0.9733 | Rec: 0.9733 | F1: 0.9733 | AUC: 0.9777005649717514\n\n\n================================================================================\n📊 ALL RESULTS SORTED BY ACCURACY (HIGHEST TO LOWEST)\n================================================================================\n    Accuracy  Precision  Recall  F1-Score  ROC-AUC                       Model             Configuration\n1     0.9867     0.9875  0.9867    0.9867   0.9993              LGBMClassifier                       LDA\n2     0.9733     0.9763  0.9733    0.9732   0.9876        ExtraTreesClassifier          MI + SMOTE+Tomek\n3     0.9733     0.9741  0.9733    0.9735   0.9998               MLPClassifier          MI + SMOTE+Tomek\n4     0.9733     0.9733  0.9733    0.9733   0.9883               XGBClassifier                       LDA\n5     0.9733     0.9733  0.9733    0.9733   0.9768        ExtraTreesClassifier                       LDA\n6     0.9733     0.9733  0.9733    0.9733   0.9980      RandomForestClassifier                       LDA\n7     0.9733     0.9733  0.9733    0.9733   0.9778      DecisionTreeClassifier                        MI\n8     0.9733     0.9733  0.9733    0.9733   0.9924              LGBMClassifier                        MI\n9     0.9733     0.9733  0.9733    0.9733   0.9969      RandomForestClassifier                        MI\n10    0.9733     0.9765  0.9733    0.9733   0.9982               MLPClassifier                        MI\n11    0.9733     0.9763  0.9733    0.9732   0.9975              LGBMClassifier          MI + SMOTE+Tomek\n12    0.9733     0.9763  0.9733    0.9732   0.9922  GradientBoostingClassifier          MI + SMOTE+Tomek\n13    0.9733     0.9763  0.9733    0.9732   0.9971      RandomForestClassifier          MI + SMOTE+Tomek\n14    0.9733     0.9733  0.9733    0.9733   0.9777      DecisionTreeClassifier                       LDA\n15    0.9600     0.9617  0.9600    0.9606   0.9946      RandomForestClassifier  PowerTransformer + SMOTE\n16    0.9600     0.9663  0.9600    0.9612   0.9937          LogisticRegression  PowerTransformer + SMOTE\n17    0.9600     0.9606  0.9600    0.9599   0.9847  GradientBoostingClassifier                       LDA\n18    0.9600     0.9606  0.9600    0.9599   0.9942      RandomForestClassifier      Boruta + SMOTE+Tomek\n19    0.9600     0.9606  0.9600    0.9599   0.9825  GradientBoostingClassifier      Boruta + SMOTE+Tomek\n20    0.9600     0.9663  0.9600    0.9595   0.9953          AdaBoostClassifier  PowerTransformer + SMOTE\n21    0.9600     0.9606  0.9600    0.9599   0.9870  GradientBoostingClassifier                        MI\n22    0.9600     0.9606  0.9600    0.9599   0.9765        ExtraTreesClassifier                        MI\n23    0.9600     0.9608  0.9600    0.9600   0.9931               XGBClassifier          MI + SMOTE+Tomek\n24    0.9467     0.9505  0.9467    0.9476   0.9892          AdaBoostClassifier                       LDA\n25    0.9467     0.9495  0.9467    0.9466   0.9771               XGBClassifier                        MI\n26    0.9467     0.9491  0.9467    0.9463   0.9964          AdaBoostClassifier                        MI\n27    0.9467     0.9467  0.9467    0.9467   0.9717          LogisticRegression                        MI\n28    0.9467     0.9483  0.9467    0.9472   0.9542      DecisionTreeClassifier          MI + SMOTE+Tomek\n29    0.9467     0.9499  0.9467    0.9476   0.9763          LogisticRegression      Boruta + SMOTE+Tomek\n30    0.9467     0.9520  0.9467    0.9474   0.9886                         SVC  PowerTransformer + SMOTE\n31    0.9467     0.9482  0.9467    0.9472   0.9881  GradientBoostingClassifier  PowerTransformer + SMOTE\n32    0.9467     0.9573  0.9467    0.9484   0.9865               MLPClassifier      Boruta + SMOTE+Tomek\n33    0.9333     0.9358  0.9333    0.9343   0.9946                         SVC                        MI\n34    0.9333     0.9403  0.9333    0.9345   0.9868              LGBMClassifier  PowerTransformer + SMOTE\n35    0.9333     0.9419  0.9333    0.9341   0.9788                         SVC      Boruta + SMOTE+Tomek\n36    0.9333     0.9403  0.9333    0.9345   0.9666        ExtraTreesClassifier  PowerTransformer + SMOTE\n37    0.9333     0.9363  0.9333    0.9338   0.9890              LGBMClassifier      Boruta + SMOTE+Tomek\n38    0.9333     0.9419  0.9333    0.9341   0.9895                         SVC          MI + SMOTE+Tomek\n39    0.9333     0.9337  0.9333    0.9324   0.9862               XGBClassifier  PowerTransformer + SMOTE\n40    0.9333     0.9341  0.9333    0.9333   0.9830               XGBClassifier      Boruta + SMOTE+Tomek\n41    0.9333     0.9341  0.9333    0.9333   0.9768        ExtraTreesClassifier      Boruta + SMOTE+Tomek\n42    0.9333     0.9413  0.9333    0.9354   0.9775               MLPClassifier                       LDA\n43    0.9333     0.9413  0.9333    0.9354   0.9811                         SVC                       LDA\n44    0.9200     0.9245  0.9200    0.9213   0.9701               MLPClassifier  PowerTransformer + SMOTE\n45    0.9200     0.9245  0.9200    0.9213   0.9386      DecisionTreeClassifier      Boruta + SMOTE+Tomek\n46    0.9067     0.9178  0.9067    0.9102   0.9898          AdaBoostClassifier          MI + SMOTE+Tomek\n47    0.9067     0.9065  0.9067    0.9062   0.9820          AdaBoostClassifier      Boruta + SMOTE+Tomek\n48    0.8933     0.9074  0.8933    0.8968   0.9193      DecisionTreeClassifier  PowerTransformer + SMOTE\n49    0.8800     0.8747  0.8800    0.8744   0.9733          LogisticRegression                       LDA\n50    0.8267     0.8754  0.8267    0.8377   0.9642          LogisticRegression          MI + SMOTE+Tomek\n\n================================================================================\n🏆 TOP 10 MODEL-CONFIGURATION COMBINATIONS\n================================================================================\n    Accuracy  Precision  Recall  F1-Score  ROC-AUC                   Model     Configuration\n1     0.9867     0.9875  0.9867    0.9867   0.9993          LGBMClassifier               LDA\n2     0.9733     0.9763  0.9733    0.9732   0.9876    ExtraTreesClassifier  MI + SMOTE+Tomek\n3     0.9733     0.9741  0.9733    0.9735   0.9998           MLPClassifier  MI + SMOTE+Tomek\n4     0.9733     0.9733  0.9733    0.9733   0.9883           XGBClassifier               LDA\n5     0.9733     0.9733  0.9733    0.9733   0.9768    ExtraTreesClassifier               LDA\n6     0.9733     0.9733  0.9733    0.9733   0.9980  RandomForestClassifier               LDA\n7     0.9733     0.9733  0.9733    0.9733   0.9778  DecisionTreeClassifier                MI\n8     0.9733     0.9733  0.9733    0.9733   0.9924          LGBMClassifier                MI\n9     0.9733     0.9733  0.9733    0.9733   0.9969  RandomForestClassifier                MI\n10    0.9733     0.9765  0.9733    0.9733   0.9982           MLPClassifier                MI\n\n================================================================================\n🥇 BEST MODEL PER CONFIGURATION\n================================================================================\n\nBoruta + SMOTE+Tomek:\n  Model:      RandomForestClassifier\n  Accuracy:   0.9600\n  Precision:  0.9606\n  Recall:     0.9600\n  F1-Score:   0.9599\n  ROC-AUC:    0.9941694915254238\n\nPowerTransformer + SMOTE:\n  Model:      LogisticRegression\n  Accuracy:   0.9600\n  Precision:  0.9663\n  Recall:     0.9600\n  F1-Score:   0.9612\n  ROC-AUC:    0.99370572869206\n\nMI + SMOTE+Tomek:\n  Model:      MLPClassifier\n  Accuracy:   0.9733\n  Precision:  0.9741\n  Recall:     0.9733\n  F1-Score:   0.9735\n  ROC-AUC:    0.9997777777777778\n\nMI:\n  Model:      MLPClassifier\n  Accuracy:   0.9733\n  Precision:  0.9765\n  Recall:     0.9733\n  F1-Score:   0.9733\n  ROC-AUC:    0.9982033898305086\n\nLDA:\n  Model:      LGBMClassifier\n  Accuracy:   0.9867\n  Precision:  0.9875\n  Recall:     0.9867\n  F1-Score:   0.9867\n  ROC-AUC:    0.9993276836158193\n\n================================================================================\n🎯 BEST CONFIGURATION PER MODEL\n================================================================================\n\nAdaBoostClassifier:\n  Configuration: PowerTransformer + SMOTE\n  Accuracy:      0.9600\n  F1-Score:      0.9595\n  ROC-AUC:       0.9952862523540491\n\nDecisionTreeClassifier:\n  Configuration: MI\n  Accuracy:      0.9733\n  F1-Score:      0.9733\n  ROC-AUC:       0.9778116760828626\n\nExtraTreesClassifier:\n  Configuration: MI + SMOTE+Tomek\n  Accuracy:      0.9733\n  F1-Score:      0.9732\n  ROC-AUC:       0.9876120527306969\n\nGradientBoostingClassifier:\n  Configuration: MI + SMOTE+Tomek\n  Accuracy:      0.9733\n  F1-Score:      0.9732\n  ROC-AUC:       0.9921883239171374\n\nLGBMClassifier:\n  Configuration: LDA\n  Accuracy:      0.9867\n  F1-Score:      0.9867\n  ROC-AUC:       0.9993276836158193\n\nLogisticRegression:\n  Configuration: PowerTransformer + SMOTE\n  Accuracy:      0.9600\n  F1-Score:      0.9612\n  ROC-AUC:       0.99370572869206\n\nMLPClassifier:\n  Configuration: MI + SMOTE+Tomek\n  Accuracy:      0.9733\n  F1-Score:      0.9735\n  ROC-AUC:       0.9997777777777778\n\nRandomForestClassifier:\n  Configuration: MI + SMOTE+Tomek\n  Accuracy:      0.9733\n  F1-Score:      0.9732\n  ROC-AUC:       0.9970922787193973\n\nSVC:\n  Configuration: PowerTransformer + SMOTE\n  Accuracy:      0.9467\n  F1-Score:      0.9474\n  ROC-AUC:       0.9886218334244579\n\nXGBClassifier:\n  Configuration: LDA\n  Accuracy:      0.9733\n  F1-Score:      0.9733\n  ROC-AUC:       0.9883352165725048\n\n================================================================================\n🌟 OVERALL BEST MODEL-CONFIGURATION COMBINATION\n================================================================================\nModel:         LGBMClassifier\nConfiguration: LDA\nAccuracy:      0.9867\nPrecision:     0.9875\nRecall:        0.9867\nF1-Score:      0.9867\nROC-AUC:       0.9993276836158193\n\n================================================================================\n📈 PIVOT TABLE: ACCURACY BY MODEL AND CONFIGURATION\n================================================================================\nConfiguration               Boruta + SMOTE+Tomek    LDA     MI  MI + SMOTE+Tomek  PowerTransformer + SMOTE\nModel                                                                                                     \nAdaBoostClassifier                        0.9067 0.9467 0.9467            0.9067                    0.9600\nDecisionTreeClassifier                    0.9200 0.9733 0.9733            0.9467                    0.8933\nExtraTreesClassifier                      0.9333 0.9733 0.9600            0.9733                    0.9333\nGradientBoostingClassifier                0.9600 0.9600 0.9600            0.9733                    0.9467\nLGBMClassifier                            0.9333 0.9867 0.9733            0.9733                    0.9333\nLogisticRegression                        0.9467 0.8800 0.9467            0.8267                    0.9600\nMLPClassifier                             0.9467 0.9333 0.9733            0.9733                    0.9200\nRandomForestClassifier                    0.9600 0.9733 0.9733            0.9733                    0.9600\nSVC                                       0.9333 0.9333 0.9333            0.9333                    0.9467\nXGBClassifier                             0.9333 0.9733 0.9467            0.9600                    0.9333\n\n================================================================================\n📈 PIVOT TABLE: F1-SCORE BY MODEL AND CONFIGURATION\n================================================================================\nConfiguration               Boruta + SMOTE+Tomek    LDA     MI  MI + SMOTE+Tomek  PowerTransformer + SMOTE\nModel                                                                                                     \nAdaBoostClassifier                        0.9062 0.9476 0.9463            0.9102                    0.9595\nDecisionTreeClassifier                    0.9213 0.9733 0.9733            0.9472                    0.8968\nExtraTreesClassifier                      0.9333 0.9733 0.9599            0.9732                    0.9345\nGradientBoostingClassifier                0.9599 0.9599 0.9599            0.9732                    0.9472\nLGBMClassifier                            0.9338 0.9867 0.9733            0.9732                    0.9345\nLogisticRegression                        0.9476 0.8744 0.9467            0.8377                    0.9612\nMLPClassifier                             0.9484 0.9354 0.9733            0.9735                    0.9213\nRandomForestClassifier                    0.9599 0.9733 0.9733            0.9732                    0.9606\nSVC                                       0.9341 0.9354 0.9343            0.9341                    0.9474\nXGBClassifier                             0.9333 0.9733 0.9466            0.9600                    0.9324\n\n================================================================================\n💾 Results exported to 'model_comparison_results.csv'\n================================================================================\n\n================================================================================\n📊 SUMMARY STATISTICS\n================================================================================\nTotal experiments run: 50\nNumber of models: 10\nNumber of configurations: 5\n\nAccuracy Statistics:\n  Mean:    0.9464\n  Median:  0.9467\n  Std Dev: 0.0288\n  Min:     0.8267\n  Max:     0.9867\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"# Stacking,voting and ensembling","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import (AdaBoostClassifier, RandomForestClassifier, \n                               ExtraTreesClassifier, GradientBoostingClassifier,\n                               StackingClassifier, VotingClassifier, BaggingClassifier)\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (accuracy_score, precision_score, recall_score, \n                              f1_score, roc_auc_score)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PowerTransformer, RobustScaler\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.over_sampling import SMOTE\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ==============================================================================\n# STEP 1: PREPARE ALL 5 CONFIGURATIONS\n# ==============================================================================\n\nprint(\"=\"*80)\nprint(\"PREPARING ALL CONFIGURATIONS\")\nprint(\"=\"*80)\n\n# Configuration 1: Boruta + SMOTE+Tomek\nprint(\"1. Boruta + SMOTE+Tomek...\")\nsmotetomek_boruta = SMOTETomek(random_state=42)\nX_train_boruta_smotetomek, y_train_boruta_smotetomek = smotetomek_boruta.fit_resample(X_train_boruta, y_train)\nX_test_boruta_smotetomek = X_test_boruta.copy()\ny_test_boruta_smotetomek = y_test.copy()\n\n# Configuration 2: PowerTransformer + SMOTE\nprint(\"2. PowerTransformer + SMOTE...\")\nsmote_power = SMOTE(random_state=42, k_neighbors=5)\nX_train_power_smote, y_train_power_smote = smote_power.fit_resample(X_train_power, y_train)\nX_test_power_smote = X_test_power.copy()\ny_test_power_smote = y_test.copy()\n\n# Configuration 3: MI + SMOTE+Tomek\nprint(\"3. MI + SMOTE+Tomek...\")\nmi = SelectKBest(score_func=mutual_info_classif, k=5)\nX_train_mi = mi.fit_transform(X_train_robust, y_train)\nX_test_mi = mi.transform(X_test_robust)\n\nsmotetomek_mi = SMOTETomek(random_state=42)\nX_train_mi_smotetomek, y_train_mi_smotetomek = smotetomek_mi.fit_resample(X_train_mi, y_train)\nX_test_mi_smotetomek = X_test_mi.copy()\ny_test_mi_smotetomek = y_test.copy()\n\n# Configuration 4: MI only\nprint(\"4. MI only...\")\ny_train_mi = y_train.copy()\ny_test_mi = y_test.copy()\n\n# Configuration 5: LDA\nprint(\"5. LDA...\")\nlda = LinearDiscriminantAnalysis(n_components=2)\nX_train_lda = lda.fit_transform(X_train_mi, y_train)\nX_test_lda = lda.transform(X_test_mi)\ny_train_lda = y_train.copy()\ny_test_lda = y_test.copy()\n\nprint(\"\\n✓ All configurations prepared!\\n\")\n\n# ==============================================================================\n# STEP 2: DEFINE BASE CLASSIFIERS\n# ==============================================================================\n\ndef get_base_classifiers():\n    \"\"\"Returns dictionary of base classifiers\"\"\"\n    return {\n        'MLPClassifier': MLPClassifier(random_state=42, max_iter=1000),\n        'SVC': SVC(random_state=42, probability=True),\n        'AdaBoostClassifier': AdaBoostClassifier(random_state=42, algorithm='SAMME'),\n        'RandomForestClassifier': RandomForestClassifier(random_state=42, n_estimators=100),\n        'ExtraTreesClassifier': ExtraTreesClassifier(random_state=42, n_estimators=100),\n        'GradientBoostingClassifier': GradientBoostingClassifier(random_state=42),\n        'XGBClassifier': XGBClassifier(random_state=42, eval_metric='logloss', use_label_encoder=False),\n        'LGBMClassifier': LGBMClassifier(random_state=42, verbose=-1),\n        'DecisionTreeClassifier': DecisionTreeClassifier(random_state=42)\n    }\n\n# ==============================================================================\n# STEP 3: DEFINE ENSEMBLE CLASSIFIERS\n# ==============================================================================\n\ndef get_ensemble_classifiers():\n    \"\"\"Returns dictionary of ensemble classifiers using improved base estimators\"\"\"\n\n    # Diverse base estimators for ensembles\n    base_estimators = [\n        ('xgb', XGBClassifier(random_state=42, eval_metric='logloss', use_label_encoder=False, n_jobs=-1)),\n        ('lgbm', LGBMClassifier(random_state=42, verbose=-1, n_jobs=-1)),\n        ('et', ExtraTreesClassifier(random_state=42, n_estimators=100, n_jobs=-1)),\n        ('gb', GradientBoostingClassifier(random_state=42, n_estimators=100)),\n        ('svc', SVC(probability=True, kernel='rbf', C=1, gamma='scale', random_state=42))\n    ]\n\n    # Stacking Classifier with Logistic Regression meta-model\n    stacking = StackingClassifier(\n        estimators=base_estimators,\n        final_estimator=LogisticRegression(\n            random_state=42,\n            max_iter=2000,\n            solver='lbfgs',\n            n_jobs=-1\n        ),\n        cv=5,\n        n_jobs=-1,\n        passthrough=True\n    )\n\n    # Voting Classifier (Soft voting for probability averaging)\n    voting = VotingClassifier(\n        estimators=base_estimators,\n        voting='soft',\n        n_jobs=-1\n    )\n\n    # Bagging Ensemble using shallow Decision Trees (better for bagging)\n    bagging = BaggingClassifier(\n        estimator=DecisionTreeClassifier(\n            max_depth=5,\n            min_samples_split=4,\n            random_state=42\n        ),\n        n_estimators=50,\n        max_samples=0.8,\n        max_features=0.8,\n        bootstrap=True,\n        n_jobs=-1,\n        random_state=42\n    )\n\n    return {\n        'StackingClassifier': stacking,\n        'VotingClassifier': voting,\n        'BaggingEnsemble': bagging\n    }\n\n\n# ==============================================================================\n# STEP 4: HELPER FUNCTIONS\n# ==============================================================================\n\ndef calculate_metrics(y_true, y_pred, y_pred_proba=None):\n    \"\"\"Calculate classification metrics\"\"\"\n    metrics = {\n        'Accuracy': accuracy_score(y_true, y_pred),\n        'Precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),\n        'Recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),\n        'F1-Score': f1_score(y_true, y_pred, average='weighted', zero_division=0)\n    }\n    \n    if y_pred_proba is not None:\n        try:\n            if len(np.unique(y_true)) == 2:\n                metrics['ROC-AUC'] = roc_auc_score(y_true, y_pred_proba[:, 1])\n            else:\n                metrics['ROC-AUC'] = roc_auc_score(y_true, y_pred_proba, \n                                                    multi_class='ovr', average='weighted')\n        except:\n            metrics['ROC-AUC'] = 'N/A'\n    else:\n        metrics['ROC-AUC'] = 'N/A'\n    \n    return metrics\n\ndef train_and_evaluate(model, X_train, y_train, X_test, y_test, model_name, config_name):\n    \"\"\"Train model and return metrics\"\"\"\n    try:\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        \n        try:\n            y_pred_proba = model.predict_proba(X_test)\n        except:\n            y_pred_proba = None\n        \n        metrics = calculate_metrics(y_test, y_pred, y_pred_proba)\n        metrics['Model'] = model_name\n        metrics['Configuration'] = config_name\n        metrics['Model_Type'] = 'Ensemble' if model_name in ['StackingClassifier', 'VotingClassifier', 'BaggingEnsemble'] else 'Base'\n        \n        return metrics\n    \n    except Exception as e:\n        print(f\"  ❌ Error: {str(e)}\")\n        return {\n            'Model': model_name,\n            'Configuration': config_name,\n            'Model_Type': 'Ensemble' if model_name in ['StackingClassifier', 'VotingClassifier', 'BaggingEnsemble'] else 'Base',\n            'Accuracy': 0.0,\n            'Precision': 0.0,\n            'Recall': 0.0,\n            'F1-Score': 0.0,\n            'ROC-AUC': 'Error'\n        }\n\n# ==============================================================================\n# STEP 5: PREPARE CONFIGURATIONS LIST\n# ==============================================================================\n\nconfigurations = [\n    ('Boruta + SMOTE+Tomek', X_train_boruta_smotetomek, y_train_boruta_smotetomek, \n     X_test_boruta_smotetomek, y_test_boruta_smotetomek),\n    ('PowerTransformer + SMOTE', X_train_power_smote, y_train_power_smote, \n     X_test_power_smote, y_test_power_smote),\n    ('MI + SMOTE+Tomek', X_train_mi_smotetomek, y_train_mi_smotetomek, \n     X_test_mi_smotetomek, y_test_mi_smotetomek),\n    ('MI', X_train_mi, y_train_mi, X_test_mi, y_test_mi),\n    ('LDA', X_train_lda, y_train_lda, X_test_lda, y_test_lda)\n]\n\n# ==============================================================================\n# STEP 6: TRAIN ALL BASE MODELS ON ALL CONFIGURATIONS\n# ==============================================================================\n\nall_results = []\n\nprint(\"=\"*80)\nprint(\"PHASE 1: TRAINING BASE MODELS\")\nprint(\"=\"*80)\nprint()\n\nfor config_name, X_tr, y_tr, X_te, y_te in configurations:\n    print(\"=\"*80)\n    print(f\"CONFIGURATION: {config_name}\")\n    print(\"=\"*80)\n    print(f\"Training shape: {X_tr.shape}, Test shape: {X_te.shape}\")\n    print()\n    \n    base_classifiers = get_base_classifiers()\n    \n    for model_name, model in base_classifiers.items():\n        print(f\"  Training {model_name}...\", end=' ')\n        \n        metrics = train_and_evaluate(\n            model, X_tr, y_tr, X_te, y_te, model_name, config_name\n        )\n        all_results.append(metrics)\n        \n        print(f\"✓ Acc: {metrics['Accuracy']:.4f} | F1: {metrics['F1-Score']:.4f}\")\n    \n    print()\n\n# ==============================================================================\n# STEP 7: TRAIN ALL ENSEMBLE MODELS ON ALL CONFIGURATIONS\n# ==============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"PHASE 2: TRAINING ENSEMBLE MODELS\")\nprint(\"=\"*80)\nprint()\n\nfor config_name, X_tr, y_tr, X_te, y_te in configurations:\n    print(\"=\"*80)\n    print(f\"CONFIGURATION: {config_name}\")\n    print(\"=\"*80)\n    print(f\"Training shape: {X_tr.shape}, Test shape: {X_te.shape}\")\n    print()\n    \n    ensemble_classifiers = get_ensemble_classifiers()\n    \n    for model_name, model in ensemble_classifiers.items():\n        print(f\"  Training {model_name}...\", end=' ')\n        \n        metrics = train_and_evaluate(\n            model, X_tr, y_tr, X_te, y_te, model_name, config_name\n        )\n        all_results.append(metrics)\n        \n        print(f\"✓ Acc: {metrics['Accuracy']:.4f} | F1: {metrics['F1-Score']:.4f}\")\n    \n    print()\n\n# ==============================================================================\n# STEP 8: CREATE RESULTS DATAFRAME\n# ==============================================================================\n\nresults_df = pd.DataFrame(all_results)\n\n# ==============================================================================\n# STEP 9: DISPLAY ALL RESULTS SORTED BY ACCURACY\n# ==============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"🏆 ALL RESULTS SORTED BY ACCURACY (HIGHEST TO LOWEST)\")\nprint(\"=\"*80)\nprint()\n\nresults_sorted = results_df.sort_values('Accuracy', ascending=False).reset_index(drop=True)\nresults_sorted.index = results_sorted.index + 1\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.float_format', '{:.4f}'.format)\n\nprint(results_sorted.to_string())\n\n# ==============================================================================\n# STEP 10: TOP PERFORMERS\n# ==============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"🥇 TOP 10 MODEL-CONFIGURATION COMBINATIONS\")\nprint(\"=\"*80)\nprint()\nprint(results_sorted.head(10).to_string())\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"🥈 TOP 5 BASE MODELS\")\nprint(\"=\"*80)\nprint()\nbase_models = results_sorted[results_sorted['Model_Type'] == 'Base'].head(5)\nprint(base_models.to_string())\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"🥉 TOP 3 ENSEMBLE MODELS\")\nprint(\"=\"*80)\nprint()\nensemble_models = results_sorted[results_sorted['Model_Type'] == 'Ensemble'].head(3)\nprint(ensemble_models.to_string())\n\n# ==============================================================================\n# STEP 11: BEST MODEL PER CONFIGURATION\n# ==============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"📊 BEST MODEL PER CONFIGURATION\")\nprint(\"=\"*80)\n\nfor config in configurations:\n    config_name = config[0]\n    config_results = results_df[results_df['Configuration'] == config_name]\n    best_model = config_results.loc[config_results['Accuracy'].idxmax()]\n    print(f\"\\n{config_name}:\")\n    print(f\"  Model:      {best_model['Model']} ({best_model['Model_Type']})\")\n    print(f\"  Accuracy:   {best_model['Accuracy']:.4f}\")\n    print(f\"  Precision:  {best_model['Precision']:.4f}\")\n    print(f\"  Recall:     {best_model['Recall']:.4f}\")\n    print(f\"  F1-Score:   {best_model['F1-Score']:.4f}\")\n    print(f\"  ROC-AUC:    {best_model['ROC-AUC']}\")\n\n# ==============================================================================\n# STEP 12: BEST CONFIGURATION PER MODEL\n# ==============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"🎯 BEST CONFIGURATION PER MODEL\")\nprint(\"=\"*80)\n\nall_models = results_df['Model'].unique()\nfor model_name in sorted(all_models):\n    model_results = results_df[results_df['Model'] == model_name]\n    best_config = model_results.loc[model_results['Accuracy'].idxmax()]\n    model_type = best_config['Model_Type']\n    print(f\"\\n{model_name} ({model_type}):\")\n    print(f\"  Best Config:  {best_config['Configuration']}\")\n    print(f\"  Accuracy:     {best_config['Accuracy']:.4f}\")\n    print(f\"  F1-Score:     {best_config['F1-Score']:.4f}\")\n    print(f\"  ROC-AUC:      {best_config['ROC-AUC']}\")\n\n# ==============================================================================\n# STEP 13: OVERALL BEST MODEL\n# ==============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"🌟 OVERALL BEST MODEL-CONFIGURATION COMBINATION\")\nprint(\"=\"*80)\nbest_overall = results_df.loc[results_df['Accuracy'].idxmax()]\nprint(f\"\\nModel:         {best_overall['Model']}\")\nprint(f\"Type:          {best_overall['Model_Type']}\")\nprint(f\"Configuration: {best_overall['Configuration']}\")\nprint(f\"Accuracy:      {best_overall['Accuracy']:.4f}\")\nprint(f\"Precision:     {best_overall['Precision']:.4f}\")\nprint(f\"Recall:        {best_overall['Recall']:.4f}\")\nprint(f\"F1-Score:      {best_overall['F1-Score']:.4f}\")\nprint(f\"ROC-AUC:       {best_overall['ROC-AUC']}\")\n\n# ==============================================================================\n# STEP 14: COMPARISON - BASE VS ENSEMBLE\n# ==============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"⚖️ BASE MODELS VS ENSEMBLE MODELS COMPARISON\")\nprint(\"=\"*80)\n\nbase_avg = results_df[results_df['Model_Type'] == 'Base']['Accuracy'].mean()\nensemble_avg = results_df[results_df['Model_Type'] == 'Ensemble']['Accuracy'].mean()\n\nprint(f\"\\nBase Models Average Accuracy:     {base_avg:.4f}\")\nprint(f\"Ensemble Models Average Accuracy: {ensemble_avg:.4f}\")\nprint(f\"Difference:                       {ensemble_avg - base_avg:+.4f}\")\n\nif ensemble_avg > base_avg:\n    print(f\"\\n✓ Ensemble models perform {((ensemble_avg/base_avg - 1) * 100):.2f}% better on average\")\nelse:\n    print(f\"\\n✗ Base models perform {((base_avg/ensemble_avg - 1) * 100):.2f}% better on average\")\n\n# ==============================================================================\n# STEP 15: PIVOT TABLES\n# ==============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"📈 PIVOT TABLE: ACCURACY BY MODEL AND CONFIGURATION\")\nprint(\"=\"*80)\npivot_accuracy = results_df.pivot_table(\n    index='Model', \n    columns='Configuration', \n    values='Accuracy',\n    aggfunc='first'\n)\nprint(pivot_accuracy.to_string())\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"📈 PIVOT TABLE: F1-SCORE BY MODEL AND CONFIGURATION\")\nprint(\"=\"*80)\npivot_f1 = results_df.pivot_table(\n    index='Model', \n    columns='Configuration', \n    values='F1-Score',\n    aggfunc='first'\n)\nprint(pivot_f1.to_string())\n\n# ==============================================================================\n# STEP 16: SUMMARY STATISTICS\n# ==============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"📊 SUMMARY STATISTICS\")\nprint(\"=\"*80)\nprint(f\"\\nTotal Experiments:        {len(results_df)}\")\nprint(f\"Base Models:              {len(results_df[results_df['Model_Type'] == 'Base'])}\")\nprint(f\"Ensemble Models:          {len(results_df[results_df['Model_Type'] == 'Ensemble'])}\")\nprint(f\"Number of Configurations: {len(configurations)}\")\nprint(f\"Number of Model Types:    {results_df['Model'].nunique()}\")\n\nprint(f\"\\nOverall Accuracy Statistics:\")\nprint(f\"  Mean:    {results_df['Accuracy'].mean():.4f}\")\nprint(f\"  Median:  {results_df['Accuracy'].median():.4f}\")\nprint(f\"  Std Dev: {results_df['Accuracy'].std():.4f}\")\nprint(f\"  Min:     {results_df['Accuracy'].min():.4f}\")\nprint(f\"  Max:     {results_df['Accuracy'].max():.4f}\")\n\n# ==============================================================================\n# STEP 17: EXPORT RESULTS\n# ==============================================================================\n\nresults_df.to_csv('complete_model_comparison_results.csv', index=False)\nresults_sorted.to_csv('complete_model_comparison_sorted.csv', index=False)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"💾 RESULTS EXPORTED\")\nprint(\"=\"*80)\nprint(\"  ✓ complete_model_comparison_results.csv\")\nprint(\"  ✓ complete_model_comparison_sorted.csv\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:06:24.032752Z","iopub.execute_input":"2025-10-06T14:06:24.033072Z","iopub.status.idle":"2025-10-06T14:07:08.551992Z","shell.execute_reply.started":"2025-10-06T14:06:24.033050Z","shell.execute_reply":"2025-10-06T14:07:08.550987Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nPREPARING ALL CONFIGURATIONS\n================================================================================\n1. Boruta + SMOTE+Tomek...\n2. PowerTransformer + SMOTE...\n3. MI + SMOTE+Tomek...\n4. MI only...\n5. LDA...\n\n✓ All configurations prepared!\n\n================================================================================\nPHASE 1: TRAINING BASE MODELS\n================================================================================\n\n================================================================================\nCONFIGURATION: Boruta + SMOTE+Tomek\n================================================================================\nTraining shape: (519, 10), Test shape: (75, 10)\n\n  Training MLPClassifier... ✓ Acc: 0.9467 | F1: 0.9484\n  Training SVC... ✓ Acc: 0.9333 | F1: 0.9341\n  Training AdaBoostClassifier... ✓ Acc: 0.9067 | F1: 0.9062\n  Training RandomForestClassifier... ✓ Acc: 0.9600 | F1: 0.9599\n  Training ExtraTreesClassifier... ✓ Acc: 0.9333 | F1: 0.9333\n  Training GradientBoostingClassifier... ✓ Acc: 0.9600 | F1: 0.9599\n  Training XGBClassifier... ✓ Acc: 0.9333 | F1: 0.9333\n  Training LGBMClassifier... ✓ Acc: 0.9333 | F1: 0.9338\n  Training DecisionTreeClassifier... ✓ Acc: 0.9200 | F1: 0.9213\n\n================================================================================\nCONFIGURATION: PowerTransformer + SMOTE\n================================================================================\nTraining shape: (525, 24), Test shape: (75, 24)\n\n  Training MLPClassifier... ✓ Acc: 0.9200 | F1: 0.9213\n  Training SVC... ✓ Acc: 0.9467 | F1: 0.9474\n  Training AdaBoostClassifier... ✓ Acc: 0.9600 | F1: 0.9595\n  Training RandomForestClassifier... ✓ Acc: 0.9600 | F1: 0.9606\n  Training ExtraTreesClassifier... ✓ Acc: 0.9333 | F1: 0.9345\n  Training GradientBoostingClassifier... ✓ Acc: 0.9467 | F1: 0.9472\n  Training XGBClassifier... ✓ Acc: 0.9333 | F1: 0.9324\n  Training LGBMClassifier... ✓ Acc: 0.9333 | F1: 0.9345\n  Training DecisionTreeClassifier... ✓ Acc: 0.8933 | F1: 0.8968\n\n================================================================================\nCONFIGURATION: MI + SMOTE+Tomek\n================================================================================\nTraining shape: (523, 5), Test shape: (75, 5)\n\n  Training MLPClassifier... ✓ Acc: 0.9733 | F1: 0.9735\n  Training SVC... ✓ Acc: 0.9333 | F1: 0.9341\n  Training AdaBoostClassifier... ✓ Acc: 0.9067 | F1: 0.9102\n  Training RandomForestClassifier... ✓ Acc: 0.9733 | F1: 0.9732\n  Training ExtraTreesClassifier... ✓ Acc: 0.9733 | F1: 0.9732\n  Training GradientBoostingClassifier... ✓ Acc: 0.9733 | F1: 0.9732\n  Training XGBClassifier... ✓ Acc: 0.9600 | F1: 0.9600\n  Training LGBMClassifier... ✓ Acc: 0.9733 | F1: 0.9732\n  Training DecisionTreeClassifier... ✓ Acc: 0.9467 | F1: 0.9472\n\n================================================================================\nCONFIGURATION: MI\n================================================================================\nTraining shape: (299, 5), Test shape: (75, 5)\n\n  Training MLPClassifier... ✓ Acc: 0.9733 | F1: 0.9733\n  Training SVC... ✓ Acc: 0.9333 | F1: 0.9343\n  Training AdaBoostClassifier... ✓ Acc: 0.9467 | F1: 0.9463\n  Training RandomForestClassifier... ✓ Acc: 0.9733 | F1: 0.9733\n  Training ExtraTreesClassifier... ✓ Acc: 0.9600 | F1: 0.9599\n  Training GradientBoostingClassifier... ✓ Acc: 0.9600 | F1: 0.9599\n  Training XGBClassifier... ✓ Acc: 0.9467 | F1: 0.9466\n  Training LGBMClassifier... ✓ Acc: 0.9733 | F1: 0.9733\n  Training DecisionTreeClassifier... ✓ Acc: 0.9733 | F1: 0.9733\n\n================================================================================\nCONFIGURATION: LDA\n================================================================================\nTraining shape: (299, 2), Test shape: (75, 2)\n\n  Training MLPClassifier... ✓ Acc: 0.9333 | F1: 0.9354\n  Training SVC... ✓ Acc: 0.9333 | F1: 0.9354\n  Training AdaBoostClassifier... ✓ Acc: 0.9467 | F1: 0.9476\n  Training RandomForestClassifier... ✓ Acc: 0.9733 | F1: 0.9733\n  Training ExtraTreesClassifier... ✓ Acc: 0.9733 | F1: 0.9733\n  Training GradientBoostingClassifier... ✓ Acc: 0.9600 | F1: 0.9599\n  Training XGBClassifier... ✓ Acc: 0.9733 | F1: 0.9733\n  Training LGBMClassifier... ✓ Acc: 0.9867 | F1: 0.9867\n  Training DecisionTreeClassifier... ✓ Acc: 0.9733 | F1: 0.9733\n\n\n================================================================================\nPHASE 2: TRAINING ENSEMBLE MODELS\n================================================================================\n\n================================================================================\nCONFIGURATION: Boruta + SMOTE+Tomek\n================================================================================\nTraining shape: (519, 10), Test shape: (75, 10)\n\n  Training StackingClassifier... ✓ Acc: 0.9200 | F1: 0.9210\n  Training VotingClassifier... ✓ Acc: 0.9333 | F1: 0.9333\n  Training BaggingEnsemble... ✓ Acc: 0.9600 | F1: 0.9599\n\n================================================================================\nCONFIGURATION: PowerTransformer + SMOTE\n================================================================================\nTraining shape: (525, 24), Test shape: (75, 24)\n\n  Training StackingClassifier... ✓ Acc: 0.9600 | F1: 0.9605\n  Training VotingClassifier... ✓ Acc: 0.9333 | F1: 0.9345\n  Training BaggingEnsemble... ✓ Acc: 0.9600 | F1: 0.9605\n\n================================================================================\nCONFIGURATION: MI + SMOTE+Tomek\n================================================================================\nTraining shape: (523, 5), Test shape: (75, 5)\n\n  Training StackingClassifier... ✓ Acc: 0.9733 | F1: 0.9732\n  Training VotingClassifier... ✓ Acc: 0.9733 | F1: 0.9732\n  Training BaggingEnsemble... ✓ Acc: 0.9600 | F1: 0.9595\n\n================================================================================\nCONFIGURATION: MI\n================================================================================\nTraining shape: (299, 5), Test shape: (75, 5)\n\n  Training StackingClassifier... ✓ Acc: 0.9333 | F1: 0.9332\n  Training VotingClassifier... ✓ Acc: 0.9600 | F1: 0.9599\n  Training BaggingEnsemble... ✓ Acc: 0.9467 | F1: 0.9463\n\n================================================================================\nCONFIGURATION: LDA\n================================================================================\nTraining shape: (299, 2), Test shape: (75, 2)\n\n  Training StackingClassifier... ✓ Acc: 0.9467 | F1: 0.9467\n  Training VotingClassifier... ✓ Acc: 0.9733 | F1: 0.9733\n  Training BaggingEnsemble... ✓ Acc: 0.9467 | F1: 0.9461\n\n\n================================================================================\n🏆 ALL RESULTS SORTED BY ACCURACY (HIGHEST TO LOWEST)\n================================================================================\n\n    Accuracy  Precision  Recall  F1-Score  ROC-AUC                       Model             Configuration Model_Type\n1     0.9867     0.9875  0.9867    0.9867   0.9993              LGBMClassifier                       LDA       Base\n2     0.9733     0.9733  0.9733    0.9733   0.9969      RandomForestClassifier                        MI       Base\n3     0.9733     0.9733  0.9733    0.9733   0.9924              LGBMClassifier                        MI       Base\n4     0.9733     0.9733  0.9733    0.9733   0.9778      DecisionTreeClassifier                        MI       Base\n5     0.9733     0.9733  0.9733    0.9733   0.9980      RandomForestClassifier                       LDA       Base\n6     0.9733     0.9765  0.9733    0.9733   0.9982               MLPClassifier                        MI       Base\n7     0.9733     0.9733  0.9733    0.9733   0.9768        ExtraTreesClassifier                       LDA       Base\n8     0.9733     0.9763  0.9733    0.9732   0.9975              LGBMClassifier          MI + SMOTE+Tomek       Base\n9     0.9733     0.9763  0.9733    0.9732   0.9922  GradientBoostingClassifier          MI + SMOTE+Tomek       Base\n10    0.9733     0.9763  0.9733    0.9732   0.9876        ExtraTreesClassifier          MI + SMOTE+Tomek       Base\n11    0.9733     0.9763  0.9733    0.9732   0.9971      RandomForestClassifier          MI + SMOTE+Tomek       Base\n12    0.9733     0.9733  0.9733    0.9733   0.9883               XGBClassifier                       LDA       Base\n13    0.9733     0.9741  0.9733    0.9735   0.9998               MLPClassifier          MI + SMOTE+Tomek       Base\n14    0.9733     0.9733  0.9733    0.9733   0.9777      DecisionTreeClassifier                       LDA       Base\n15    0.9733     0.9733  0.9733    0.9733   0.9962            VotingClassifier                       LDA   Ensemble\n16    0.9733     0.9763  0.9733    0.9732   0.9967          StackingClassifier          MI + SMOTE+Tomek   Ensemble\n17    0.9733     0.9763  0.9733    0.9732   0.9964            VotingClassifier          MI + SMOTE+Tomek   Ensemble\n18    0.9600     0.9616  0.9600    0.9605   0.9953          StackingClassifier  PowerTransformer + SMOTE   Ensemble\n19    0.9600     0.9606  0.9600    0.9599   0.9982             BaggingEnsemble      Boruta + SMOTE+Tomek   Ensemble\n20    0.9600     0.9606  0.9600    0.9599   0.9765        ExtraTreesClassifier                        MI       Base\n21    0.9600     0.9606  0.9600    0.9599   0.9942      RandomForestClassifier      Boruta + SMOTE+Tomek       Base\n22    0.9600     0.9606  0.9600    0.9599   0.9825  GradientBoostingClassifier      Boruta + SMOTE+Tomek       Base\n23    0.9600     0.9608  0.9600    0.9600   0.9931               XGBClassifier          MI + SMOTE+Tomek       Base\n24    0.9600     0.9606  0.9600    0.9599   0.9951            VotingClassifier                        MI   Ensemble\n25    0.9600     0.9663  0.9600    0.9595   0.9984             BaggingEnsemble          MI + SMOTE+Tomek   Ensemble\n26    0.9600     0.9606  0.9600    0.9599   0.9847  GradientBoostingClassifier                       LDA       Base\n27    0.9600     0.9663  0.9600    0.9595   0.9953          AdaBoostClassifier  PowerTransformer + SMOTE       Base\n28    0.9600     0.9617  0.9600    0.9606   0.9946      RandomForestClassifier  PowerTransformer + SMOTE       Base\n29    0.9600     0.9616  0.9600    0.9605   0.9980             BaggingEnsemble  PowerTransformer + SMOTE   Ensemble\n30    0.9600     0.9606  0.9600    0.9599   0.9870  GradientBoostingClassifier                        MI       Base\n31    0.9467     0.9491  0.9467    0.9463   0.9982             BaggingEnsemble                        MI   Ensemble\n32    0.9467     0.9467  0.9467    0.9467   0.9948          StackingClassifier                       LDA   Ensemble\n33    0.9467     0.9505  0.9467    0.9476   0.9892          AdaBoostClassifier                       LDA       Base\n34    0.9467     0.9573  0.9467    0.9484   0.9865               MLPClassifier      Boruta + SMOTE+Tomek       Base\n35    0.9467     0.9495  0.9467    0.9466   0.9771               XGBClassifier                        MI       Base\n36    0.9467     0.9483  0.9467    0.9472   0.9542      DecisionTreeClassifier          MI + SMOTE+Tomek       Base\n37    0.9467     0.9520  0.9467    0.9474   0.9886                         SVC  PowerTransformer + SMOTE       Base\n38    0.9467     0.9482  0.9467    0.9472   0.9881  GradientBoostingClassifier  PowerTransformer + SMOTE       Base\n39    0.9467     0.9461  0.9467    0.9461   0.9969             BaggingEnsemble                       LDA   Ensemble\n40    0.9467     0.9491  0.9467    0.9463   0.9964          AdaBoostClassifier                        MI       Base\n41    0.9333     0.9358  0.9333    0.9343   0.9946                         SVC                        MI       Base\n42    0.9333     0.9413  0.9333    0.9354   0.9811                         SVC                       LDA       Base\n43    0.9333     0.9341  0.9333    0.9333   0.9768        ExtraTreesClassifier      Boruta + SMOTE+Tomek       Base\n44    0.9333     0.9341  0.9333    0.9333   0.9830               XGBClassifier      Boruta + SMOTE+Tomek       Base\n45    0.9333     0.9363  0.9333    0.9338   0.9890              LGBMClassifier      Boruta + SMOTE+Tomek       Base\n46    0.9333     0.9338  0.9333    0.9332   0.9958          StackingClassifier                        MI   Ensemble\n47    0.9333     0.9413  0.9333    0.9354   0.9775               MLPClassifier                       LDA       Base\n48    0.9333     0.9403  0.9333    0.9345   0.9666        ExtraTreesClassifier  PowerTransformer + SMOTE       Base\n49    0.9333     0.9403  0.9333    0.9345   0.9922            VotingClassifier  PowerTransformer + SMOTE   Ensemble\n50    0.9333     0.9419  0.9333    0.9341   0.9788                         SVC      Boruta + SMOTE+Tomek       Base\n51    0.9333     0.9341  0.9333    0.9333   0.9906            VotingClassifier      Boruta + SMOTE+Tomek   Ensemble\n52    0.9333     0.9403  0.9333    0.9345   0.9868              LGBMClassifier  PowerTransformer + SMOTE       Base\n53    0.9333     0.9419  0.9333    0.9341   0.9895                         SVC          MI + SMOTE+Tomek       Base\n54    0.9333     0.9337  0.9333    0.9324   0.9862               XGBClassifier  PowerTransformer + SMOTE       Base\n55    0.9200     0.9225  0.9200    0.9210   0.9828          StackingClassifier      Boruta + SMOTE+Tomek   Ensemble\n56    0.9200     0.9245  0.9200    0.9213   0.9701               MLPClassifier  PowerTransformer + SMOTE       Base\n57    0.9200     0.9245  0.9200    0.9213   0.9386      DecisionTreeClassifier      Boruta + SMOTE+Tomek       Base\n58    0.9067     0.9178  0.9067    0.9102   0.9898          AdaBoostClassifier          MI + SMOTE+Tomek       Base\n59    0.9067     0.9065  0.9067    0.9062   0.9820          AdaBoostClassifier      Boruta + SMOTE+Tomek       Base\n60    0.8933     0.9074  0.8933    0.8968   0.9193      DecisionTreeClassifier  PowerTransformer + SMOTE       Base\n\n================================================================================\n🥇 TOP 10 MODEL-CONFIGURATION COMBINATIONS\n================================================================================\n\n    Accuracy  Precision  Recall  F1-Score  ROC-AUC                       Model     Configuration Model_Type\n1     0.9867     0.9875  0.9867    0.9867   0.9993              LGBMClassifier               LDA       Base\n2     0.9733     0.9733  0.9733    0.9733   0.9969      RandomForestClassifier                MI       Base\n3     0.9733     0.9733  0.9733    0.9733   0.9924              LGBMClassifier                MI       Base\n4     0.9733     0.9733  0.9733    0.9733   0.9778      DecisionTreeClassifier                MI       Base\n5     0.9733     0.9733  0.9733    0.9733   0.9980      RandomForestClassifier               LDA       Base\n6     0.9733     0.9765  0.9733    0.9733   0.9982               MLPClassifier                MI       Base\n7     0.9733     0.9733  0.9733    0.9733   0.9768        ExtraTreesClassifier               LDA       Base\n8     0.9733     0.9763  0.9733    0.9732   0.9975              LGBMClassifier  MI + SMOTE+Tomek       Base\n9     0.9733     0.9763  0.9733    0.9732   0.9922  GradientBoostingClassifier  MI + SMOTE+Tomek       Base\n10    0.9733     0.9763  0.9733    0.9732   0.9876        ExtraTreesClassifier  MI + SMOTE+Tomek       Base\n\n================================================================================\n🥈 TOP 5 BASE MODELS\n================================================================================\n\n   Accuracy  Precision  Recall  F1-Score  ROC-AUC                   Model Configuration Model_Type\n1    0.9867     0.9875  0.9867    0.9867   0.9993          LGBMClassifier           LDA       Base\n2    0.9733     0.9733  0.9733    0.9733   0.9969  RandomForestClassifier            MI       Base\n3    0.9733     0.9733  0.9733    0.9733   0.9924          LGBMClassifier            MI       Base\n4    0.9733     0.9733  0.9733    0.9733   0.9778  DecisionTreeClassifier            MI       Base\n5    0.9733     0.9733  0.9733    0.9733   0.9980  RandomForestClassifier           LDA       Base\n\n================================================================================\n🥉 TOP 3 ENSEMBLE MODELS\n================================================================================\n\n    Accuracy  Precision  Recall  F1-Score  ROC-AUC               Model     Configuration Model_Type\n15    0.9733     0.9733  0.9733    0.9733   0.9962    VotingClassifier               LDA   Ensemble\n16    0.9733     0.9763  0.9733    0.9732   0.9967  StackingClassifier  MI + SMOTE+Tomek   Ensemble\n17    0.9733     0.9763  0.9733    0.9732   0.9964    VotingClassifier  MI + SMOTE+Tomek   Ensemble\n\n================================================================================\n📊 BEST MODEL PER CONFIGURATION\n================================================================================\n\nBoruta + SMOTE+Tomek:\n  Model:      RandomForestClassifier (Base)\n  Accuracy:   0.9600\n  Precision:  0.9606\n  Recall:     0.9600\n  F1-Score:   0.9599\n  ROC-AUC:    0.9941694915254238\n\nPowerTransformer + SMOTE:\n  Model:      AdaBoostClassifier (Base)\n  Accuracy:   0.9600\n  Precision:  0.9663\n  Recall:     0.9600\n  F1-Score:   0.9595\n  ROC-AUC:    0.9952862523540491\n\nMI + SMOTE+Tomek:\n  Model:      MLPClassifier (Base)\n  Accuracy:   0.9733\n  Precision:  0.9741\n  Recall:     0.9733\n  F1-Score:   0.9735\n  ROC-AUC:    0.9997777777777778\n\nMI:\n  Model:      MLPClassifier (Base)\n  Accuracy:   0.9733\n  Precision:  0.9765\n  Recall:     0.9733\n  F1-Score:   0.9733\n  ROC-AUC:    0.9982033898305086\n\nLDA:\n  Model:      LGBMClassifier (Base)\n  Accuracy:   0.9867\n  Precision:  0.9875\n  Recall:     0.9867\n  F1-Score:   0.9867\n  ROC-AUC:    0.9993276836158193\n\n================================================================================\n🎯 BEST CONFIGURATION PER MODEL\n================================================================================\n\nAdaBoostClassifier (Base):\n  Best Config:  PowerTransformer + SMOTE\n  Accuracy:     0.9600\n  F1-Score:     0.9595\n  ROC-AUC:      0.9952862523540491\n\nBaggingEnsemble (Ensemble):\n  Best Config:  Boruta + SMOTE+Tomek\n  Accuracy:     0.9600\n  F1-Score:     0.9599\n  ROC-AUC:      0.9981996233521657\n\nDecisionTreeClassifier (Base):\n  Best Config:  MI\n  Accuracy:     0.9733\n  F1-Score:     0.9733\n  ROC-AUC:      0.9778116760828626\n\nExtraTreesClassifier (Base):\n  Best Config:  MI + SMOTE+Tomek\n  Accuracy:     0.9733\n  F1-Score:     0.9732\n  ROC-AUC:      0.9876120527306969\n\nGradientBoostingClassifier (Base):\n  Best Config:  MI + SMOTE+Tomek\n  Accuracy:     0.9733\n  F1-Score:     0.9732\n  ROC-AUC:      0.9921883239171374\n\nLGBMClassifier (Base):\n  Best Config:  LDA\n  Accuracy:     0.9867\n  F1-Score:     0.9867\n  ROC-AUC:      0.9993276836158193\n\nMLPClassifier (Base):\n  Best Config:  MI + SMOTE+Tomek\n  Accuracy:     0.9733\n  F1-Score:     0.9735\n  ROC-AUC:      0.9997777777777778\n\nRandomForestClassifier (Base):\n  Best Config:  MI + SMOTE+Tomek\n  Accuracy:     0.9733\n  F1-Score:     0.9732\n  ROC-AUC:      0.9970922787193973\n\nSVC (Base):\n  Best Config:  PowerTransformer + SMOTE\n  Accuracy:     0.9467\n  F1-Score:     0.9474\n  ROC-AUC:      0.9886218334244579\n\nStackingClassifier (Ensemble):\n  Best Config:  MI + SMOTE+Tomek\n  Accuracy:     0.9733\n  F1-Score:     0.9732\n  ROC-AUC:      0.9966516007532956\n\nVotingClassifier (Ensemble):\n  Best Config:  MI + SMOTE+Tomek\n  Accuracy:     0.9733\n  F1-Score:     0.9732\n  ROC-AUC:      0.9964256120527307\n\nXGBClassifier (Base):\n  Best Config:  LDA\n  Accuracy:     0.9733\n  F1-Score:     0.9733\n  ROC-AUC:      0.9883352165725048\n\n================================================================================\n🌟 OVERALL BEST MODEL-CONFIGURATION COMBINATION\n================================================================================\n\nModel:         LGBMClassifier\nType:          Base\nConfiguration: LDA\nAccuracy:      0.9867\nPrecision:     0.9875\nRecall:        0.9867\nF1-Score:      0.9867\nROC-AUC:       0.9993276836158193\n\n================================================================================\n⚖️ BASE MODELS VS ENSEMBLE MODELS COMPARISON\n================================================================================\n\nBase Models Average Accuracy:     0.9502\nEnsemble Models Average Accuracy: 0.9520\nDifference:                       +0.0018\n\n✓ Ensemble models perform 0.19% better on average\n\n================================================================================\n📈 PIVOT TABLE: ACCURACY BY MODEL AND CONFIGURATION\n================================================================================\nConfiguration               Boruta + SMOTE+Tomek    LDA     MI  MI + SMOTE+Tomek  PowerTransformer + SMOTE\nModel                                                                                                     \nAdaBoostClassifier                        0.9067 0.9467 0.9467            0.9067                    0.9600\nBaggingEnsemble                           0.9600 0.9467 0.9467            0.9600                    0.9600\nDecisionTreeClassifier                    0.9200 0.9733 0.9733            0.9467                    0.8933\nExtraTreesClassifier                      0.9333 0.9733 0.9600            0.9733                    0.9333\nGradientBoostingClassifier                0.9600 0.9600 0.9600            0.9733                    0.9467\nLGBMClassifier                            0.9333 0.9867 0.9733            0.9733                    0.9333\nMLPClassifier                             0.9467 0.9333 0.9733            0.9733                    0.9200\nRandomForestClassifier                    0.9600 0.9733 0.9733            0.9733                    0.9600\nSVC                                       0.9333 0.9333 0.9333            0.9333                    0.9467\nStackingClassifier                        0.9200 0.9467 0.9333            0.9733                    0.9600\nVotingClassifier                          0.9333 0.9733 0.9600            0.9733                    0.9333\nXGBClassifier                             0.9333 0.9733 0.9467            0.9600                    0.9333\n\n================================================================================\n📈 PIVOT TABLE: F1-SCORE BY MODEL AND CONFIGURATION\n================================================================================\nConfiguration               Boruta + SMOTE+Tomek    LDA     MI  MI + SMOTE+Tomek  PowerTransformer + SMOTE\nModel                                                                                                     \nAdaBoostClassifier                        0.9062 0.9476 0.9463            0.9102                    0.9595\nBaggingEnsemble                           0.9599 0.9461 0.9463            0.9595                    0.9605\nDecisionTreeClassifier                    0.9213 0.9733 0.9733            0.9472                    0.8968\nExtraTreesClassifier                      0.9333 0.9733 0.9599            0.9732                    0.9345\nGradientBoostingClassifier                0.9599 0.9599 0.9599            0.9732                    0.9472\nLGBMClassifier                            0.9338 0.9867 0.9733            0.9732                    0.9345\nMLPClassifier                             0.9484 0.9354 0.9733            0.9735                    0.9213\nRandomForestClassifier                    0.9599 0.9733 0.9733            0.9732                    0.9606\nSVC                                       0.9341 0.9354 0.9343            0.9341                    0.9474\nStackingClassifier                        0.9210 0.9467 0.9332            0.9732                    0.9605\nVotingClassifier                          0.9333 0.9733 0.9599            0.9732                    0.9345\nXGBClassifier                             0.9333 0.9733 0.9466            0.9600                    0.9324\n\n================================================================================\n📊 SUMMARY STATISTICS\n================================================================================\n\nTotal Experiments:        60\nBase Models:              45\nEnsemble Models:          15\nNumber of Configurations: 5\nNumber of Model Types:    12\n\nOverall Accuracy Statistics:\n  Mean:    0.9507\n  Median:  0.9533\n  Std Dev: 0.0206\n  Min:     0.8933\n  Max:     0.9867\n\n================================================================================\n💾 RESULTS EXPORTED\n================================================================================\n  ✓ complete_model_comparison_results.csv\n  ✓ complete_model_comparison_sorted.csv\n================================================================================\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}