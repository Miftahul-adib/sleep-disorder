{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6491929,"sourceType":"datasetVersion","datasetId":3321433}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installing dependencies","metadata":{}},{"cell_type":"code","source":"!pip install boruta category_encoders xgboost catboost\n!pip uninstall -y scikit-learn imbalanced-learn\n\n!pip install scikit-learn==1.4.2 imbalanced-learn==0.12.0\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, RobustScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV, RandomizedSearchCV\nfrom sklearn.feature_selection import mutual_info_classif, SelectKBest\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler, SMOTENC\nfrom imblearn.combine import SMOTETomek, SMOTEENN\nfrom imblearn.under_sampling import CondensedNearestNeighbour, TomekLinks, RandomUnderSampler\nfrom boruta import BorutaPy\nfrom keras.models import Model, Sequential\nfrom keras.layers import Input, Dense\nfrom keras.optimizers import Adam\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE, BorderlineSMOTE, SVMSMOTE, ADASYN\nfrom imblearn.under_sampling import RandomUnderSampler, ClusterCentroids, NearMiss\nfrom imblearn.under_sampling import TomekLinks\n\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE, BorderlineSMOTE, SVMSMOTE, ADASYN\nfrom imblearn.under_sampling import RandomUnderSampler, ClusterCentroids, NearMiss, TomekLinks\nfrom imblearn.combine import SMOTETomek, SMOTEENN","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T19:07:56.887940Z","iopub.execute_input":"2025-10-11T19:07:56.888369Z","iopub.status.idle":"2025-10-11T19:08:36.376688Z","shell.execute_reply.started":"2025-10-11T19:07:56.888333Z","shell.execute_reply":"2025-10-11T19:08:36.375414Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: boruta in /usr/local/lib/python3.11/dist-packages (0.4.3)\nRequirement already satisfied: category_encoders in /usr/local/lib/python3.11/dist-packages (2.7.0)\nRequirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.0.3)\nRequirement already satisfied: catboost in /usr/local/lib/python3.11/dist-packages (1.2.8)\nRequirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.11/dist-packages (from boruta) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.11/dist-packages (from boruta) (1.2.2)\nRequirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.11/dist-packages (from boruta) (1.15.3)\nRequirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (2.2.3)\nRequirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.0.1)\nRequirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (0.14.5)\nRequirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.21)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.7.2)\nRequirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10.4->boruta) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10.4->boruta) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10.4->boruta) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10.4->boruta) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10.4->boruta) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10.4->boruta) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.17.1->boruta) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.17.1->boruta) (3.6.0)\nRequirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.9.0->category_encoders) (25.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.3.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.0.9)\nRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (8.5.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.10.4->boruta) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.10.4->boruta) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.10.4->boruta) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.10.4->boruta) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.10.4->boruta) (2024.2.0)\nFound existing installation: scikit-learn 1.2.2\nUninstalling scikit-learn-1.2.2:\n  Successfully uninstalled scikit-learn-1.2.2\nFound existing installation: imbalanced-learn 0.13.0\nUninstalling imbalanced-learn-0.13.0:\n  Successfully uninstalled imbalanced-learn-0.13.0\nCollecting scikit-learn==1.4.2\n  Downloading scikit_learn-1.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nCollecting imbalanced-learn==0.12.0\n  Downloading imbalanced_learn-0.12.0-py3-none-any.whl.metadata (8.2 kB)\nRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2) (1.26.4)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2) (1.15.3)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.4.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.4.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.4.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.4.2) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.4.2) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.4.2) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.5->scikit-learn==1.4.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.5->scikit-learn==1.4.2) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.5->scikit-learn==1.4.2) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.5->scikit-learn==1.4.2) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.5->scikit-learn==1.4.2) (2024.2.0)\nDownloading scikit_learn-1.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading imbalanced_learn-0.12.0-py3-none-any.whl (257 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.7/257.7 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: scikit-learn, imbalanced-learn\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.4.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed imbalanced-learn-0.12.0 scikit-learn-1.4.2\n","output_type":"stream"},{"name":"stderr","text":"2025-10-11 19:08:19.004487: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760209699.275792      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760209699.346753      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Data preprocessing\n\n","metadata":{}},{"cell_type":"markdown","source":"### Data Preprocessing & Feature Engineering\n\n- Loaded the **Sleep Health & Lifestyle** dataset and filled missing values with `\"None\"`.\n- Split **Blood Pressure** into numeric `Systolic BP` and `Diastolic BP`, then dropped `Person ID` and the original column.\n- Grouped rare occupations (`Manager`, `Sales Representative`, `Scientist`, `Software Engineer`) into **\"Other\"**.\n- Converted **BMI Category** labels into numeric averages:\n  - `Normal → 22`\n  - `Overweight → 27`\n  - `Obese → 30`\n- Created new interaction and ratio features:\n  - `Stress_sleep_interaction = Stress Level / Quality of Sleep`\n  - `BMI_Activity = BMI Category * Physical Activity Level`\n  - `Sleep_Heart_ratio = Sleep Duration / Heart Rate`\n  - `Sleep_Steps_ratio = Sleep Duration / Daily Steps`\n  - `Sleep_Stress_ratio = Sleep Duration / Stress Level`\n- One-hot encoded **Occupation** and label-encoded **Gender** and **Sleep Disorder**.\n- Defined:\n  - `X` = all feature columns\n  - `y` = target (`Sleep Disorder`)\n- Performed an **80/20 stratified train-test split** using `random_state=42` for reproducibility.\n","metadata":{}},{"cell_type":"code","source":"# ==============================\n# Combined Feature Engineering Pipeline\n# ==============================\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n\n\n\ndf = pd.read_csv(\"/kaggle/input/sleep-health-and-lifestyle-dataset/Sleep_health_and_lifestyle_dataset.csv\")\n\n\ndf.fillna(\"None\", inplace=True)\n\n# Dividing Blood Pressure into Systolic and Diastolic BP\ndf[['Systolic BP', 'Diastolic BP']] = df['Blood Pressure'].str.split('/', expand=True).astype(int)\ndf.drop(['Person ID', 'Blood Pressure'], axis=1, inplace=True)\n\n# Labeling less number of careers as other\ndf['Occupation'] = df['Occupation'].replace(['Manager', 'Sales Representative', 'Scientist', 'Software Engineer'], 'Other')\n\n# Adding the average BMI for the range\ndf['BMI Category'] = df['BMI Category'].replace({'Normal':22, 'Normal Weight':22, 'Overweight':27, 'Obese':30})\n\n# Creating Interaction features\ndf['Stress_sleep_interaction'] = df['Stress Level'] / df['Quality of Sleep']\ndf['BMI_Activity'] = df['BMI Category'] * df['Physical Activity Level']\ndf['Sleep_Heart_ratio'] = df['Sleep Duration'] / df['Heart Rate']\ndf['Sleep_Steps_ratio'] = df['Sleep Duration'] / df['Daily Steps']\ndf['Sleep_Stress_ratio'] = df['Sleep Duration'] / df['Stress Level']\n\ndf = pd.get_dummies(df, columns=['Occupation'], drop_first=False)\n\nlabel_encoder = LabelEncoder()\ncolumns = ['Gender', 'Sleep Disorder']\nfor col in columns:\n  df[col] = label_encoder.fit_transform(df[col])\n\n\n\n\nX = df.drop('Sleep Disorder', axis=1)\ny = df['Sleep Disorder']\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T19:08:36.378549Z","iopub.execute_input":"2025-10-11T19:08:36.379107Z","iopub.status.idle":"2025-10-11T19:08:36.454508Z","shell.execute_reply.started":"2025-10-11T19:08:36.379081Z","shell.execute_reply":"2025-10-11T19:08:36.453140Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Preparing Pipeline \n\n**Prereqs:** `X_train, X_test, y_train, y_test`\n\n---\n\n### 1) Boruta → AE → SMOTE+Tomek\n- Scale: `MinMaxScaler()`\n- Boruta: `RandomForestClassifier(n_estimators=100, random_state=42)` + `BorutaPy(n_estimators='auto', verbose=0, random_state=42)`\n- Autoencoder: Dense `32 → 16(bottleneck) → 32 → n_features`, `Adam(lr=0.001)`, `loss='mse'`, `epochs=10`, `batch_size=32`, `verbose=0`\n- Balance: `SMOTETomek(random_state=42)`\n\n---\n\n### 2) PowerTransformer → SMOTE\n- Transform: `PowerTransformer(method='yeo-johnson')`\n- Balance: `SMOTE(random_state=42, k_neighbors=5)`\n\n---\n\n### 3) RobustScaler → MI(k=10) → SMOTE+Tomek\n- Scale: `RobustScaler()`\n- Select: `SelectKBest(mutual_info_classif, k=10)`\n- Balance: `SMOTETomek(random_state=42)`\n\n---\n\n### 4) MI(k=5) → LDA\n- Select: `SelectKBest(mutual_info_classif, k=5)`\n- Reduce: `LinearDiscriminantAnalysis(n_components=2)`\n\n---\n\n### 5) MI only\n- Select: `SelectKBest(mutual_info_classif, k=10)`\n\n**Note:** Resampling applies to **train** only; test is copied unchanged.\n\n","metadata":{}},{"cell_type":"code","source":"# ==============================================================\n# IMPORTS (no logic changes, just what's needed)\n# ==============================================================\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler, PowerTransformer\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.over_sampling import SMOTE\n\nfrom boruta import BorutaPy\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.optimizers import Adam\n\n\n# ==============================================================\n# CONFIG 1 — Boruta + SMOTE + Tomek\n# (keeps your variables: X_train_boruta_smotetomek, y_train_boruta_smotetomek, etc.)\n# ==============================================================\n\n# Normalize for Boruta (RF works fine without, but you used it—kept as-is)\nscaler = MinMaxScaler()\nX_train_normalized = scaler.fit_transform(X_train)\nX_test_normalized  = scaler.transform(X_test)\n\n# Boruta with RandomForest\nrfc = RandomForestClassifier(n_estimators=100, random_state=42)\nboruta_selector = BorutaPy(rfc, n_estimators='auto', verbose=0, random_state=42)\nX_train_boruta = boruta_selector.fit_transform(X_train_normalized, y_train)\nX_test_boruta  = boruta_selector.transform(X_test_normalized)\n\n# Autoencoder (exact as you had it)\nn_features = X_train_boruta.shape[1]\ninput_layer = Input(shape=(n_features,))\nencoded     = Dense(32, activation='relu')(input_layer)\nbottleneck  = Dense(16, activation='relu')(encoded)\ndecoded     = Dense(32, activation='relu')(bottleneck)\noutput_layer= Dense(n_features, activation='sigmoid')(decoded)\n\nautoencoder = Model(inputs=input_layer, outputs=output_layer)\nautoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\nautoencoder.fit(X_train_boruta, X_train_boruta, epochs=10, batch_size=32, verbose=0)\n\n# Encoder-only transform (you created these; keeping intact)\nencoder         = Model(inputs=input_layer, outputs=bottleneck)\nX_train_encoded = encoder.predict(X_train_boruta)\nX_test_encoded  = encoder.predict(X_test_boruta)\n\n# SMOTE + Tomek on Boruta features (final names kept)\nsmotetomek_boruta = SMOTETomek(random_state=42)\nX_train_boruta_smotetomek, y_train_boruta_smotetomek = smotetomek_boruta.fit_resample(X_train_boruta, y_train)\nX_test_boruta_smotetomek = X_test_boruta.copy()\ny_test_boruta_smotetomek = y_test.copy()\n\n\n# ==============================================================\n# CONFIG 2 — PowerTransformer + SMOTE\n# (keeps your variables: X_train_power_smote, y_train_power_smote, etc.)\n# ==============================================================\n\nscaler = PowerTransformer(method='yeo-johnson')\nX_train_power = scaler.fit_transform(X_train)\nX_test_power  = scaler.transform(X_test)\n\nsmote_power = SMOTE(random_state=42, k_neighbors=5)\nX_train_power_smote, y_train_power_smote = smote_power.fit_resample(X_train_power, y_train)\nX_test_power_smote = X_test_power.copy()\ny_test_power_smote = y_test.copy()\n\n\n# ==============================================================\n# CONFIG 3 — MI + SMOTE + Tomek\n# (keeps your variables: X_train_mi_smotetomek, y_train_mi_smotetomek, etc.)\n# ==============================================================\n\n# Robust scaling before MI (as you did)\nscaler = RobustScaler()\nX_train_robust = scaler.fit_transform(X_train)\nX_test_robust  = scaler.transform(X_test)\n\n# Mutual Information selection (k=5) — names kept\nmi = SelectKBest(score_func=mutual_info_classif, k=10)\nX_train_mi = mi.fit_transform(X_train_robust, y_train)\nX_test_mi  = mi.transform(X_test_robust)\n\n# SMOTE + Tomek over MI features — final names kept\nsmotetomek_mi = SMOTETomek(random_state=42)\nX_train_mi_smotetomek, y_train_mi_smotetomek = smotetomek_mi.fit_resample(X_train_mi, y_train)\nX_test_mi_smotetomek = X_test_mi.copy()\ny_test_mi_smotetomek = y_test.copy()\n\n\n# ==============================================================\n# CONFIG 4 — LDA\n# (keeps your variables: X_train_lda, X_test_lda, y_train_lda, y_test_lda)\n# ==============================================================\nmi = SelectKBest(score_func=mutual_info_classif, k=5)\nX_train_mi = mi.fit_transform(X_train_robust, y_train)\nX_test_mi  = mi.transform(X_test_robust)\n\n\nlda = LinearDiscriminantAnalysis(n_components=2)\nX_train_lda = lda.fit_transform(X_train_mi, y_train)  # uses MI features as you wrote\nX_test_lda  = lda.transform(X_test_mi)\ny_train_lda = y_train.copy()\ny_test_lda  = y_test.copy()\n\n\n# ==============================================================\n# CONFIG 5 — MI (no resampling)\n# (keeps your variables: X_train_mi, X_test_mi, y_train_mi, y_test_mi)\n# ==============================================================\n\n# Re-apply MI exactly as you had it (duplicated in your script—left intact)\nmi = SelectKBest(score_func=mutual_info_classif, k=10)\nX_train_mi = mi.fit_transform(X_train_robust, y_train)\nX_test_mi  = mi.transform(X_test_robust)\n\n\n# Final MI-only targets (kept exactly)\ny_train_mi = y_train.copy()\ny_test_mi  = y_test.copy()\n\n\n# ==============================================================\nprint(\"All configurations prepared!\\n\")\n# ==============================================================\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T19:08:36.455675Z","iopub.execute_input":"2025-10-11T19:08:36.455999Z","iopub.status.idle":"2025-10-11T19:08:49.831236Z","shell.execute_reply.started":"2025-10-11T19:08:36.455974Z","shell.execute_reply":"2025-10-11T19:08:49.829967Z"}},"outputs":[{"name":"stderr","text":"2025-10-11 19:08:45.922100: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\nAll configurations prepared!\n\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Base & Ensemble Training and evaluation\n\n### 0) Setup\n- Imports ML libs; suppresses warnings.\n- Assumes prebuilt configs: **Boruta+SMOTETomek**, **Power+SMOTE**, **MI+SMOTETomek**, **MI**, **LDA**.\n\n---\n\n### 1) Base Models (`get_base_classifiers`)\n- **MLPClassifier** `(random_state=42, max_iter=1000)`\n- **SVC** `(random_state=42, probability=True)` → enables ROC-AUC\n- **AdaBoostClassifier** `(random_state=42, algorithm='SAMME')`\n- **RandomForestClassifier** `(random_state=42, n_estimators=100)`\n- **ExtraTreesClassifier** `(random_state=42, n_estimators=100)`\n- **GradientBoostingClassifier** `(random_state=42)`\n- **XGBClassifier** `(random_state=42, eval_metric='logloss', use_label_encoder=False)`\n- **LGBMClassifier** `(random_state=42, verbose=-1)`\n- **DecisionTreeClassifier** `(random_state=42)`\n\n---\n\n### 2) Ensembles (`get_ensemble_classifiers`)\n**Base estimators used across ensembles**\n- `XGBClassifier(random_state=42, eval_metric='logloss', n_jobs=-1)`\n- `LGBMClassifier(random_state=42, verbose=-1, n_jobs=-1)`\n- `ExtraTreesClassifier(random_state=42, n_estimators=100, n_jobs=-1)`\n- `GradientBoostingClassifier(random_state=42, n_estimators=100)`\n- `SVC(kernel='rbf', C=1, gamma='scale', probability=True, random_state=42)`\n\n**StackingClassifier**\n- `estimators=base_estimators`\n- `final_estimator=LogisticRegression(random_state=42, max_iter=2000, solver='lbfgs', n_jobs=-1)`\n- `cv=5`, `n_jobs=-1`, `passthrough=True`\n\n**VotingClassifier (soft)**\n- `estimators=base_estimators`, `voting='soft'`, `n_jobs=-1`\n\n**BaggingClassifier**\n- `estimator=DecisionTreeClassifier(max_depth=5, min_samples_split=4, random_state=42)`\n- `n_estimators=50`, `max_samples=0.8`, `max_features=0.8`, `bootstrap=True`, `n_jobs=-1`, `random_state=42`\n\n---\n\n### 3) Metrics (`calculate_metrics`)\n- **Accuracy**, **Precision/Recall/F1 (weighted, `zero_division=0`)**\n- **ROC-AUC**: binary → `proba[:,1]`; multiclass → `ovr`, `average='weighted'`; if missing → `'N/A'`.\n\n---\n\n### 4) Training/Eval (`train_and_evaluate`)\n- `fit → predict → (try) predict_proba`\n- Returns dict with metrics + `Model`, `Configuration`, `Model_Type=('Base'|'Ensemble')`.\n- On error: zeros and `'Error'`.\n\n---\n\n### 5) Experiment Grid\n- Loops **all base** then **all ensemble** models over each configuration.\n- Collects `all_results` → `results_df`.\n\n---\n\n### 6) Reporting & Export\n- Sort by **Accuracy**; print **Top-10**, **Top-5 Base**, **Top-3 Ensemble**.\n- **Best per configuration** and **best config per model**.\n- **Overall best** combo (by Accuracy).\n- **Base vs Ensemble** average Accuracy + % difference.\n- **Pivots**: Accuracy and F1 (Model × Configuration).\n- **Summary stats**: counts + Accuracy mean/median/std/min/max.\n- Saves:\n  - `complete_model_comparison_results.csv`\n  - `complete_model_comparison_sorted.csv`\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import (AdaBoostClassifier, RandomForestClassifier, \n                               ExtraTreesClassifier, GradientBoostingClassifier,\n                               StackingClassifier, VotingClassifier, BaggingClassifier)\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (accuracy_score, precision_score, recall_score, \n                              f1_score, roc_auc_score)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PowerTransformer, RobustScaler\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.over_sampling import SMOTE\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n# ==============================================================================\n# STEP 2: DEFINE BASE CLASSIFIERS\n# ==============================================================================\n\ndef get_base_classifiers():\n    \"\"\"Returns dictionary of base classifiers\"\"\"\n    return {\n        'MLPClassifier': MLPClassifier(random_state=42, max_iter=1000),\n        'SVC': SVC(random_state=42, probability=True),\n        'AdaBoostClassifier': AdaBoostClassifier(random_state=42, algorithm='SAMME'),\n        'RandomForestClassifier': RandomForestClassifier(random_state=42, n_estimators=100),\n        'ExtraTreesClassifier': ExtraTreesClassifier(random_state=42, n_estimators=100),\n        'GradientBoostingClassifier': GradientBoostingClassifier(random_state=42),\n        'XGBClassifier': XGBClassifier(random_state=42, eval_metric='logloss', use_label_encoder=False),\n        'LGBMClassifier': LGBMClassifier(random_state=42, verbose=-1),\n        'DecisionTreeClassifier': DecisionTreeClassifier(random_state=42)\n    }\n\n# ==============================================================================\n# STEP 3: DEFINE ENSEMBLE CLASSIFIERS\n# ==============================================================================\n\ndef get_ensemble_classifiers():\n    \"\"\"Returns dictionary of ensemble classifiers using improved base estimators\"\"\"\n\n    # Diverse base estimators for ensembles\n    base_estimators = [\n        ('xgb', XGBClassifier(random_state=42, eval_metric='logloss', use_label_encoder=False, n_jobs=-1)),\n        ('lgbm', LGBMClassifier(random_state=42, verbose=-1, n_jobs=-1)),\n        ('et', ExtraTreesClassifier(random_state=42, n_estimators=100, n_jobs=-1)),\n        ('gb', GradientBoostingClassifier(random_state=42, n_estimators=100)),\n        ('svc', SVC(probability=True, kernel='rbf', C=1, gamma='scale', random_state=42))\n    ]\n\n    # Stacking Classifier with Logistic Regression meta-model\n    stacking = StackingClassifier(\n        estimators=base_estimators,\n        final_estimator=LogisticRegression(\n            random_state=42,\n            max_iter=2000,\n            solver='lbfgs',\n            n_jobs=-1\n        ),\n        cv=5,\n        n_jobs=-1,\n        passthrough=True\n    )\n\n    # Voting Classifier (Soft voting for probability averaging)\n    voting = VotingClassifier(\n        estimators=base_estimators,\n        voting='soft',\n        n_jobs=-1\n    )\n\n    # Bagging Ensemble using shallow Decision Trees (better for bagging)\n    bagging = BaggingClassifier(\n        estimator=DecisionTreeClassifier(\n            max_depth=5,\n            min_samples_split=4,\n            random_state=42\n        ),\n        n_estimators=50,\n        max_samples=0.8,\n        max_features=0.8,\n        bootstrap=True,\n        n_jobs=-1,\n        random_state=42\n    )\n\n    return {\n        'StackingClassifier': stacking,\n        'VotingClassifier': voting,\n        'BaggingEnsemble': bagging\n    }\n\n\n# ==============================================================================\n# STEP 4: HELPER FUNCTIONS\n# ==============================================================================\n\ndef calculate_metrics(y_true, y_pred, y_pred_proba=None):\n    \"\"\"Calculate classification metrics\"\"\"\n    metrics = {\n        'Accuracy': accuracy_score(y_true, y_pred),\n        'Precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),\n        'Recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),\n        'F1-Score': f1_score(y_true, y_pred, average='weighted', zero_division=0)\n    }\n    \n    if y_pred_proba is not None:\n        try:\n            if len(np.unique(y_true)) == 2:\n                metrics['ROC-AUC'] = roc_auc_score(y_true, y_pred_proba[:, 1])\n            else:\n                metrics['ROC-AUC'] = roc_auc_score(y_true, y_pred_proba, \n                                                    multi_class='ovr', average='weighted')\n        except:\n            metrics['ROC-AUC'] = 'N/A'\n    else:\n        metrics['ROC-AUC'] = 'N/A'\n    \n    return metrics\n\ndef train_and_evaluate(model, X_train, y_train, X_test, y_test, model_name, config_name):\n    \"\"\"Train model and return metrics\"\"\"\n    try:\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        \n        try:\n            y_pred_proba = model.predict_proba(X_test)\n        except:\n            y_pred_proba = None\n        \n        metrics = calculate_metrics(y_test, y_pred, y_pred_proba)\n        metrics['Model'] = model_name\n        metrics['Configuration'] = config_name\n        metrics['Model_Type'] = 'Ensemble' if model_name in ['StackingClassifier', 'VotingClassifier', 'BaggingEnsemble'] else 'Base'\n        \n        return metrics\n    \n    except Exception as e:\n        print(f\"  ❌ Error: {str(e)}\")\n        return {\n            'Model': model_name,\n            'Configuration': config_name,\n            'Model_Type': 'Ensemble' if model_name in ['StackingClassifier', 'VotingClassifier', 'BaggingEnsemble'] else 'Base',\n            'Accuracy': 0.0,\n            'Precision': 0.0,\n            'Recall': 0.0,\n            'F1-Score': 0.0,\n            'ROC-AUC': 'Error'\n        }\n\n# ==============================================================================\n# STEP 5: PREPARE CONFIGURATIONS LIST\n# ==============================================================================\n\nconfigurations = [\n    ('Boruta + SMOTE+Tomek',\n     X_train_boruta_smotetomek, y_train_boruta_smotetomek,\n     X_test_boruta_smotetomek, y_test_boruta_smotetomek),\n\n    ('PowerTransformer + SMOTE',\n     X_train_power_smote, y_train_power_smote,\n     X_test_power_smote, y_test_power_smote),\n\n    ('MI + SMOTE+Tomek',\n     X_train_mi_smotetomek, y_train_mi_smotetomek,\n     X_test_mi_smotetomek, y_test_mi_smotetomek),\n\n    ('MI',\n     X_train_mi, y_train_mi,\n     X_test_mi, y_test_mi),\n\n    ('LDA',\n     X_train_lda, y_train_lda,\n     X_test_lda, y_test_lda),\n]\n\n\n\n# ==============================================================================\n# STEP 6: TRAIN ALL BASE MODELS ON ALL CONFIGURATIONS\n# ==============================================================================\n\nall_results = []\n\nprint(\"=\"*80)\nprint(\"PHASE 1: TRAINING BASE MODELS\")\nprint(\"=\"*80)\nprint()\n\nfor config_name, X_tr, y_tr, X_te, y_te in configurations:\n    print(\"=\"*80)\n    print(f\"CONFIGURATION: {config_name}\")\n    print(\"=\"*80)\n    print(f\"Training shape: {X_tr.shape}, Test shape: {X_te.shape}\")\n    print()\n    \n    base_classifiers = get_base_classifiers()\n    \n    for model_name, model in base_classifiers.items():\n        print(f\"  Training {model_name}...\", end=' ')\n        \n        metrics = train_and_evaluate(\n            model, X_tr, y_tr, X_te, y_te, model_name, config_name\n        )\n        all_results.append(metrics)\n        \n        print(f\"✓ Acc: {metrics['Accuracy']:.4f} | F1: {metrics['F1-Score']:.4f}\")\n    \n    print()\n\n# ==============================================================================\n# STEP 7: TRAIN ALL ENSEMBLE MODELS ON ALL CONFIGURATIONS\n# ==============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"PHASE 2: TRAINING ENSEMBLE MODELS\")\nprint(\"=\"*80)\nprint()\n\nfor config_name, X_tr, y_tr, X_te, y_te in configurations:\n    print(\"=\"*80)\n    print(f\"CONFIGURATION: {config_name}\")\n    print(\"=\"*80)\n    print(f\"Training shape: {X_tr.shape}, Test shape: {X_te.shape}\")\n    print()\n    \n    ensemble_classifiers = get_ensemble_classifiers()\n    \n    for model_name, model in ensemble_classifiers.items():\n        print(f\"  Training {model_name}...\", end=' ')\n        \n        metrics = train_and_evaluate(\n            model, X_tr, y_tr, X_te, y_te, model_name, config_name\n        )\n        all_results.append(metrics)\n        \n        print(f\"✓ Acc: {metrics['Accuracy']:.4f} | F1: {metrics['F1-Score']:.4f}\")\n    \n    print()\n\n# ==============================================================================\n# STEP 8: CREATE RESULTS DATAFRAME\n# ==============================================================================\n\nresults_df = pd.DataFrame(all_results)\n\n# ==============================================================================\n# STEP 9: DISPLAY ALL RESULTS SORTED BY ACCURACY\n# ==============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"🏆 ALL RESULTS SORTED BY ACCURACY (HIGHEST TO LOWEST)\")\nprint(\"=\"*80)\nprint()\n\nresults_sorted = results_df.sort_values('Accuracy', ascending=False).reset_index(drop=True)\nresults_sorted.index = results_sorted.index + 1\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.float_format', '{:.4f}'.format)\n\nprint(results_sorted.to_string())\n\n# ==============================================================================\n# STEP 10: TOP PERFORMERS\n# ==============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"🥇 TOP 10 MODEL-CONFIGURATION COMBINATIONS\")\nprint(\"=\"*80)\nprint()\nprint(results_sorted.head(10).to_string())\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"🥈 TOP 5 BASE MODELS\")\nprint(\"=\"*80)\nprint()\nbase_models = results_sorted[results_sorted['Model_Type'] == 'Base'].head(5)\nprint(base_models.to_string())\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"🥉 TOP 3 ENSEMBLE MODELS\")\nprint(\"=\"*80)\nprint()\nensemble_models = results_sorted[results_sorted['Model_Type'] == 'Ensemble'].head(3)\nprint(ensemble_models.to_string())\n\n# ==============================================================================\n# STEP 11: BEST MODEL PER CONFIGURATION\n# ==============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"📊 BEST MODEL PER CONFIGURATION\")\nprint(\"=\"*80)\n\nfor config in configurations:\n    config_name = config[0]\n    config_results = results_df[results_df['Configuration'] == config_name]\n    best_model = config_results.loc[config_results['Accuracy'].idxmax()]\n    print(f\"\\n{config_name}:\")\n    print(f\"  Model:      {best_model['Model']} ({best_model['Model_Type']})\")\n    print(f\"  Accuracy:   {best_model['Accuracy']:.4f}\")\n    print(f\"  Precision:  {best_model['Precision']:.4f}\")\n    print(f\"  Recall:     {best_model['Recall']:.4f}\")\n    print(f\"  F1-Score:   {best_model['F1-Score']:.4f}\")\n    print(f\"  ROC-AUC:    {best_model['ROC-AUC']}\")\n\n# ==============================================================================\n# STEP 12: BEST CONFIGURATION PER MODEL\n# ==============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"🎯 BEST CONFIGURATION PER MODEL\")\nprint(\"=\"*80)\n\nall_models = results_df['Model'].unique()\nfor model_name in sorted(all_models):\n    model_results = results_df[results_df['Model'] == model_name]\n    best_config = model_results.loc[model_results['Accuracy'].idxmax()]\n    model_type = best_config['Model_Type']\n    print(f\"\\n{model_name} ({model_type}):\")\n    print(f\"  Best Config:  {best_config['Configuration']}\")\n    print(f\"  Accuracy:     {best_config['Accuracy']:.4f}\")\n    print(f\"  F1-Score:     {best_config['F1-Score']:.4f}\")\n    print(f\"  ROC-AUC:      {best_config['ROC-AUC']}\")\n\n# ==============================================================================\n# STEP 13: OVERALL BEST MODEL\n# ==============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"🌟 OVERALL BEST MODEL-CONFIGURATION COMBINATION\")\nprint(\"=\"*80)\nbest_overall = results_df.loc[results_df['Accuracy'].idxmax()]\nprint(f\"\\nModel:         {best_overall['Model']}\")\nprint(f\"Type:          {best_overall['Model_Type']}\")\nprint(f\"Configuration: {best_overall['Configuration']}\")\nprint(f\"Accuracy:      {best_overall['Accuracy']:.4f}\")\nprint(f\"Precision:     {best_overall['Precision']:.4f}\")\nprint(f\"Recall:        {best_overall['Recall']:.4f}\")\nprint(f\"F1-Score:      {best_overall['F1-Score']:.4f}\")\nprint(f\"ROC-AUC:       {best_overall['ROC-AUC']}\")\n\n# ==============================================================================\n# STEP 14: COMPARISON - BASE VS ENSEMBLE\n# ==============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"⚖️ BASE MODELS VS ENSEMBLE MODELS COMPARISON\")\nprint(\"=\"*80)\n\nbase_avg = results_df[results_df['Model_Type'] == 'Base']['Accuracy'].mean()\nensemble_avg = results_df[results_df['Model_Type'] == 'Ensemble']['Accuracy'].mean()\n\nprint(f\"\\nBase Models Average Accuracy:     {base_avg:.4f}\")\nprint(f\"Ensemble Models Average Accuracy: {ensemble_avg:.4f}\")\nprint(f\"Difference:                       {ensemble_avg - base_avg:+.4f}\")\n\nif ensemble_avg > base_avg:\n    print(f\"\\n✓ Ensemble models perform {((ensemble_avg/base_avg - 1) * 100):.2f}% better on average\")\nelse:\n    print(f\"\\n✗ Base models perform {((base_avg/ensemble_avg - 1) * 100):.2f}% better on average\")\n\n# ==============================================================================\n# STEP 15: PIVOT TABLES\n# ==============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"📈 PIVOT TABLE: ACCURACY BY MODEL AND CONFIGURATION\")\nprint(\"=\"*80)\npivot_accuracy = results_df.pivot_table(\n    index='Model', \n    columns='Configuration', \n    values='Accuracy',\n    aggfunc='first'\n)\nprint(pivot_accuracy.to_string())\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"📈 PIVOT TABLE: F1-SCORE BY MODEL AND CONFIGURATION\")\nprint(\"=\"*80)\npivot_f1 = results_df.pivot_table(\n    index='Model', \n    columns='Configuration', \n    values='F1-Score',\n    aggfunc='first'\n)\nprint(pivot_f1.to_string())\n\n# ==============================================================================\n# STEP 16: SUMMARY STATISTICS\n# ==============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"📊 SUMMARY STATISTICS\")\nprint(\"=\"*80)\nprint(f\"\\nTotal Experiments:        {len(results_df)}\")\nprint(f\"Base Models:              {len(results_df[results_df['Model_Type'] == 'Base'])}\")\nprint(f\"Ensemble Models:          {len(results_df[results_df['Model_Type'] == 'Ensemble'])}\")\nprint(f\"Number of Configurations: {len(configurations)}\")\nprint(f\"Number of Model Types:    {results_df['Model'].nunique()}\")\n\nprint(f\"\\nOverall Accuracy Statistics:\")\nprint(f\"  Mean:    {results_df['Accuracy'].mean():.4f}\")\nprint(f\"  Median:  {results_df['Accuracy'].median():.4f}\")\nprint(f\"  Std Dev: {results_df['Accuracy'].std():.4f}\")\nprint(f\"  Min:     {results_df['Accuracy'].min():.4f}\")\nprint(f\"  Max:     {results_df['Accuracy'].max():.4f}\")\n\n# ==============================================================================\n# STEP 17: EXPORT RESULTS\n# ==============================================================================\n\nresults_df.to_csv('complete_model_comparison_results.csv', index=False)\nresults_sorted.to_csv('complete_model_comparison_sorted.csv', index=False)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"💾 RESULTS EXPORTED\")\nprint(\"=\"*80)\nprint(\"  ✓ complete_model_comparison_results.csv\")\nprint(\"  ✓ complete_model_comparison_sorted.csv\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T19:08:49.833616Z","iopub.execute_input":"2025-10-11T19:08:49.833955Z","iopub.status.idle":"2025-10-11T19:09:51.711556Z","shell.execute_reply.started":"2025-10-11T19:08:49.833927Z","shell.execute_reply":"2025-10-11T19:09:51.710336Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nPHASE 1: TRAINING BASE MODELS\n================================================================================\n\n================================================================================\nCONFIGURATION: Boruta + SMOTE+Tomek\n================================================================================\nTraining shape: (519, 10), Test shape: (75, 10)\n\n  Training MLPClassifier... ✓ Acc: 0.9467 | F1: 0.9484\n  Training SVC... ✓ Acc: 0.9333 | F1: 0.9341\n  Training AdaBoostClassifier... ✓ Acc: 0.9067 | F1: 0.9062\n  Training RandomForestClassifier... ✓ Acc: 0.9600 | F1: 0.9599\n  Training ExtraTreesClassifier... ✓ Acc: 0.9333 | F1: 0.9333\n  Training GradientBoostingClassifier... ✓ Acc: 0.9600 | F1: 0.9599\n  Training XGBClassifier... ✓ Acc: 0.9333 | F1: 0.9333\n  Training LGBMClassifier... ✓ Acc: 0.9333 | F1: 0.9338\n  Training DecisionTreeClassifier... ✓ Acc: 0.9200 | F1: 0.9213\n\n================================================================================\nCONFIGURATION: PowerTransformer + SMOTE\n================================================================================\nTraining shape: (525, 24), Test shape: (75, 24)\n\n  Training MLPClassifier... ✓ Acc: 0.9200 | F1: 0.9213\n  Training SVC... ✓ Acc: 0.9467 | F1: 0.9474\n  Training AdaBoostClassifier... ✓ Acc: 0.9600 | F1: 0.9595\n  Training RandomForestClassifier... ✓ Acc: 0.9600 | F1: 0.9606\n  Training ExtraTreesClassifier... ✓ Acc: 0.9333 | F1: 0.9345\n  Training GradientBoostingClassifier... ✓ Acc: 0.9467 | F1: 0.9472\n  Training XGBClassifier... ✓ Acc: 0.9333 | F1: 0.9324\n  Training LGBMClassifier... ✓ Acc: 0.9333 | F1: 0.9345\n  Training DecisionTreeClassifier... ✓ Acc: 0.8933 | F1: 0.8968\n\n================================================================================\nCONFIGURATION: MI + SMOTE+Tomek\n================================================================================\nTraining shape: (523, 10), Test shape: (75, 10)\n\n  Training MLPClassifier... ✓ Acc: 0.9600 | F1: 0.9599\n  Training SVC... ✓ Acc: 0.9467 | F1: 0.9464\n  Training AdaBoostClassifier... ✓ Acc: 0.9200 | F1: 0.9222\n  Training RandomForestClassifier... ✓ Acc: 0.9600 | F1: 0.9599\n  Training ExtraTreesClassifier... ✓ Acc: 0.9600 | F1: 0.9599\n  Training GradientBoostingClassifier... ✓ Acc: 0.9333 | F1: 0.9333\n  Training XGBClassifier... ✓ Acc: 0.9600 | F1: 0.9599\n  Training LGBMClassifier... ✓ Acc: 0.9733 | F1: 0.9733\n  Training DecisionTreeClassifier... ✓ Acc: 0.9200 | F1: 0.9210\n\n================================================================================\nCONFIGURATION: MI\n================================================================================\nTraining shape: (299, 10), Test shape: (75, 10)\n\n  Training MLPClassifier... ✓ Acc: 0.9467 | F1: 0.9472\n  Training SVC... ✓ Acc: 0.9067 | F1: 0.9089\n  Training AdaBoostClassifier... ✓ Acc: 0.9600 | F1: 0.9600\n  Training RandomForestClassifier... ✓ Acc: 0.9733 | F1: 0.9733\n  Training ExtraTreesClassifier... ✓ Acc: 0.9600 | F1: 0.9599\n  Training GradientBoostingClassifier... ✓ Acc: 0.9733 | F1: 0.9733\n  Training XGBClassifier... ✓ Acc: 0.9333 | F1: 0.9333\n  Training LGBMClassifier... ✓ Acc: 0.9600 | F1: 0.9599\n  Training DecisionTreeClassifier... ✓ Acc: 0.9600 | F1: 0.9605\n\n================================================================================\nCONFIGURATION: LDA\n================================================================================\nTraining shape: (299, 2), Test shape: (75, 2)\n\n  Training MLPClassifier... ✓ Acc: 0.9333 | F1: 0.9354\n  Training SVC... ✓ Acc: 0.9333 | F1: 0.9354\n  Training AdaBoostClassifier... ✓ Acc: 0.9467 | F1: 0.9476\n  Training RandomForestClassifier... ✓ Acc: 0.9733 | F1: 0.9733\n  Training ExtraTreesClassifier... ✓ Acc: 0.9733 | F1: 0.9733\n  Training GradientBoostingClassifier... ✓ Acc: 0.9600 | F1: 0.9599\n  Training XGBClassifier... ✓ Acc: 0.9733 | F1: 0.9733\n  Training LGBMClassifier... ✓ Acc: 0.9867 | F1: 0.9867\n  Training DecisionTreeClassifier... ✓ Acc: 0.9733 | F1: 0.9733\n\n\n================================================================================\nPHASE 2: TRAINING ENSEMBLE MODELS\n================================================================================\n\n================================================================================\nCONFIGURATION: Boruta + SMOTE+Tomek\n================================================================================\nTraining shape: (519, 10), Test shape: (75, 10)\n\n  Training StackingClassifier... ✓ Acc: 0.9200 | F1: 0.9210\n  Training VotingClassifier... ✓ Acc: 0.9333 | F1: 0.9333\n  Training BaggingEnsemble... ✓ Acc: 0.9600 | F1: 0.9599\n\n================================================================================\nCONFIGURATION: PowerTransformer + SMOTE\n================================================================================\nTraining shape: (525, 24), Test shape: (75, 24)\n\n  Training StackingClassifier... ✓ Acc: 0.9600 | F1: 0.9605\n  Training VotingClassifier... ✓ Acc: 0.9333 | F1: 0.9345\n  Training BaggingEnsemble... ✓ Acc: 0.9600 | F1: 0.9605\n\n================================================================================\nCONFIGURATION: MI + SMOTE+Tomek\n================================================================================\nTraining shape: (523, 10), Test shape: (75, 10)\n\n  Training StackingClassifier... ✓ Acc: 0.9600 | F1: 0.9599\n  Training VotingClassifier... ✓ Acc: 0.9600 | F1: 0.9599\n  Training BaggingEnsemble... ✓ Acc: 0.9733 | F1: 0.9732\n\n================================================================================\nCONFIGURATION: MI\n================================================================================\nTraining shape: (299, 10), Test shape: (75, 10)\n\n  Training StackingClassifier... ✓ Acc: 0.9600 | F1: 0.9599\n  Training VotingClassifier... ✓ Acc: 0.9600 | F1: 0.9599\n  Training BaggingEnsemble... ✓ Acc: 0.9733 | F1: 0.9733\n\n================================================================================\nCONFIGURATION: LDA\n================================================================================\nTraining shape: (299, 2), Test shape: (75, 2)\n\n  Training StackingClassifier... ✓ Acc: 0.9467 | F1: 0.9467\n  Training VotingClassifier... ✓ Acc: 0.9733 | F1: 0.9733\n  Training BaggingEnsemble... ✓ Acc: 0.9467 | F1: 0.9461\n\n\n================================================================================\n🏆 ALL RESULTS SORTED BY ACCURACY (HIGHEST TO LOWEST)\n================================================================================\n\n    Accuracy  Precision  Recall  F1-Score  ROC-AUC                       Model             Configuration Model_Type\n1     0.9867     0.9875  0.9867    0.9867   0.9993              LGBMClassifier                       LDA       Base\n2     0.9733     0.9733  0.9733    0.9733   0.9948      RandomForestClassifier                        MI       Base\n3     0.9733     0.9763  0.9733    0.9732   0.9978             BaggingEnsemble          MI + SMOTE+Tomek   Ensemble\n4     0.9733     0.9733  0.9733    0.9733   0.9980      RandomForestClassifier                       LDA       Base\n5     0.9733     0.9733  0.9733    0.9733   0.9768        ExtraTreesClassifier                       LDA       Base\n6     0.9733     0.9733  0.9733    0.9733   0.9913              LGBMClassifier          MI + SMOTE+Tomek       Base\n7     0.9733     0.9733  0.9733    0.9733   0.9883               XGBClassifier                       LDA       Base\n8     0.9733     0.9733  0.9733    0.9733   0.9777      DecisionTreeClassifier                       LDA       Base\n9     0.9733     0.9733  0.9733    0.9733   0.9897  GradientBoostingClassifier                        MI       Base\n10    0.9733     0.9733  0.9733    0.9733   0.9962            VotingClassifier                       LDA   Ensemble\n11    0.9733     0.9733  0.9733    0.9733   0.9980             BaggingEnsemble                        MI   Ensemble\n12    0.9600     0.9606  0.9600    0.9599   0.9978          StackingClassifier                        MI   Ensemble\n13    0.9600     0.9606  0.9600    0.9599   0.9933            VotingClassifier          MI + SMOTE+Tomek   Ensemble\n14    0.9600     0.9606  0.9600    0.9599   0.9784        ExtraTreesClassifier                        MI       Base\n15    0.9600     0.9616  0.9600    0.9605   0.9779      DecisionTreeClassifier                        MI       Base\n16    0.9600     0.9608  0.9600    0.9600   0.9907          AdaBoostClassifier                        MI       Base\n17    0.9600     0.9606  0.9600    0.9599   0.9942      RandomForestClassifier      Boruta + SMOTE+Tomek       Base\n18    0.9600     0.9606  0.9600    0.9599   0.9890               XGBClassifier          MI + SMOTE+Tomek       Base\n19    0.9600     0.9606  0.9600    0.9599   0.9847  GradientBoostingClassifier                       LDA       Base\n20    0.9600     0.9606  0.9600    0.9599   0.9777        ExtraTreesClassifier          MI + SMOTE+Tomek       Base\n21    0.9600     0.9606  0.9600    0.9599   0.9937      RandomForestClassifier          MI + SMOTE+Tomek       Base\n22    0.9600     0.9606  0.9600    0.9599   0.9825  GradientBoostingClassifier      Boruta + SMOTE+Tomek       Base\n23    0.9600     0.9606  0.9600    0.9599   0.9947               MLPClassifier          MI + SMOTE+Tomek       Base\n24    0.9600     0.9606  0.9600    0.9599   0.9924            VotingClassifier                        MI   Ensemble\n25    0.9600     0.9606  0.9600    0.9599   0.9904              LGBMClassifier                        MI       Base\n26    0.9600     0.9616  0.9600    0.9605   0.9951          StackingClassifier  PowerTransformer + SMOTE   Ensemble\n27    0.9600     0.9616  0.9600    0.9605   0.9980             BaggingEnsemble  PowerTransformer + SMOTE   Ensemble\n28    0.9600     0.9606  0.9600    0.9599   0.9935          StackingClassifier          MI + SMOTE+Tomek   Ensemble\n29    0.9600     0.9617  0.9600    0.9606   0.9946      RandomForestClassifier  PowerTransformer + SMOTE       Base\n30    0.9600     0.9663  0.9600    0.9595   0.9953          AdaBoostClassifier  PowerTransformer + SMOTE       Base\n31    0.9600     0.9606  0.9600    0.9599   0.9982             BaggingEnsemble      Boruta + SMOTE+Tomek   Ensemble\n32    0.9467     0.9467  0.9467    0.9467   0.9948          StackingClassifier                       LDA   Ensemble\n33    0.9467     0.9505  0.9467    0.9476   0.9892          AdaBoostClassifier                       LDA       Base\n34    0.9467     0.9573  0.9467    0.9484   0.9865               MLPClassifier      Boruta + SMOTE+Tomek       Base\n35    0.9467     0.9461  0.9467    0.9461   0.9969             BaggingEnsemble                       LDA   Ensemble\n36    0.9467     0.9482  0.9467    0.9472   0.9978               MLPClassifier                        MI       Base\n37    0.9467     0.9520  0.9467    0.9474   0.9886                         SVC  PowerTransformer + SMOTE       Base\n38    0.9467     0.9482  0.9467    0.9472   0.9881  GradientBoostingClassifier  PowerTransformer + SMOTE       Base\n39    0.9467     0.9509  0.9467    0.9464   0.9958                         SVC          MI + SMOTE+Tomek       Base\n40    0.9333     0.9419  0.9333    0.9341   0.9788                         SVC      Boruta + SMOTE+Tomek       Base\n41    0.9333     0.9341  0.9333    0.9333   0.9768        ExtraTreesClassifier      Boruta + SMOTE+Tomek       Base\n42    0.9333     0.9341  0.9333    0.9333   0.9830               XGBClassifier      Boruta + SMOTE+Tomek       Base\n43    0.9333     0.9363  0.9333    0.9338   0.9890              LGBMClassifier      Boruta + SMOTE+Tomek       Base\n44    0.9333     0.9403  0.9333    0.9345   0.9666        ExtraTreesClassifier  PowerTransformer + SMOTE       Base\n45    0.9333     0.9403  0.9333    0.9345   0.9922            VotingClassifier  PowerTransformer + SMOTE   Ensemble\n46    0.9333     0.9337  0.9333    0.9324   0.9862               XGBClassifier  PowerTransformer + SMOTE       Base\n47    0.9333     0.9341  0.9333    0.9333   0.9879               XGBClassifier                        MI       Base\n48    0.9333     0.9341  0.9333    0.9333   0.9906            VotingClassifier      Boruta + SMOTE+Tomek   Ensemble\n49    0.9333     0.9341  0.9333    0.9333   0.9895  GradientBoostingClassifier          MI + SMOTE+Tomek       Base\n50    0.9333     0.9413  0.9333    0.9354   0.9811                         SVC                       LDA       Base\n51    0.9333     0.9413  0.9333    0.9354   0.9775               MLPClassifier                       LDA       Base\n52    0.9333     0.9403  0.9333    0.9345   0.9868              LGBMClassifier  PowerTransformer + SMOTE       Base\n53    0.9200     0.9225  0.9200    0.9210   0.9828          StackingClassifier      Boruta + SMOTE+Tomek   Ensemble\n54    0.9200     0.9263  0.9200    0.9222   0.9741          AdaBoostClassifier          MI + SMOTE+Tomek       Base\n55    0.9200     0.9225  0.9200    0.9210   0.9384      DecisionTreeClassifier          MI + SMOTE+Tomek       Base\n56    0.9200     0.9245  0.9200    0.9213   0.9701               MLPClassifier  PowerTransformer + SMOTE       Base\n57    0.9200     0.9245  0.9200    0.9213   0.9386      DecisionTreeClassifier      Boruta + SMOTE+Tomek       Base\n58    0.9067     0.9124  0.9067    0.9089   0.9951                         SVC                        MI       Base\n59    0.9067     0.9065  0.9067    0.9062   0.9820          AdaBoostClassifier      Boruta + SMOTE+Tomek       Base\n60    0.8933     0.9074  0.8933    0.8968   0.9193      DecisionTreeClassifier  PowerTransformer + SMOTE       Base\n\n================================================================================\n🥇 TOP 10 MODEL-CONFIGURATION COMBINATIONS\n================================================================================\n\n    Accuracy  Precision  Recall  F1-Score  ROC-AUC                       Model     Configuration Model_Type\n1     0.9867     0.9875  0.9867    0.9867   0.9993              LGBMClassifier               LDA       Base\n2     0.9733     0.9733  0.9733    0.9733   0.9948      RandomForestClassifier                MI       Base\n3     0.9733     0.9763  0.9733    0.9732   0.9978             BaggingEnsemble  MI + SMOTE+Tomek   Ensemble\n4     0.9733     0.9733  0.9733    0.9733   0.9980      RandomForestClassifier               LDA       Base\n5     0.9733     0.9733  0.9733    0.9733   0.9768        ExtraTreesClassifier               LDA       Base\n6     0.9733     0.9733  0.9733    0.9733   0.9913              LGBMClassifier  MI + SMOTE+Tomek       Base\n7     0.9733     0.9733  0.9733    0.9733   0.9883               XGBClassifier               LDA       Base\n8     0.9733     0.9733  0.9733    0.9733   0.9777      DecisionTreeClassifier               LDA       Base\n9     0.9733     0.9733  0.9733    0.9733   0.9897  GradientBoostingClassifier                MI       Base\n10    0.9733     0.9733  0.9733    0.9733   0.9962            VotingClassifier               LDA   Ensemble\n\n================================================================================\n🥈 TOP 5 BASE MODELS\n================================================================================\n\n   Accuracy  Precision  Recall  F1-Score  ROC-AUC                   Model     Configuration Model_Type\n1    0.9867     0.9875  0.9867    0.9867   0.9993          LGBMClassifier               LDA       Base\n2    0.9733     0.9733  0.9733    0.9733   0.9948  RandomForestClassifier                MI       Base\n4    0.9733     0.9733  0.9733    0.9733   0.9980  RandomForestClassifier               LDA       Base\n5    0.9733     0.9733  0.9733    0.9733   0.9768    ExtraTreesClassifier               LDA       Base\n6    0.9733     0.9733  0.9733    0.9733   0.9913          LGBMClassifier  MI + SMOTE+Tomek       Base\n\n================================================================================\n🥉 TOP 3 ENSEMBLE MODELS\n================================================================================\n\n    Accuracy  Precision  Recall  F1-Score  ROC-AUC             Model     Configuration Model_Type\n3     0.9733     0.9763  0.9733    0.9732   0.9978   BaggingEnsemble  MI + SMOTE+Tomek   Ensemble\n10    0.9733     0.9733  0.9733    0.9733   0.9962  VotingClassifier               LDA   Ensemble\n11    0.9733     0.9733  0.9733    0.9733   0.9980   BaggingEnsemble                MI   Ensemble\n\n================================================================================\n📊 BEST MODEL PER CONFIGURATION\n================================================================================\n\nBoruta + SMOTE+Tomek:\n  Model:      RandomForestClassifier (Base)\n  Accuracy:   0.9600\n  Precision:  0.9606\n  Recall:     0.9600\n  F1-Score:   0.9599\n  ROC-AUC:    0.9941694915254238\n\nPowerTransformer + SMOTE:\n  Model:      AdaBoostClassifier (Base)\n  Accuracy:   0.9600\n  Precision:  0.9663\n  Recall:     0.9600\n  F1-Score:   0.9595\n  ROC-AUC:    0.9952862523540491\n\nMI + SMOTE+Tomek:\n  Model:      LGBMClassifier (Base)\n  Accuracy:   0.9733\n  Precision:  0.9733\n  Recall:     0.9733\n  F1-Score:   0.9733\n  ROC-AUC:    0.9912655367231639\n\nMI:\n  Model:      RandomForestClassifier (Base)\n  Accuracy:   0.9733\n  Precision:  0.9733\n  Recall:     0.9733\n  F1-Score:   0.9733\n  ROC-AUC:    0.9948436911487759\n\nLDA:\n  Model:      LGBMClassifier (Base)\n  Accuracy:   0.9867\n  Precision:  0.9875\n  Recall:     0.9867\n  F1-Score:   0.9867\n  ROC-AUC:    0.9993276836158193\n\n================================================================================\n🎯 BEST CONFIGURATION PER MODEL\n================================================================================\n\nAdaBoostClassifier (Base):\n  Best Config:  PowerTransformer + SMOTE\n  Accuracy:     0.9600\n  F1-Score:     0.9595\n  ROC-AUC:      0.9952862523540491\n\nBaggingEnsemble (Ensemble):\n  Best Config:  MI + SMOTE+Tomek\n  Accuracy:     0.9733\n  F1-Score:     0.9732\n  ROC-AUC:      0.9977627118644068\n\nDecisionTreeClassifier (Base):\n  Best Config:  LDA\n  Accuracy:     0.9733\n  F1-Score:     0.9733\n  ROC-AUC:      0.9777005649717514\n\nExtraTreesClassifier (Base):\n  Best Config:  LDA\n  Accuracy:     0.9733\n  F1-Score:     0.9733\n  ROC-AUC:      0.9768003766478344\n\nGradientBoostingClassifier (Base):\n  Best Config:  MI\n  Accuracy:     0.9733\n  F1-Score:     0.9733\n  ROC-AUC:      0.9896873822975518\n\nLGBMClassifier (Base):\n  Best Config:  LDA\n  Accuracy:     0.9867\n  F1-Score:     0.9867\n  ROC-AUC:      0.9993276836158193\n\nMLPClassifier (Base):\n  Best Config:  MI + SMOTE+Tomek\n  Accuracy:     0.9600\n  F1-Score:     0.9599\n  ROC-AUC:      0.9946591337099813\n\nRandomForestClassifier (Base):\n  Best Config:  MI\n  Accuracy:     0.9733\n  F1-Score:     0.9733\n  ROC-AUC:      0.9948436911487759\n\nSVC (Base):\n  Best Config:  PowerTransformer + SMOTE\n  Accuracy:     0.9467\n  F1-Score:     0.9474\n  ROC-AUC:      0.9886218334244579\n\nStackingClassifier (Ensemble):\n  Best Config:  PowerTransformer + SMOTE\n  Accuracy:     0.9600\n  F1-Score:     0.9605\n  ROC-AUC:      0.9950809792843692\n\nVotingClassifier (Ensemble):\n  Best Config:  LDA\n  Accuracy:     0.9733\n  F1-Score:     0.9733\n  ROC-AUC:      0.9961996233521656\n\nXGBClassifier (Base):\n  Best Config:  LDA\n  Accuracy:     0.9733\n  F1-Score:     0.9733\n  ROC-AUC:      0.9883352165725048\n\n================================================================================\n🌟 OVERALL BEST MODEL-CONFIGURATION COMBINATION\n================================================================================\n\nModel:         LGBMClassifier\nType:          Base\nConfiguration: LDA\nAccuracy:      0.9867\nPrecision:     0.9875\nRecall:        0.9867\nF1-Score:      0.9867\nROC-AUC:       0.9993276836158193\n\n================================================================================\n⚖️ BASE MODELS VS ENSEMBLE MODELS COMPARISON\n================================================================================\n\nBase Models Average Accuracy:     0.9470\nEnsemble Models Average Accuracy: 0.9547\nDifference:                       +0.0077\n\n✓ Ensemble models perform 0.81% better on average\n\n================================================================================\n📈 PIVOT TABLE: ACCURACY BY MODEL AND CONFIGURATION\n================================================================================\nConfiguration               Boruta + SMOTE+Tomek    LDA     MI  MI + SMOTE+Tomek  PowerTransformer + SMOTE\nModel                                                                                                     \nAdaBoostClassifier                        0.9067 0.9467 0.9600            0.9200                    0.9600\nBaggingEnsemble                           0.9600 0.9467 0.9733            0.9733                    0.9600\nDecisionTreeClassifier                    0.9200 0.9733 0.9600            0.9200                    0.8933\nExtraTreesClassifier                      0.9333 0.9733 0.9600            0.9600                    0.9333\nGradientBoostingClassifier                0.9600 0.9600 0.9733            0.9333                    0.9467\nLGBMClassifier                            0.9333 0.9867 0.9600            0.9733                    0.9333\nMLPClassifier                             0.9467 0.9333 0.9467            0.9600                    0.9200\nRandomForestClassifier                    0.9600 0.9733 0.9733            0.9600                    0.9600\nSVC                                       0.9333 0.9333 0.9067            0.9467                    0.9467\nStackingClassifier                        0.9200 0.9467 0.9600            0.9600                    0.9600\nVotingClassifier                          0.9333 0.9733 0.9600            0.9600                    0.9333\nXGBClassifier                             0.9333 0.9733 0.9333            0.9600                    0.9333\n\n================================================================================\n📈 PIVOT TABLE: F1-SCORE BY MODEL AND CONFIGURATION\n================================================================================\nConfiguration               Boruta + SMOTE+Tomek    LDA     MI  MI + SMOTE+Tomek  PowerTransformer + SMOTE\nModel                                                                                                     \nAdaBoostClassifier                        0.9062 0.9476 0.9600            0.9222                    0.9595\nBaggingEnsemble                           0.9599 0.9461 0.9733            0.9732                    0.9605\nDecisionTreeClassifier                    0.9213 0.9733 0.9605            0.9210                    0.8968\nExtraTreesClassifier                      0.9333 0.9733 0.9599            0.9599                    0.9345\nGradientBoostingClassifier                0.9599 0.9599 0.9733            0.9333                    0.9472\nLGBMClassifier                            0.9338 0.9867 0.9599            0.9733                    0.9345\nMLPClassifier                             0.9484 0.9354 0.9472            0.9599                    0.9213\nRandomForestClassifier                    0.9599 0.9733 0.9733            0.9599                    0.9606\nSVC                                       0.9341 0.9354 0.9089            0.9464                    0.9474\nStackingClassifier                        0.9210 0.9467 0.9599            0.9599                    0.9605\nVotingClassifier                          0.9333 0.9733 0.9599            0.9599                    0.9345\nXGBClassifier                             0.9333 0.9733 0.9333            0.9599                    0.9324\n\n================================================================================\n📊 SUMMARY STATISTICS\n================================================================================\n\nTotal Experiments:        60\nBase Models:              45\nEnsemble Models:          15\nNumber of Configurations: 5\nNumber of Model Types:    12\n\nOverall Accuracy Statistics:\n  Mean:    0.9489\n  Median:  0.9600\n  Std Dev: 0.0201\n  Min:     0.8933\n  Max:     0.9867\n\n================================================================================\n💾 RESULTS EXPORTED\n================================================================================\n  ✓ complete_model_comparison_results.csv\n  ✓ complete_model_comparison_sorted.csv\n================================================================================\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}