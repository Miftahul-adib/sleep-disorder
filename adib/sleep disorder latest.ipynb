{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6491929,"sourceType":"datasetVersion","datasetId":3321433}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Summary of Configurations and Models\n\n### Preprocessing & Sampling Overview\n- **Preprocessing types:** 2 main types (Normalization & Robust Scaling, plus feature selection & transformation options)  \n- **Sampling methods:** 6 (SVMSMOTE, BorderlineSMOTE, RandomOverSampler, SMOTE, SMOTE+Tomek, SMOTE+ENN)  \n- **Outliers:** Can be removed or kept (optional in this code)  \n\n### Configurations Summary\n**Total configurations:** 53  \n\n| Type | # Configurations |\n|------|----------------|\n| Original | 1 |\n| Normalized | 7 |\n| Robust | 7 |\n| Mutual Information (MI) | 7 |\n| Boruta | 7 |\n| LDA | 7 |\n| Autoencoder | 7 |\n| PowerTransformer | 4 |\n| Polynomial Features | 4 |\n| AdaBoost | 2 |\n\n#### Configuration Details\n\n1. **Original**  \n   - 1 configuration: `Original Data`\n\n2. **Normalized (MinMaxScaler)**  \n   - 7 configurations: `Normalized Data`, `Normalized+SVMSMOTE`, `Normalized+BorderlineSMOTE`, `Normalized+RandomOverSampler`, `Normalized+SMOTE`, `Normalized+SMOTE+Tomek`, `Normalized+SMOTE+ENN`\n\n3. **Robust Scaler**  \n   - 7 configurations: `Robust Data`, `Robust+SVMSMOTE`, `Robust+BorderlineSMOTE`, `Robust+RandomOverSampler`, `Robust+SMOTE`, `Robust+SMOTE+Tomek`, `Robust+SMOTE+ENN`\n\n4. **Feature Selection – Mutual Information (MI)**  \n   - 7 configurations: `MI`, `MI+SVMSMOTE`, `MI+BorderlineSMOTE`, `MI+RandomOverSampler`, `MI+SMOTE`, `MI+SMOTE+Tomek`, `MI+SMOTE+ENN`\n\n5. **Feature Selection – Boruta**  \n   - 7 configurations: `Boruta`, `Boruta+SVMSMOTE`, `Boruta+BorderlineSMOTE`, `Boruta+RandomOverSampler`, `Boruta+SMOTE`, `Boruta+SMOTE+Tomek`, `Boruta+SMOTE+ENN`\n\n6. **LDA**  \n   - 7 configurations: `LDA`, `LDA+SVMSMOTE`, `LDA+BorderlineSMOTE`, `LDA+RandomOverSampler`, `LDA+SMOTE`, `LDA+SMOTE+Tomek`, `LDA+SMOTE+ENN`\n\n7. **Autoencoder**  \n   - 7 configurations: `Autoencoder`, `Autoencoder+SVMSMOTE`, `Autoencoder+BorderlineSMOTE`, `Autoencoder+RandomOverSampler`, `Autoencoder+SMOTE`, `Autoencoder+SMOTE+Tomek`, `Autoencoder+SMOTE+ENN`\n\n8. **PowerTransformer**  \n   - 4 configurations: `PowerTransformer`, `PowerTransformer+SMOTE`, `PowerTransformer+SMOTE+Tomek`, `PowerTransformer+SMOTE+ENN`\n\n9. **Polynomial Features**  \n   - 4 configurations: `PolynomialFeatures`, `PolynomialFeatures+SMOTE`, `PolynomialFeatures+SMOTE+Tomek`, `PolynomialFeatures+SMOTE+ENN`\n\n10. **AdaBoost**  \n    - 2 configurations: `AdaBoost Normalized+SMOTE Balanced`, `AdaBoost Normalized+SMOTE Balanced+Calibrated`  \n\n---\n\n## Models Used\n| Model | # Variants / Notes |\n|-------|------------------|\n| Logistic Regression | 2 |\n| AdaBoost | 3 (1 default + 2 calibrated) |\n| Gradient Boosting | 2 |\n| CatBoost | 7 (Default + Calibrated + 5 variants) |\n| LightGBM | 2 |\n| XGBoost | 1 |\n| Extra Trees | 1 |\n| KNN | 1 |\n| MLP | 1 |\n| Random Forest | 1 |\n\n\n---\n\n## Future Plan\n\n1️⃣ **Initial Model Evaluation:**  \n- Train all models across all configurations to check accuracy.  \n- Select the **top 10 configurations** and evaluate all metrics for these.\n\n2️⃣ **Ensembling & Stacking:**  \n- Train selected models using stacking, ensembling, and voting on the top 10 configurations.  \n- Measure all required metrics.  \n\n✅ **Final Step:**  \n- Sort results based on **accuracy and other metrics** to determine the best performing models.\nmetrics\n","metadata":{}},{"cell_type":"markdown","source":"## Top 10 Model Configurations by Test Accuracy\n\n| Rank | Model                 | Configuration             | Test Accuracy |\n|------|----------------------|--------------------------|---------------|\n| 1    | Gradient Boosting     | Boruta + SMOTE + Tomek   | 0.9733        |\n| 2    | Gradient Boosting     | PowerTransformer + SMOTE | 0.9733        |\n| 3    | XGBoost               | MI + SMOTE + Tomek       | 0.9733        |\n| 4    | XGBoost               | LDA                      | 0.9733        |\n| 5    | Extra Trees           | MI                       | 0.9733        |\n| 6    | Extra Trees           | MI + SMOTE + Tomek       | 0.9733        |\n| 7    | Random Forest         | MI + SMOTE + Tomek       | 0.9733        |\n| 8    | Random Forest         | Boruta + SMOTE + Tomek   | 0.9733        |\n| 9    | Logistic Regression   | Robust + SVMSMOTE        | 0.9600        |\n| 10   | Logistic Regression   | Robust + RandomOverSampler | 0.9600      |\n\n---\n\n## Top 5 Configurations Only\n\n1. Boruta + SMOTE + Tomek  \n2. PowerTransformer + SMOTE  \n3. MI + SMOTE + Tomek  \n4. LDA  \n5. MI\n","metadata":{}},{"cell_type":"code","source":"\n\n!pip install boruta category_encoders xgboost catboost","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:52:58.382275Z","iopub.execute_input":"2025-10-06T08:52:58.382985Z","iopub.status.idle":"2025-10-06T08:53:01.646252Z","shell.execute_reply.started":"2025-10-06T08:52:58.382959Z","shell.execute_reply":"2025-10-06T08:53:01.645461Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -y scikit-learn imbalanced-learn\n\n!pip install scikit-learn==1.4.2 imbalanced-learn==0.12.0\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, RobustScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV, RandomizedSearchCV\nfrom sklearn.feature_selection import mutual_info_classif, SelectKBest\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler, SMOTENC\nfrom imblearn.combine import SMOTETomek, SMOTEENN\nfrom imblearn.under_sampling import CondensedNearestNeighbour, TomekLinks, RandomUnderSampler\nfrom boruta import BorutaPy\nfrom keras.models import Model, Sequential\nfrom keras.layers import Input, Dense\nfrom keras.optimizers import Adam\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:53:01.647725Z","iopub.execute_input":"2025-10-06T08:53:01.648034Z","iopub.status.idle":"2025-10-06T08:53:22.554543Z","shell.execute_reply.started":"2025-10-06T08:53:01.647998Z","shell.execute_reply":"2025-10-06T08:53:22.553903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler, SMOTE, BorderlineSMOTE, SVMSMOTE, ADASYN\nfrom imblearn.under_sampling import RandomUnderSampler, ClusterCentroids, NearMiss\nfrom imblearn.under_sampling import TomekLinks\n\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE, BorderlineSMOTE, SVMSMOTE, ADASYN\nfrom imblearn.under_sampling import RandomUnderSampler, ClusterCentroids, NearMiss, TomekLinks\nfrom imblearn.combine import SMOTETomek, SMOTEENN","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:53:22.555316Z","iopub.execute_input":"2025-10-06T08:53:22.555748Z","iopub.status.idle":"2025-10-06T08:53:22.560411Z","shell.execute_reply.started":"2025-10-06T08:53:22.555724Z","shell.execute_reply":"2025-10-06T08:53:22.559624Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# configs","metadata":{}},{"cell_type":"markdown","source":"# Preprocessing 1","metadata":{}},{"cell_type":"code","source":"'''\n\ndf = pd.read_csv(\"/kaggle/input/sleep-health-and-lifestyle-dataset/Sleep_health_and_lifestyle_dataset.csv\")\ndf.fillna(\"None\", inplace=True)\n\n# Dividing Blood Pressure into Systolic and Diastolic BP\ndf[['Systolic BP', 'Diastolic BP']] = df['Blood Pressure'].str.split('/', expand=True).astype(int)\ndf.drop(['Person ID', 'Blood Pressure'], axis=1, inplace=True)\n\n# Labeling less number of careers as other\ndf['Occupation'] = df['Occupation'].replace(['Manager', 'Sales Representative', 'Scientist', 'Software Engineer'], 'Other')\n\n# Adding the average BMI for the range\ndf['BMI Category'] = df['BMI Category'].replace({'Normal':22, 'Normal Weight':22, 'Overweight':27, 'Obese':30})\n\n# Creating Interaction features\ndf['Stress_sleep_interaction'] = df['Stress Level'] / df['Quality of Sleep']\ndf['BMI_Activity'] = df['BMI Category'] * df['Physical Activity Level']\ndf['Sleep_Heart_ratio'] = df['Sleep Duration'] / df['Heart Rate']\ndf['Sleep_Steps_ratio'] = df['Sleep Duration'] / df['Daily Steps']\ndf['Sleep_Stress_ratio'] = df['Sleep Duration'] / df['Stress Level']\n\ndf = pd.get_dummies(df, columns=['Occupation'], drop_first=False)\n\nlabel_encoder = LabelEncoder()\ncolumns = ['Gender', 'Sleep Disorder']\nfor col in columns:\n  df[col] = label_encoder.fit_transform(df[col])\n\nnum_col = ['Age', 'Sleep Duration', 'Quality of Sleep', 'Physical Activity Level', 'Stress Level', 'Stress_sleep_interaction',\n          'Sleep_Heart_ratio', 'Sleep_Steps_ratio', 'Sleep_Stress_ratio', 'Heart Rate', 'Daily Steps',\n           'Systolic BP', 'Diastolic BP']\n\n\nX = df.drop('Sleep Disorder', axis=1)\ny = df['Sleep Disorder']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n\n\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:53:22.562724Z","iopub.execute_input":"2025-10-06T08:53:22.563471Z","iopub.status.idle":"2025-10-06T08:53:22.599719Z","shell.execute_reply.started":"2025-10-06T08:53:22.563445Z","shell.execute_reply":"2025-10-06T08:53:22.599131Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# preprocessing 2","metadata":{}},{"cell_type":"code","source":"# ==============================\n# Combined Feature Engineering Pipeline\n# ==============================\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n# ------------------------------\n# Step 1: Load dataset\n# ------------------------------\ndf = pd.read_csv(\"/kaggle/input/sleep-health-and-lifestyle-dataset/Sleep_health_and_lifestyle_dataset.csv\")\ndf.fillna(\"None\", inplace=True)\n\n# ------------------------------\n# Step 2: Basic preprocessing\n# ------------------------------\n# Split Blood Pressure\ndf[['Systolic BP', 'Diastolic BP']] = df['Blood Pressure'].str.split('/', expand=True).astype(int)\ndf.drop(['Person ID', 'Blood Pressure'], axis=1, inplace=True)\n\n# Group rare occupations\ndf['Occupation'] = df['Occupation'].replace(['Manager', 'Sales Representative', 'Scientist', 'Software Engineer'], 'Other')\n\n# Map BMI categories to numeric\ndf['BMI Category'] = df['BMI Category'].replace({'Normal': 22, 'Normal Weight': 22, 'Overweight': 27, 'Obese': 30})\n\n# Label encode categorical columns\nlabel_encoder = LabelEncoder()\nfor col in ['Gender', 'Sleep Disorder']:\n    df[col] = label_encoder.fit_transform(df[col])\n\n# One-hot encode Occupation\ndf = pd.get_dummies(df, columns=['Occupation'], drop_first=False)\n\n# ------------------------------\n# Step 3: Feature Engineering\n# ------------------------------\nepsilon = 1e-6\n\n# ---- Ratio & difference features ----\ndf['Sleep_Heart_ratio'] = df['Sleep Duration'] / (df['Heart Rate'] + epsilon)\ndf['Sleep_Steps_ratio'] = df['Sleep Duration'] / (df['Daily Steps'] + epsilon)\ndf['Stress_Activity_ratio'] = df['Stress Level'] / (df['Physical Activity Level'] + epsilon)\ndf['Heart_Sleep_diff'] = df['Heart Rate'] - df['Sleep Duration']\n\n\n\n# ---- Aggregation features ----\ndf['Total_Activity'] = df['Physical Activity Level'] + df['Daily Steps']\ndf['Stress_per_Activity'] = df['Stress Level'] / (df['Total_Activity'] + epsilon)\n\n# ---- Interaction features ----\ndf['Sleep_Stress_interaction'] = df['Sleep Duration'] * df['Stress Level']\ndf['BMI_Activity_interaction'] = df['BMI Category'] * df['Physical Activity Level']\n\n# ---- Categorical interaction features ----\ndf['Gender_Occupation'] = df['Gender'].astype(str) + \"_\" + df['Occupation_Other'].astype(str)\ndf = pd.get_dummies(df, columns=['Gender_Occupation'], drop_first=False)\n\n# ------------------------------\n# Step 4: Prepare features & target\n# ------------------------------\nX = df.drop('Sleep Disorder', axis=1)\ny = df['Sleep Disorder']\n\n# ------------------------------\n# Step 5: Train-test split\n# ------------------------------\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# ------------------------------\n# Step 6: Summary\n# ------------------------------\nprint(\"X_train shape:\", X_train.shape)\nprint(\"X_test shape:\", X_test.shape)\nprint(\"y_train distribution:\\n\", y_train.value_counts())\nprint(\"y_test distribution:\\n\", y_test.value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:53:22.600583Z","iopub.execute_input":"2025-10-06T08:53:22.600807Z","iopub.status.idle":"2025-10-06T08:53:22.688518Z","shell.execute_reply.started":"2025-10-06T08:53:22.600789Z","shell.execute_reply":"2025-10-06T08:53:22.687910Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(y.isna().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:53:22.689354Z","iopub.execute_input":"2025-10-06T08:53:22.689650Z","iopub.status.idle":"2025-10-06T08:53:22.693783Z","shell.execute_reply.started":"2025-10-06T08:53:22.689625Z","shell.execute_reply":"2025-10-06T08:53:22.693252Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Outlier removal(optional)","metadata":{}},{"cell_type":"code","source":"# Calculate Q1, Q3, and IQR for numerical columns in X_train\n'''\nQ1 = X_train[num_col].quantile(0.25)\nQ3 = X_train[num_col].quantile(0.75)\nIQR = Q3 - Q1\n\n# Remove outliers from X_train\nX_train = X_train[~((X_train[num_col] < (Q1 - 2 * IQR)) | (X_train[num_col] > (Q3 + 2* IQR))).any(axis=1)]\ny_train = y_train[X_train.index]\n\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:53:22.694444Z","iopub.execute_input":"2025-10-06T08:53:22.694632Z","iopub.status.idle":"2025-10-06T08:53:22.710411Z","shell.execute_reply.started":"2025-10-06T08:53:22.694615Z","shell.execute_reply":"2025-10-06T08:53:22.709795Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# original(robust/normalized)+ sampling","metadata":{}},{"cell_type":"code","source":"# ============================================\n# MinMaxScaler Configurations\n# ============================================\nscaler = MinMaxScaler()\nX_train_normalized = scaler.fit_transform(X_train)\nX_test_normalized = scaler.transform(X_test)\n\n# MinMaxScaler + SVMSMOTE\nsvmsmote_normalized = SVMSMOTE(random_state=42, k_neighbors=5)\nX_train_normalized_svmsmote, y_train_normalized_svmsmote = svmsmote_normalized.fit_resample(X_train_normalized, y_train)\nX_test_normalized_svmsmote = X_test_normalized.copy()\n\n# MinMaxScaler + BorderlineSMOTE\nbordersmote_normalized = BorderlineSMOTE(random_state=42, k_neighbors=5)\nX_train_normalized_bordersmote, y_train_normalized_bordersmote = bordersmote_normalized.fit_resample(X_train_normalized, y_train)\nX_test_normalized_bordersmote = X_test_normalized.copy()\n\n# MinMaxScaler + RandomOverSampler\nros_normalized = RandomOverSampler(random_state=42)\nX_train_normalized_ros, y_train_normalized_ros = ros_normalized.fit_resample(X_train_normalized, y_train)\nX_test_normalized_ros = X_test_normalized.copy()\n\n# MinMaxScaler + SMOTE\nsmote_normalized = SMOTE(random_state=42, k_neighbors=5)\nX_train_normalized_smote, y_train_normalized_smote = smote_normalized.fit_resample(X_train_normalized, y_train)\nX_test_normalized_smote = X_test_normalized.copy()\n\n# MinMaxScaler + SMOTE+Tomek\nsmotetomek_normalized = SMOTETomek(random_state=42)\nX_train_normalized_smotetomek, y_train_normalized_smotetomek = smotetomek_normalized.fit_resample(X_train_normalized, y_train)\nX_test_normalized_smotetomek = X_test_normalized.copy()\n\n# MinMaxScaler + SMOTE+ENN\nsmoteenn_normalized = SMOTEENN(random_state=42)\nX_train_normalized_smoteenn, y_train_normalized_smoteenn = smoteenn_normalized.fit_resample(X_train_normalized, y_train)\nX_test_normalized_smoteenn = X_test_normalized.copy()\n\n# ============================================\n# RobustScaler Configurations\n# ============================================\nscaler = RobustScaler()\nX_train_robust = scaler.fit_transform(X_train)\nX_test_robust = scaler.transform(X_test)\n\n# RobustScaler + SVMSMOTE\nsvmsmote_robust = SVMSMOTE(random_state=42, k_neighbors=5)\nX_train_robust_svmsmote, y_train_robust_svmsmote = svmsmote_robust.fit_resample(X_train_robust, y_train)\nX_test_robust_svmsmote = X_test_robust.copy()\n\n# RobustScaler + BorderlineSMOTE\nbordersmote_robust = BorderlineSMOTE(random_state=42, k_neighbors=5)\nX_train_robust_bordersmote, y_train_robust_bordersmote = bordersmote_robust.fit_resample(X_train_robust, y_train)\nX_test_robust_bordersmote = X_test_robust.copy()\n\n# RobustScaler + RandomOverSampler\nros_robust = RandomOverSampler(random_state=42)\nX_train_robust_ros, y_train_robust_ros = ros_robust.fit_resample(X_train_robust, y_train)\nX_test_robust_ros = X_test_robust.copy()\n\n# RobustScaler + SMOTE\nsmote_robust = SMOTE(random_state=42, k_neighbors=5)\nX_train_robust_smote, y_train_robust_smote = smote_robust.fit_resample(X_train_robust, y_train)\nX_test_robust_smote = X_test_robust.copy()\n\n# RobustScaler + SMOTE+Tomek\nsmotetomek_robust = SMOTETomek(random_state=42)\nX_train_robust_smotetomek, y_train_robust_smotetomek = smotetomek_robust.fit_resample(X_train_robust, y_train)\nX_test_robust_smotetomek = X_test_robust.copy()\n\n# RobustScaler + SMOTE+ENN\nsmoteenn_robust = SMOTEENN(random_state=42)\nX_train_robust_smoteenn, y_train_robust_smoteenn = smoteenn_robust.fit_resample(X_train_robust, y_train)\nX_test_robust_smoteenn = X_test_robust.copy()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:53:22.711054Z","iopub.execute_input":"2025-10-06T08:53:22.711421Z","iopub.status.idle":"2025-10-06T08:53:22.868348Z","shell.execute_reply.started":"2025-10-06T08:53:22.711401Z","shell.execute_reply":"2025-10-06T08:53:22.867732Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# Original - RobustScaler → Mutual Information → LDA\n\n","metadata":{}},{"cell_type":"markdown","source":"## basic","metadata":{}},{"cell_type":"code","source":"scaler = RobustScaler()\nX_train_robust = scaler.fit_transform(X_train)\nX_test_robust = scaler.transform(X_test)\n\n# Feature selection using Mutual Information\nmi = SelectKBest(score_func=mutual_info_classif, k=5)\nX_train_mi = mi.fit_transform(X_train_robust, y_train)\nX_test_mi = mi.transform(X_test_robust)\n\n# Applying Linear Discriminant Analysis (LDA)\nlda = LinearDiscriminantAnalysis(n_components=2)\nX_train_lda = lda.fit_transform(X_train_mi, y_train)\nX_test_lda = lda.transform(X_test_mi)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:53:22.869169Z","iopub.execute_input":"2025-10-06T08:53:22.869416Z","iopub.status.idle":"2025-10-06T08:53:23.008535Z","shell.execute_reply.started":"2025-10-06T08:53:22.869397Z","shell.execute_reply":"2025-10-06T08:53:23.007992Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## basic+sampling","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SVMSMOTE, BorderlineSMOTE, RandomOverSampler, SMOTE\nfrom imblearn.combine import SMOTETomek, SMOTEENN\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\n\n# ============================================\n# Configuration 1: SVMSMOTE\n# ============================================\nsvmsmote = SVMSMOTE(random_state=42, k_neighbors=5)\nX_train_svmsmote_temp, y_train_svmsmote = svmsmote.fit_resample(X_train, y_train)\n\nscaler_svmsmote = RobustScaler()\nX_train_svmsmote_scaled = scaler_svmsmote.fit_transform(X_train_svmsmote_temp)\nX_test_svmsmote_scaled = scaler_svmsmote.transform(X_test)\n\nmi_svmsmote = SelectKBest(score_func=mutual_info_classif, k=5)\nX_train_svmsmote_mi = mi_svmsmote.fit_transform(X_train_svmsmote_scaled, y_train_svmsmote)\nX_test_svmsmote_mi = mi_svmsmote.transform(X_test_svmsmote_scaled)\n\nlda_svmsmote = LinearDiscriminantAnalysis(n_components=2)\nX_train_lda_svmsmote = lda_svmsmote.fit_transform(X_train_svmsmote_mi, y_train_svmsmote)\nX_test_lda_svmsmote = lda_svmsmote.transform(X_test_svmsmote_mi)\n\n# ============================================\n# Configuration 2: BorderlineSMOTE\n# ============================================\nbordersmote = BorderlineSMOTE(random_state=42, k_neighbors=5)\nX_train_bordersmote_temp, y_train_bordersmote = bordersmote.fit_resample(X_train, y_train)\n\nscaler_bordersmote = RobustScaler()\nX_train_bordersmote_scaled = scaler_bordersmote.fit_transform(X_train_bordersmote_temp)\nX_test_bordersmote_scaled = scaler_bordersmote.transform(X_test)\n\nmi_bordersmote = SelectKBest(score_func=mutual_info_classif, k=5)\nX_train_bordersmote_mi = mi_bordersmote.fit_transform(X_train_bordersmote_scaled, y_train_bordersmote)\nX_test_bordersmote_mi = mi_bordersmote.transform(X_test_bordersmote_scaled)\n\nlda_bordersmote = LinearDiscriminantAnalysis(n_components=2)\nX_train_lda_bordersmote = lda_bordersmote.fit_transform(X_train_bordersmote_mi, y_train_bordersmote)\nX_test_lda_bordersmote = lda_bordersmote.transform(X_test_bordersmote_mi)\n\n# ============================================\n# Configuration 3: RandomOverSampler\n# ============================================\nros = RandomOverSampler(random_state=42)\nX_train_ros_temp, y_train_ros = ros.fit_resample(X_train, y_train)\n\nscaler_ros = RobustScaler()\nX_train_ros_scaled = scaler_ros.fit_transform(X_train_ros_temp)\nX_test_ros_scaled = scaler_ros.transform(X_test)\n\nmi_ros = SelectKBest(score_func=mutual_info_classif, k=5)\nX_train_ros_mi = mi_ros.fit_transform(X_train_ros_scaled, y_train_ros)\nX_test_ros_mi = mi_ros.transform(X_test_ros_scaled)\n\nlda_ros = LinearDiscriminantAnalysis(n_components=2)\nX_train_lda_ros = lda_ros.fit_transform(X_train_ros_mi, y_train_ros)\nX_test_lda_ros = lda_ros.transform(X_test_ros_mi)\n\n# ============================================\n# Configuration 4: SMOTE\n# ============================================\nsmote = SMOTE(random_state=42, k_neighbors=5)\nX_train_smote_temp, y_train_smote = smote.fit_resample(X_train, y_train)\n\nscaler_smote = RobustScaler()\nX_train_smote_scaled = scaler_smote.fit_transform(X_train_smote_temp)\nX_test_smote_scaled = scaler_smote.transform(X_test)\n\nmi_smote = SelectKBest(score_func=mutual_info_classif, k=5)\nX_train_smote_mi = mi_smote.fit_transform(X_train_smote_scaled, y_train_smote)\nX_test_smote_mi = mi_smote.transform(X_test_smote_scaled)\n\nlda_smote = LinearDiscriminantAnalysis(n_components=2)\nX_train_lda_smote = lda_smote.fit_transform(X_train_smote_mi, y_train_smote)\nX_test_lda_smote = lda_smote.transform(X_test_smote_mi)\n\n# ============================================\n# Configuration 5: SMOTE + Tomek\n# ============================================\nsmote_tomek = SMOTETomek(random_state=42)\nX_train_smotetomek_temp, y_train_smotetomek = smote_tomek.fit_resample(X_train, y_train)\n\nscaler_smotetomek = RobustScaler()\nX_train_smotetomek_scaled = scaler_smotetomek.fit_transform(X_train_smotetomek_temp)\nX_test_smotetomek_scaled = scaler_smotetomek.transform(X_test)\n\nmi_smotetomek = SelectKBest(score_func=mutual_info_classif, k=5)\nX_train_smotetomek_mi = mi_smotetomek.fit_transform(X_train_smotetomek_scaled, y_train_smotetomek)\nX_test_smotetomek_mi = mi_smotetomek.transform(X_test_smotetomek_scaled)\n\nlda_smotetomek = LinearDiscriminantAnalysis(n_components=2)\nX_train_lda_smotetomek = lda_smotetomek.fit_transform(X_train_smotetomek_mi, y_train_smotetomek)\nX_test_lda_smotetomek = lda_smotetomek.transform(X_test_smotetomek_mi)\n\n# ============================================\n# Configuration 6: SMOTE + ENN\n# ============================================\nsmote_enn = SMOTEENN(random_state=42)\nX_train_smoteenn_temp, y_train_smoteenn = smote_enn.fit_resample(X_train, y_train)\n\nscaler_smoteenn = RobustScaler()\nX_train_smoteenn_scaled = scaler_smoteenn.fit_transform(X_train_smoteenn_temp)\nX_test_smoteenn_scaled = scaler_smoteenn.transform(X_test)\n\nmi_smoteenn = SelectKBest(score_func=mutual_info_classif, k=5)\nX_train_smoteenn_mi = mi_smoteenn.fit_transform(X_train_smoteenn_scaled, y_train_smoteenn)\nX_test_smoteenn_mi = mi_smoteenn.transform(X_test_smoteenn_scaled)\n\nlda_smoteenn = LinearDiscriminantAnalysis(n_components=2)\nX_train_lda_smoteenn = lda_smoteenn.fit_transform(X_train_smoteenn_mi, y_train_smoteenn)\nX_test_lda_smoteenn = lda_smoteenn.transform(X_test_smoteenn_mi)\n\n# ============================================\n# Summary\n# ============================================\nprint(\"Configuration shapes:\")\nprint(f\"Original LDA - Train: {X_train_lda.shape}, Test: {X_test_lda.shape}\")\nprint(f\"SVMSMOTE - Train: {X_train_lda_svmsmote.shape}, Test: {X_test_lda_svmsmote.shape}\")\nprint(f\"BorderlineSMOTE - Train: {X_train_lda_bordersmote.shape}, Test: {X_test_lda_bordersmote.shape}\")\nprint(f\"RandomOverSampler - Train: {X_train_lda_ros.shape}, Test: {X_test_lda_ros.shape}\")\nprint(f\"SMOTE - Train: {X_train_lda_smote.shape}, Test: {X_test_lda_smote.shape}\")\nprint(f\"SMOTE+Tomek - Train: {X_train_lda_smotetomek.shape}, Test: {X_test_lda_smotetomek.shape}\")\nprint(f\"SMOTE+ENN - Train: {X_train_lda_smoteenn.shape}, Test: {X_test_lda_smoteenn.shape}\")\n\nconfigs = {\n    'LDA_Original': {\n        'X_train': X_train_lda,\n        'y_train': y_train,\n        'X_test': X_test_lda\n    },\n    'LDA_SVMSMOTE': {\n        'X_train': X_train_lda_svmsmote,\n        'y_train': y_train_svmsmote,\n        'X_test': X_test_lda_svmsmote\n    },\n    'LDA_BorderlineSMOTE': {\n        'X_train': X_train_lda_bordersmote,\n        'y_train': y_train_bordersmote,\n        'X_test': X_test_lda_bordersmote\n    },\n    'LDA_RandomOverSampler': {\n        'X_train': X_train_lda_ros,\n        'y_train': y_train_ros,\n        'X_test': X_test_lda_ros\n    },\n    'LDA_SMOTE': {\n        'X_train': X_train_lda_smote,\n        'y_train': y_train_smote,\n        'X_test': X_test_lda_smote\n    },\n    'LDA_SMOTE+Tomek': {\n        'X_train': X_train_lda_smotetomek,\n        'y_train': y_train_smotetomek,\n        'X_test': X_test_lda_smotetomek\n    },\n    'LDA_SMOTE+ENN': {\n        'X_train': X_train_lda_smoteenn,\n        'y_train': y_train_smoteenn,\n        'X_test': X_test_lda_smoteenn\n    }\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:53:23.010964Z","iopub.execute_input":"2025-10-06T08:53:23.011153Z","iopub.status.idle":"2025-10-06T08:53:23.789180Z","shell.execute_reply.started":"2025-10-06T08:53:23.011138Z","shell.execute_reply":"2025-10-06T08:53:23.788262Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Original - MinMaxScaler → Boruta → Autoencoder","metadata":{}},{"cell_type":"markdown","source":"## basic","metadata":{}},{"cell_type":"code","source":"# Imports\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom boruta import BorutaPy\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# -----------------------------\n# RandomForest classifier (for Boruta feature selection)\n# -----------------------------\nscaler = MinMaxScaler()\nrfc = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Normalize data\nX_train_normalized = scaler.fit_transform(X_train)\nX_test_normalized = scaler.transform(X_test)\n\n# -----------------------------\n# Boruta Feature Selection\n# -----------------------------\nboruta_selector = BorutaPy(rfc, n_estimators='auto', verbose=0, random_state=42)\n\nX_train_boruta = boruta_selector.fit_transform(X_train_normalized, y_train)\nX_test_boruta = boruta_selector.transform(X_test_normalized)\n\n# -----------------------------\n# Autoencoder Architecture\n# -----------------------------\nn_features = X_train_boruta.shape[1]\ninput_layer = Input(shape=(n_features,))\n\n# Encoder\nencoded = Dense(32, activation='relu')(input_layer)\nbottleneck = Dense(16, activation='relu')(encoded)\n\n# Decoder\ndecoded = Dense(32, activation='relu')(bottleneck)\noutput_layer = Dense(n_features, activation='sigmoid')(decoded)\n\n# Full autoencoder model\nautoencoder = Model(inputs=input_layer, outputs=output_layer)\nautoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n\n# Train the autoencoder\nautoencoder.fit(X_train_boruta, X_train_boruta, epochs=10, batch_size=32, verbose=0)\n\n# Encoder model to extract bottleneck features\nencoder = Model(inputs=input_layer, outputs=bottleneck)\n\n# Transform data\nX_train_encoded = encoder.predict(X_train_boruta)\nX_test_encoded = encoder.predict(X_test_boruta)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:53:23.790094Z","iopub.execute_input":"2025-10-06T08:53:23.790376Z","iopub.status.idle":"2025-10-06T08:53:38.685813Z","shell.execute_reply.started":"2025-10-06T08:53:23.790350Z","shell.execute_reply":"2025-10-06T08:53:38.685202Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## basic +sampling","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SVMSMOTE, BorderlineSMOTE, RandomOverSampler, SMOTE\nfrom imblearn.combine import SMOTETomek\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom boruta import BorutaPy\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.optimizers import Adam\n\nfrom imblearn.over_sampling import SVMSMOTE, BorderlineSMOTE, RandomOverSampler, SMOTE\nfrom imblearn.combine import SMOTETomek, SMOTEENN\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom boruta import BorutaPy\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.optimizers import Adam\n\n# ============================================\n# Configuration 1: SVMSMOTE\n# ============================================\nsvmsmote = SVMSMOTE(random_state=42, k_neighbors=5)\nX_train_svmsmote_temp, y_train_svmsmote = svmsmote.fit_resample(X_train, y_train)\n\nscaler_svmsmote = MinMaxScaler()\nX_train_svmsmote_normalized = scaler_svmsmote.fit_transform(X_train_svmsmote_temp)\nX_test_svmsmote_normalized = scaler_svmsmote.transform(X_test)\n\nrfc_svmsmote = RandomForestClassifier(n_estimators=100, random_state=42)\nboruta_svmsmote = BorutaPy(rfc_svmsmote, n_estimators='auto', verbose=0, random_state=42)\nX_train_svmsmote_boruta = boruta_svmsmote.fit_transform(X_train_svmsmote_normalized, y_train_svmsmote)\nX_test_svmsmote_boruta = boruta_svmsmote.transform(X_test_svmsmote_normalized)\n\nn_features_svmsmote = X_train_svmsmote_boruta.shape[1]\ninput_layer_svmsmote = Input(shape=(n_features_svmsmote,))\nencoded_svmsmote = Dense(32, activation='relu')(input_layer_svmsmote)\nbottleneck_svmsmote = Dense(16, activation='relu')(encoded_svmsmote)\ndecoded_svmsmote = Dense(32, activation='relu')(bottleneck_svmsmote)\noutput_layer_svmsmote = Dense(n_features_svmsmote, activation='sigmoid')(decoded_svmsmote)\n\nautoencoder_svmsmote = Model(inputs=input_layer_svmsmote, outputs=output_layer_svmsmote)\nautoencoder_svmsmote.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\nautoencoder_svmsmote.fit(X_train_svmsmote_boruta, X_train_svmsmote_boruta, epochs=10, batch_size=32, verbose=0)\n\nencoder_svmsmote = Model(inputs=input_layer_svmsmote, outputs=bottleneck_svmsmote)\nX_train_encoded_svmsmote = encoder_svmsmote.predict(X_train_svmsmote_boruta)\nX_test_encoded_svmsmote = encoder_svmsmote.predict(X_test_svmsmote_boruta)\n\n# ============================================\n# Configuration 2: BorderlineSMOTE\n# ============================================\nbordersmote = BorderlineSMOTE(random_state=42, k_neighbors=5)\nX_train_bordersmote_temp, y_train_bordersmote = bordersmote.fit_resample(X_train, y_train)\n\nscaler_bordersmote = MinMaxScaler()\nX_train_bordersmote_normalized = scaler_bordersmote.fit_transform(X_train_bordersmote_temp)\nX_test_bordersmote_normalized = scaler_bordersmote.transform(X_test)\n\nrfc_bordersmote = RandomForestClassifier(n_estimators=100, random_state=42)\nboruta_bordersmote = BorutaPy(rfc_bordersmote, n_estimators='auto', verbose=0, random_state=42)\nX_train_bordersmote_boruta = boruta_bordersmote.fit_transform(X_train_bordersmote_normalized, y_train_bordersmote)\nX_test_bordersmote_boruta = boruta_bordersmote.transform(X_test_bordersmote_normalized)\n\nn_features_bordersmote = X_train_bordersmote_boruta.shape[1]\ninput_layer_bordersmote = Input(shape=(n_features_bordersmote,))\nencoded_bordersmote = Dense(32, activation='relu')(input_layer_bordersmote)\nbottleneck_bordersmote = Dense(16, activation='relu')(encoded_bordersmote)\ndecoded_bordersmote = Dense(32, activation='relu')(bottleneck_bordersmote)\noutput_layer_bordersmote = Dense(n_features_bordersmote, activation='sigmoid')(decoded_bordersmote)\n\nautoencoder_bordersmote = Model(inputs=input_layer_bordersmote, outputs=output_layer_bordersmote)\nautoencoder_bordersmote.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\nautoencoder_bordersmote.fit(X_train_bordersmote_boruta, X_train_bordersmote_boruta, epochs=10, batch_size=32, verbose=0)\n\nencoder_bordersmote = Model(inputs=input_layer_bordersmote, outputs=bottleneck_bordersmote)\nX_train_encoded_bordersmote = encoder_bordersmote.predict(X_train_bordersmote_boruta)\nX_test_encoded_bordersmote = encoder_bordersmote.predict(X_test_bordersmote_boruta)\n\n# ============================================\n# Configuration 3: RandomOverSampler\n# ============================================\nros = RandomOverSampler(random_state=42)\nX_train_ros_temp, y_train_ros = ros.fit_resample(X_train, y_train)\n\nscaler_ros = MinMaxScaler()\nX_train_ros_normalized = scaler_ros.fit_transform(X_train_ros_temp)\nX_test_ros_normalized = scaler_ros.transform(X_test)\n\nrfc_ros = RandomForestClassifier(n_estimators=100, random_state=42)\nboruta_ros = BorutaPy(rfc_ros, n_estimators='auto', verbose=0, random_state=42)\nX_train_ros_boruta = boruta_ros.fit_transform(X_train_ros_normalized, y_train_ros)\nX_test_ros_boruta = boruta_ros.transform(X_test_ros_normalized)\n\nn_features_ros = X_train_ros_boruta.shape[1]\ninput_layer_ros = Input(shape=(n_features_ros,))\nencoded_ros = Dense(32, activation='relu')(input_layer_ros)\nbottleneck_ros = Dense(16, activation='relu')(encoded_ros)\ndecoded_ros = Dense(32, activation='relu')(bottleneck_ros)\noutput_layer_ros = Dense(n_features_ros, activation='sigmoid')(decoded_ros)\n\nautoencoder_ros = Model(inputs=input_layer_ros, outputs=output_layer_ros)\nautoencoder_ros.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\nautoencoder_ros.fit(X_train_ros_boruta, X_train_ros_boruta, epochs=10, batch_size=32, verbose=0)\n\nencoder_ros = Model(inputs=input_layer_ros, outputs=bottleneck_ros)\nX_train_encoded_ros = encoder_ros.predict(X_train_ros_boruta)\nX_test_encoded_ros = encoder_ros.predict(X_test_ros_boruta)\n\n# ============================================\n# Configuration 4: SMOTE\n# ============================================\nsmote = SMOTE(random_state=42, k_neighbors=5)\nX_train_smote_temp, y_train_smote = smote.fit_resample(X_train, y_train)\n\nscaler_smote = MinMaxScaler()\nX_train_smote_normalized = scaler_smote.fit_transform(X_train_smote_temp)\nX_test_smote_normalized = scaler_smote.transform(X_test)\n\nrfc_smote = RandomForestClassifier(n_estimators=100, random_state=42)\nboruta_smote = BorutaPy(rfc_smote, n_estimators='auto', verbose=0, random_state=42)\nX_train_smote_boruta = boruta_smote.fit_transform(X_train_smote_normalized, y_train_smote)\nX_test_smote_boruta = boruta_smote.transform(X_test_smote_normalized)\n\nn_features_smote = X_train_smote_boruta.shape[1]\ninput_layer_smote = Input(shape=(n_features_smote,))\nencoded_smote = Dense(32, activation='relu')(input_layer_smote)\nbottleneck_smote = Dense(16, activation='relu')(encoded_smote)\ndecoded_smote = Dense(32, activation='relu')(bottleneck_smote)\noutput_layer_smote = Dense(n_features_smote, activation='sigmoid')(decoded_smote)\n\nautoencoder_smote = Model(inputs=input_layer_smote, outputs=output_layer_smote)\nautoencoder_smote.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\nautoencoder_smote.fit(X_train_smote_boruta, X_train_smote_boruta, epochs=10, batch_size=32, verbose=0)\n\nencoder_smote = Model(inputs=input_layer_smote, outputs=bottleneck_smote)\nX_train_encoded_smote = encoder_smote.predict(X_train_smote_boruta)\nX_test_encoded_smote = encoder_smote.predict(X_test_smote_boruta)\n\n# ============================================\n# Configuration 5: SMOTE + Tomek\n# ============================================\nsmote_tomek = SMOTETomek(random_state=42)\nX_train_smotetomek_temp, y_train_smotetomek = smote_tomek.fit_resample(X_train, y_train)\n\nscaler_smotetomek = MinMaxScaler()\nX_train_smotetomek_normalized = scaler_smotetomek.fit_transform(X_train_smotetomek_temp)\nX_test_smotetomek_normalized = scaler_smotetomek.transform(X_test)\n\nrfc_smotetomek = RandomForestClassifier(n_estimators=100, random_state=42)\nboruta_smotetomek = BorutaPy(rfc_smotetomek, n_estimators='auto', verbose=0, random_state=42)\nX_train_smotetomek_boruta = boruta_smotetomek.fit_transform(X_train_smotetomek_normalized, y_train_smotetomek)\nX_test_smotetomek_boruta = boruta_smotetomek.transform(X_test_smotetomek_normalized)\n\nn_features_smotetomek = X_train_smotetomek_boruta.shape[1]\ninput_layer_smotetomek = Input(shape=(n_features_smotetomek,))\nencoded_smotetomek = Dense(32, activation='relu')(input_layer_smotetomek)\nbottleneck_smotetomek = Dense(16, activation='relu')(encoded_smotetomek)\ndecoded_smotetomek = Dense(32, activation='relu')(bottleneck_smotetomek)\noutput_layer_smotetomek = Dense(n_features_smotetomek, activation='sigmoid')(decoded_smotetomek)\n\nautoencoder_smotetomek = Model(inputs=input_layer_smotetomek, outputs=output_layer_smotetomek)\nautoencoder_smotetomek.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\nautoencoder_smotetomek.fit(X_train_smotetomek_boruta, X_train_smotetomek_boruta, epochs=10, batch_size=32, verbose=0)\n\nencoder_smotetomek = Model(inputs=input_layer_smotetomek, outputs=bottleneck_smotetomek)\nX_train_encoded_smotetomek = encoder_smotetomek.predict(X_train_smotetomek_boruta)\nX_test_encoded_smotetomek = encoder_smotetomek.predict(X_test_smotetomek_boruta)\n\n# ============================================\n# Configuration 6: SMOTE + ENN\n# ============================================\nsmote_enn = SMOTEENN(random_state=42)\nX_train_smoteenn_temp, y_train_smoteenn = smote_enn.fit_resample(X_train, y_train)\n\nscaler_smoteenn = MinMaxScaler()\nX_train_smoteenn_normalized = scaler_smoteenn.fit_transform(X_train_smoteenn_temp)\nX_test_smoteenn_normalized = scaler_smoteenn.transform(X_test)\n\nrfc_smoteenn = RandomForestClassifier(n_estimators=100, random_state=42)\nboruta_smoteenn = BorutaPy(rfc_smoteenn, n_estimators='auto', verbose=0, random_state=42)\nX_train_smoteenn_boruta = boruta_smoteenn.fit_transform(X_train_smoteenn_normalized, y_train_smoteenn)\nX_test_smoteenn_boruta = boruta_smoteenn.transform(X_test_smoteenn_normalized)\n\nn_features_smoteenn = X_train_smoteenn_boruta.shape[1]\ninput_layer_smoteenn = Input(shape=(n_features_smoteenn,))\nencoded_smoteenn = Dense(32, activation='relu')(input_layer_smoteenn)\nbottleneck_smoteenn = Dense(16, activation='relu')(encoded_smoteenn)\ndecoded_smoteenn = Dense(32, activation='relu')(bottleneck_smoteenn)\noutput_layer_smoteenn = Dense(n_features_smoteenn, activation='sigmoid')(decoded_smoteenn)\n\nautoencoder_smoteenn = Model(inputs=input_layer_smoteenn, outputs=output_layer_smoteenn)\nautoencoder_smoteenn.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\nautoencoder_smoteenn.fit(X_train_smoteenn_boruta, X_train_smoteenn_boruta, epochs=10, batch_size=32, verbose=0)\n\nencoder_smoteenn = Model(inputs=input_layer_smoteenn, outputs=bottleneck_smoteenn)\nX_train_encoded_smoteenn = encoder_smoteenn.predict(X_train_smoteenn_boruta)\nX_test_encoded_smoteenn = encoder_smoteenn.predict(X_test_smoteenn_boruta)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:53:38.686886Z","iopub.execute_input":"2025-10-06T08:53:38.687159Z","iopub.status.idle":"2025-10-06T08:54:57.621522Z","shell.execute_reply.started":"2025-10-06T08:53:38.687132Z","shell.execute_reply":"2025-10-06T08:54:57.620914Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# boruta + sampling","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SVMSMOTE, BorderlineSMOTE, RandomOverSampler, SMOTE\nfrom imblearn.combine import SMOTETomek, SMOTEENN\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom boruta import BorutaPy\n\n# ============================================\n# Boruta Feature Selection Configurations\n# ============================================\nscaler = MinMaxScaler()\nrfc = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Normalize data\nX_train_normalized = scaler.fit_transform(X_train)\nX_test_normalized = scaler.transform(X_test)\n\n# Boruta Feature Selection (Baseline)\nboruta_selector = BorutaPy(rfc, n_estimators='auto', verbose=0, random_state=42)\nX_train_boruta = boruta_selector.fit_transform(X_train_normalized, y_train)\nX_test_boruta = boruta_selector.transform(X_test_normalized)\n\n# Boruta + SVMSMOTE\nsvmsmote_boruta = SVMSMOTE(random_state=42, k_neighbors=5)\nX_train_boruta_svmsmote, y_train_boruta_svmsmote = svmsmote_boruta.fit_resample(X_train_boruta, y_train)\nX_test_boruta_svmsmote = X_test_boruta.copy()\n\n# Boruta + BorderlineSMOTE\nbordersmote_boruta = BorderlineSMOTE(random_state=42, k_neighbors=5)\nX_train_boruta_bordersmote, y_train_boruta_bordersmote = bordersmote_boruta.fit_resample(X_train_boruta, y_train)\nX_test_boruta_bordersmote = X_test_boruta.copy()\n\n# Boruta + RandomOverSampler\nros_boruta = RandomOverSampler(random_state=42)\nX_train_boruta_ros, y_train_boruta_ros = ros_boruta.fit_resample(X_train_boruta, y_train)\nX_test_boruta_ros = X_test_boruta.copy()\n\n# Boruta + SMOTE\nsmote_boruta = SMOTE(random_state=42, k_neighbors=5)\nX_train_boruta_smote, y_train_boruta_smote = smote_boruta.fit_resample(X_train_boruta, y_train)\nX_test_boruta_smote = X_test_boruta.copy()\n\n# Boruta + SMOTE+Tomek\n\nsmotetomek_boruta = SMOTETomek(random_state=42)\nX_train_boruta_smotetomek, y_train_boruta_smotetomek = smotetomek_boruta.fit_resample(X_train_boruta, y_train)\nX_test_boruta_smotetomek = X_test_boruta.copy()\n\n# Boruta + SMOTE+ENN\nsmoteenn_boruta = SMOTEENN(random_state=42)\nX_train_boruta_smoteenn, y_train_boruta_smoteenn = smoteenn_boruta.fit_resample(X_train_boruta, y_train)\nX_test_boruta_smoteenn = X_test_boruta.copy()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:54:57.622500Z","iopub.execute_input":"2025-10-06T08:54:57.622751Z","iopub.status.idle":"2025-10-06T08:55:06.415333Z","shell.execute_reply.started":"2025-10-06T08:54:57.622725Z","shell.execute_reply":"2025-10-06T08:55:06.414718Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# mi + sampling","metadata":{}},{"cell_type":"code","source":"# ============================================\n# Mutual Information Feature Selection Configurations\n# ============================================\nscaler = RobustScaler()\nX_train_robust = scaler.fit_transform(X_train)\nX_test_robust = scaler.transform(X_test)\n\n# Mutual Information Feature Selection (Baseline)\nmi = SelectKBest(score_func=mutual_info_classif, k=5)\nX_train_mi = mi.fit_transform(X_train_robust, y_train)\nX_test_mi = mi.transform(X_test_robust)\n\n# MI + SVMSMOTE\nsvmsmote_mi = SVMSMOTE(random_state=42, k_neighbors=5)\nX_train_mi_svmsmote, y_train_mi_svmsmote = svmsmote_mi.fit_resample(X_train_mi, y_train)\nX_test_mi_svmsmote = X_test_mi.copy()\n\n# MI + BorderlineSMOTE\nbordersmote_mi = BorderlineSMOTE(random_state=42, k_neighbors=5)\nX_train_mi_bordersmote, y_train_mi_bordersmote = bordersmote_mi.fit_resample(X_train_mi, y_train)\nX_test_mi_bordersmote = X_test_mi.copy()\n\n# MI + RandomOverSampler\nros_mi = RandomOverSampler(random_state=42)\nX_train_mi_ros, y_train_mi_ros = ros_mi.fit_resample(X_train_mi, y_train)\nX_test_mi_ros = X_test_mi.copy()\n\n# MI + SMOTE\nsmote_mi = SMOTE(random_state=42, k_neighbors=5)\nX_train_mi_smote, y_train_mi_smote = smote_mi.fit_resample(X_train_mi, y_train)\nX_test_mi_smote = X_test_mi.copy()\n\n# MI + SMOTE+Tomek\nsmotetomek_mi = SMOTETomek(random_state=42)\nX_train_mi_smotetomek, y_train_mi_smotetomek = smotetomek_mi.fit_resample(X_train_mi, y_train)\nX_test_mi_smotetomek = X_test_mi.copy()\n\n# MI + SMOTE+ENN\nsmoteenn_mi = SMOTEENN(random_state=42)\nX_train_mi_smoteenn, y_train_mi_smoteenn = smoteenn_mi.fit_resample(X_train_mi, y_train)\nX_test_mi_smoteenn = X_test_mi.copy()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:55:06.416191Z","iopub.execute_input":"2025-10-06T08:55:06.416472Z","iopub.status.idle":"2025-10-06T08:55:06.550366Z","shell.execute_reply.started":"2025-10-06T08:55:06.416444Z","shell.execute_reply":"2025-10-06T08:55:06.549773Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Others","metadata":{}},{"cell_type":"code","source":"# ===============================\n# Data manipulation\n# ===============================\nimport numpy as np\nimport pandas as pd\n\n# ===============================\n# Preprocessing\n# ===============================\nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler, PowerTransformer, PolynomialFeatures\nfrom sklearn.impute import SimpleImputer\n\n# ===============================\n# Sampling / Imbalance handling\n# ===============================\nfrom imblearn.over_sampling import SMOTE, SVMSMOTE, BorderlineSMOTE, RandomOverSampler\nfrom imblearn.combine import SMOTETomek, SMOTEENN\n\n# ===============================\n# Modeling\n# ===============================\nfrom sklearn.linear_model import LogisticRegression\n\n# ===============================\n# Calibration / Threshold tuning\n# ===============================\nfrom sklearn.calibration import CalibratedClassifierCV\n\n# ===============================\n# Evaluation / Metrics\n# ===============================\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:55:06.551094Z","iopub.execute_input":"2025-10-06T08:55:06.551288Z","iopub.status.idle":"2025-10-06T08:55:06.559255Z","shell.execute_reply.started":"2025-10-06T08:55:06.551271Z","shell.execute_reply":"2025-10-06T08:55:06.558562Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scaler = PowerTransformer(method='yeo-johnson')\nX_train_power = scaler.fit_transform(X_train)\nX_test_power = scaler.transform(X_test)\n\n# PowerTransformer + SMOTE\nsmote_power = SMOTE(random_state=42, k_neighbors=5)\nX_train_power_smote, y_train_power_smote = smote_power.fit_resample(X_train_power, y_train)\nX_test_power_smote = X_test_power.copy()\n\n# PowerTransformer + SMOTE+Tomek\nsmotetomek_power = SMOTETomek(random_state=42)\nX_train_power_smotetomek, y_train_power_smotetomek = smotetomek_power.fit_resample(X_train_power, y_train)\nX_test_power_smotetomek = X_test_power.copy()\n\n# PowerTransformer + SMOTE+ENN\nsmoteenn_power = SMOTEENN(random_state=42)\nX_train_power_smoteenn, y_train_power_smoteenn = smoteenn_power.fit_resample(X_train_power, y_train)\nX_test_power_smoteenn = X_test_power.copy()\n\n\npoly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\nX_train_poly = poly.fit_transform(X_train)\nX_test_poly = poly.transform(X_test)\n\n# Polynomial + SMOTE\nsmote_poly = SMOTE(random_state=42, k_neighbors=5)\nX_train_poly_smote, y_train_poly_smote = smote_poly.fit_resample(X_train_poly, y_train)\nX_test_poly_smote = X_test_poly.copy()\n\n# Polynomial + SMOTE+Tomek\nsmotetomek_poly = SMOTETomek(random_state=42)\nX_train_poly_smotetomek, y_train_poly_smotetomek = smotetomek_poly.fit_resample(X_train_poly, y_train)\nX_test_poly_smotetomek = X_test_poly.copy()\n\n# Polynomial + SMOTE+ENN\nsmoteenn_poly = SMOTEENN(random_state=42)\nX_train_poly_smoteenn, y_train_poly_smoteenn = smoteenn_poly.fit_resample(X_train_poly, y_train)\nX_test_poly_smoteenn = X_test_poly.copy()\n\n\n# Example: train LogisticRegression directly on any preprocessed data\nfrom sklearn.linear_model import LogisticRegression\n\n# Logistic Regression with balanced class weight\nlr_balanced = LogisticRegression(class_weight='balanced', solver='saga', max_iter=5000)\nlr_balanced.fit(X_train_normalized_smote, y_train_normalized_smote)\n\n# Logistic Regression with balanced + SMOTE (optional)\nsmote_lr = SMOTE(random_state=42)\nX_train_lr, y_train_lr = smote_lr.fit_resample(X_train_normalized, y_train)\nlr_balanced_smote = LogisticRegression(class_weight='balanced', solver='saga', max_iter=5000)\nlr_balanced_smote.fit(X_train_lr, y_train_lr)\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:55:06.560258Z","iopub.execute_input":"2025-10-06T08:55:06.560506Z","iopub.status.idle":"2025-10-06T08:55:06.841775Z","shell.execute_reply.started":"2025-10-06T08:55:06.560484Z","shell.execute_reply":"2025-10-06T08:55:06.841124Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# configuration","metadata":{}},{"cell_type":"code","source":"ML_Model = []\nML_Config = []\naccuracy = []\nf1 = []\nrecall = []\nprecision = []\nauc_roc = []  # Adding a holder for AUC-ROC\n\n# Function to call for storing the results\ndef storeResults(model, config, a, b, c, d, e):\n    \"\"\"\n    Store model performance results\n\n    Parameters:\n    model: Name of the ML model\n    config: Configuration name (preprocessing steps applied)\n    a: Accuracy score\n    b: F1 score\n    c: Recall score\n    d: Precision score\n    e: AUC-ROC score\n    \"\"\"\n    ML_Model.append(model)\n    ML_Config.append(config)\n    accuracy.append(round(a, 6))\n    f1.append(round(b, 6))\n    recall.append(round(c, 6))\n    precision.append(round(d, 6))\n    auc_roc.append(round(e, 6))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:55:06.842529Z","iopub.execute_input":"2025-10-06T08:55:06.842786Z","iopub.status.idle":"2025-10-06T08:55:06.847952Z","shell.execute_reply.started":"2025-10-06T08:55:06.842767Z","shell.execute_reply":"2025-10-06T08:55:06.847150Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"configurations = [\n    ('Original Data', X_train, X_test, y_train),\n    \n    # MinMaxScaler configurations\n    ('Normalized Data', X_train_normalized, X_test_normalized, y_train),\n    ('Normalized+SVMSMOTE', X_train_normalized_svmsmote, X_test_normalized_svmsmote, y_train_normalized_svmsmote),\n    ('Normalized+BorderlineSMOTE', X_train_normalized_bordersmote, X_test_normalized_bordersmote, y_train_normalized_bordersmote),\n    ('Normalized+RandomOverSampler', X_train_normalized_ros, X_test_normalized_ros, y_train_normalized_ros),\n    ('Normalized+SMOTE', X_train_normalized_smote, X_test_normalized_smote, y_train_normalized_smote),\n    ('Normalized+SMOTE+Tomek', X_train_normalized_smotetomek, X_test_normalized_smotetomek, y_train_normalized_smotetomek),\n    ('Normalized+SMOTE+ENN', X_train_normalized_smoteenn, X_test_normalized_smoteenn, y_train_normalized_smoteenn),\n    \n    # RobustScaler configurations\n    ('Robust Data', X_train_robust, X_test_robust, y_train),\n    ('Robust+SVMSMOTE', X_train_robust_svmsmote, X_test_robust_svmsmote, y_train_robust_svmsmote),\n    ('Robust+BorderlineSMOTE', X_train_robust_bordersmote, X_test_robust_bordersmote, y_train_robust_bordersmote),\n    ('Robust+RandomOverSampler', X_train_robust_ros, X_test_robust_ros, y_train_robust_ros),\n    ('Robust+SMOTE', X_train_robust_smote, X_test_robust_smote, y_train_robust_smote),\n    ('Robust+SMOTE+Tomek', X_train_robust_smotetomek, X_test_robust_smotetomek, y_train_robust_smotetomek),\n    ('Robust+SMOTE+ENN', X_train_robust_smoteenn, X_test_robust_smoteenn, y_train_robust_smoteenn),\n    \n    # Mutual Information configurations\n    ('MI', X_train_mi, X_test_mi, y_train),\n    ('MI+SVMSMOTE', X_train_mi_svmsmote, X_test_mi_svmsmote, y_train_mi_svmsmote),\n    ('MI+BorderlineSMOTE', X_train_mi_bordersmote, X_test_mi_bordersmote, y_train_mi_bordersmote),\n    ('MI+RandomOverSampler', X_train_mi_ros, X_test_mi_ros, y_train_mi_ros),\n    ('MI+SMOTE', X_train_mi_smote, X_test_mi_smote, y_train_mi_smote),\n    ('MI+SMOTE+Tomek', X_train_mi_smotetomek, X_test_mi_smotetomek, y_train_mi_smotetomek),\n    ('MI+SMOTE+ENN', X_train_mi_smoteenn, X_test_mi_smoteenn, y_train_mi_smoteenn),\n    \n    # Boruta configurations\n    ('Boruta', X_train_boruta, X_test_boruta, y_train),\n    ('Boruta+SVMSMOTE', X_train_boruta_svmsmote, X_test_boruta_svmsmote, y_train_boruta_svmsmote),\n    ('Boruta+BorderlineSMOTE', X_train_boruta_bordersmote, X_test_boruta_bordersmote, y_train_boruta_bordersmote),\n    ('Boruta+RandomOverSampler', X_train_boruta_ros, X_test_boruta_ros, y_train_boruta_ros),\n    ('Boruta+SMOTE', X_train_boruta_smote, X_test_boruta_smote, y_train_boruta_smote),\n    ('Boruta+SMOTE+Tomek', X_train_boruta_smotetomek, X_test_boruta_smotetomek, y_train_boruta_smotetomek),\n    ('Boruta+SMOTE+ENN', X_train_boruta_smoteenn, X_test_boruta_smoteenn, y_train_boruta_smoteenn),\n    \n    # LDA configurations\n    ('LDA', X_train_lda, X_test_lda, y_train),\n    ('LDA+SVMSMOTE', X_train_lda_svmsmote, X_test_lda_svmsmote, y_train_svmsmote),\n    ('LDA+BorderlineSMOTE', X_train_lda_bordersmote, X_test_lda_bordersmote, y_train_bordersmote),\n    ('LDA+RandomOverSampler', X_train_lda_ros, X_test_lda_ros, y_train_ros),\n    ('LDA+SMOTE', X_train_lda_smote, X_test_lda_smote, y_train_smote),\n    ('LDA+SMOTE+Tomek', X_train_lda_smotetomek, X_test_lda_smotetomek, y_train_smotetomek),\n    ('LDA+SMOTE+ENN', X_train_lda_smoteenn, X_test_lda_smoteenn, y_train_smoteenn),\n    \n    # Autoencoder configurations\n    ('Autoencoder', X_train_encoded, X_test_encoded, y_train),\n    ('Autoencoder+SVMSMOTE', X_train_encoded_svmsmote, X_test_encoded_svmsmote, y_train_svmsmote),\n    ('Autoencoder+BorderlineSMOTE', X_train_encoded_bordersmote, X_test_encoded_bordersmote, y_train_bordersmote),\n    ('Autoencoder+RandomOverSampler', X_train_encoded_ros, X_test_encoded_ros, y_train_ros),\n    ('Autoencoder+SMOTE', X_train_encoded_smote, X_test_encoded_smote, y_train_smote),\n    ('Autoencoder+SMOTE+Tomek', X_train_encoded_smotetomek, X_test_encoded_smotetomek, y_train_smotetomek),\n    ('Autoencoder+SMOTE+ENN', X_train_encoded_smoteenn, X_test_encoded_smoteenn, y_train_smoteenn),\n\n    # PowerTransformer configurations\n    ('PowerTransformer', X_train_power, X_test_power, y_train),\n    ('PowerTransformer+SMOTE', X_train_power_smote, X_test_power_smote, y_train_power_smote),\n    ('PowerTransformer+SMOTE+Tomek', X_train_power_smotetomek, X_test_power_smotetomek, y_train_power_smotetomek),\n    ('PowerTransformer+SMOTE+ENN', X_train_power_smoteenn, X_test_power_smoteenn, y_train_power_smoteenn),\n\n    # PolynomialFeatures configurations\n    ('PolynomialFeatures', X_train_poly, X_test_poly, y_train),\n    ('PolynomialFeatures+SMOTE', X_train_poly_smote, X_test_poly_smote, y_train_poly_smote),\n    ('PolynomialFeatures+SMOTE+Tomek', X_train_poly_smotetomek, X_test_poly_smotetomek, y_train_poly_smotetomek),\n    ('PolynomialFeatures+SMOTE+ENN', X_train_poly_smoteenn, X_test_poly_smoteenn, y_train_poly_smoteenn),\n\n    # ============================================\n    # AdaBoost configurations\n    # ============================================\n    ('AdaBoost Normalized+SMOTE Balanced', X_train_lr, X_test_normalized, y_train_lr),\n    ('AdaBoost Normalized+SMOTE Balanced+Calibrated', X_train_lr, X_test_normalized, y_train_lr)\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:55:06.848740Z","iopub.execute_input":"2025-10-06T08:55:06.849192Z","iopub.status.idle":"2025-10-06T08:55:06.865292Z","shell.execute_reply.started":"2025-10-06T08:55:06.849170Z","shell.execute_reply":"2025-10-06T08:55:06.864580Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model training","metadata":{}},{"cell_type":"markdown","source":"## Logistic regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nimport pandas as pd\n\nfrom imblearn.over_sampling import SVMSMOTE, BorderlineSMOTE, RandomOverSampler, SMOTE\nfrom imblearn.combine import SMOTETomek, SMOTEENN\nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler\n\n\n# ============================================\n# Updated Configuration List\n# ============================================\n\n\n# Print all configurations\nprint(f\"Total configurations: {len(configurations)}\")\nfor i, (name, X_tr, X_te, y_tr) in enumerate(configurations):\n    print(f\"{i+1}. {name}: Train shape={X_tr.shape}, Test shape={X_te.shape}, Train labels={y_tr.shape}\")\n\n# Evaluate Logistic Regression for each configuration\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n   \n    # Train Logistic Regression\n    log_reg = LogisticRegression(max_iter=1000, random_state=42)\n    log_reg.fit(X_train_cfg, y_train_cfg)\n\n    # Predictions\n    y_train_lr = log_reg.predict(X_train_cfg)\n    y_test_lr = log_reg.predict(X_test_cfg)\n    y_train_lr_proba = log_reg.predict_proba(X_train_cfg)\n    y_test_lr_proba = log_reg.predict_proba(X_test_cfg)\n\n    # Compute metrics\n    metrics_dict = {\n        \"Dataset\": [\"Training\", \"Test\"],\n        \"Accuracy\": [\n            metrics.accuracy_score(y_train_cfg, y_train_lr),\n            metrics.accuracy_score(y_test, y_test_lr),\n        ],\n        \"F1 Score\": [\n            metrics.f1_score(y_train_cfg, y_train_lr, average='macro'),\n            metrics.f1_score(y_test, y_test_lr, average='macro'),\n        ],\n        \"Recall\": [\n            metrics.recall_score(y_train_cfg, y_train_lr, average='macro'),\n            metrics.recall_score(y_test, y_test_lr, average='macro'),\n        ],\n        \"Precision\": [\n            metrics.precision_score(y_train_cfg, y_train_lr, average='macro'),\n            metrics.precision_score(y_test, y_test_lr, average='macro'),\n        ],\n        \"AUC-ROC\": [\n            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_lr_proba, multi_class='ovr', average='macro'),\n            metrics.roc_auc_score(pd.get_dummies(y_test), y_test_lr_proba, multi_class='ovr', average='macro'),\n        ]\n    }\n\n    df_metrics = pd.DataFrame(metrics_dict)\n\n\n\n    # Store results (assuming storeResults function exists)\n    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_lr_proba, multi_class='ovr', average='macro')\n    storeResults(\n        'Logistic Regression',\n        name,\n        metrics.accuracy_score(y_test, y_test_lr),\n        metrics.f1_score(y_test, y_test_lr, average='macro'),\n        metrics.recall_score(y_test, y_test_lr, average='macro'),\n        metrics.precision_score(y_test, y_test_lr, average='macro'),\n        auc_score\n    )\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\n# List to store results\nresults = []\n\n# Evaluate Logistic Regression for each configuration\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n    log_reg = LogisticRegression(max_iter=1000, random_state=42)\n    log_reg.fit(X_train_cfg, y_train_cfg)\n\n    y_test_pred = log_reg.predict(X_test_cfg)\n\n    test_accuracy = metrics.accuracy_score(y_test, y_test_pred)\n    \n    # Append the name and test accuracy\n    results.append((name, test_accuracy))\n\n# Sort results by test accuracy descending\nresults_sorted = sorted(results, key=lambda x: x[1], reverse=True)\n\n# Print results\nprint(\"Configurations sorted by Test Accuracy (High to Low):\\n\")\nfor i, (name, acc) in enumerate(results_sorted, 1):\n    print(f\"{i}. {name}: Test Accuracy = {acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:55:06.866115Z","iopub.execute_input":"2025-10-06T08:55:06.866337Z","iopub.status.idle":"2025-10-06T08:55:41.491328Z","shell.execute_reply.started":"2025-10-06T08:55:06.866312Z","shell.execute_reply":"2025-10-06T08:55:41.490318Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1. Robust+SVMSMOTE: Test Accuracy = 0.9600\n2. Robust+RandomOverSampler: Test Accuracy = 0.9600\n3. PowerTransformer: Test Accuracy = 0.9600\n4. PowerTransformer+SMOTE: Test Accuracy = 0.9600\n5. PowerTransformer+SMOTE+Tomek: Test Accuracy = 0.9600","metadata":{}},{"cell_type":"markdown","source":"## adaboost","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn import metrics\nimport pandas as pd\n\nfrom imblearn.over_sampling import SVMSMOTE, BorderlineSMOTE, RandomOverSampler, SMOTE\nfrom imblearn.combine import SMOTETomek, SMOTEENN\nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler\n\nML_Model = []\nML_Config = []\naccuracy = []\nf1_score = []\nrecall = []\nprecision = []\nauc_roc = []  # Adding a holder for AUC-ROC\n\n\n\n\n# \n# Function to call for storing the results\ndef storeResults(model, config, a, b, c, d, e):\n    \"\"\"\n    Store model performance results\n\n    Parameters:\n    model: Name of the ML model\n    config: Configuration name (preprocessing steps applied)\n    a: Accuracy score\n    b: F1 score\n    c: Recall score\n    d: Precision score\n    e: AUC-ROC score\n    \"\"\"\n    ML_Model.append(model)\n    ML_Config.append(config)\n    accuracy.append(round(a, 6))\n    f1_score.append(round(b, 6))\n    recall.append(round(c, 6))\n    precision.append(round(d, 6))\n    auc_roc.append(round(e, 6))\n# ============================================\n# Updated Configuration List\n# ============================================\nconfigurations = [\n    ('Original Data', X_train, X_test, y_train),\n    \n    # MinMaxScaler configurations\n    ('Normalized Data', X_train_normalized, X_test_normalized, y_train),\n    ('Normalized+SVMSMOTE', X_train_normalized_svmsmote, X_test_normalized_svmsmote, y_train_normalized_svmsmote),\n    ('Normalized+BorderlineSMOTE', X_train_normalized_bordersmote, X_test_normalized_bordersmote, y_train_normalized_bordersmote),\n    ('Normalized+RandomOverSampler', X_train_normalized_ros, X_test_normalized_ros, y_train_normalized_ros),\n    ('Normalized+SMOTE', X_train_normalized_smote, X_test_normalized_smote, y_train_normalized_smote),\n    ('Normalized+SMOTE+Tomek', X_train_normalized_smotetomek, X_test_normalized_smotetomek, y_train_normalized_smotetomek),\n    ('Normalized+SMOTE+ENN', X_train_normalized_smoteenn, X_test_normalized_smoteenn, y_train_normalized_smoteenn),\n    \n    # RobustScaler configurations\n    ('Robust Data', X_train_robust, X_test_robust, y_train),\n    ('Robust+SVMSMOTE', X_train_robust_svmsmote, X_test_robust_svmsmote, y_train_robust_svmsmote),\n    ('Robust+BorderlineSMOTE', X_train_robust_bordersmote, X_test_robust_bordersmote, y_train_robust_bordersmote),\n    ('Robust+RandomOverSampler', X_train_robust_ros, X_test_robust_ros, y_train_robust_ros),\n    ('Robust+SMOTE', X_train_robust_smote, X_test_robust_smote, y_train_robust_smote),\n    ('Robust+SMOTE+Tomek', X_train_robust_smotetomek, X_test_robust_smotetomek, y_train_robust_smotetomek),\n    ('Robust+SMOTE+ENN', X_train_robust_smoteenn, X_test_robust_smoteenn, y_train_robust_smoteenn),\n    \n    # Mutual Information configurations\n    ('MI', X_train_mi, X_test_mi, y_train),\n    ('MI+SVMSMOTE', X_train_mi_svmsmote, X_test_mi_svmsmote, y_train_mi_svmsmote),\n    ('MI+BorderlineSMOTE', X_train_mi_bordersmote, X_test_mi_bordersmote, y_train_mi_bordersmote),\n    ('MI+RandomOverSampler', X_train_mi_ros, X_test_mi_ros, y_train_mi_ros),\n    ('MI+SMOTE', X_train_mi_smote, X_test_mi_smote, y_train_mi_smote),\n    ('MI+SMOTE+Tomek', X_train_mi_smotetomek, X_test_mi_smotetomek, y_train_mi_smotetomek),\n    ('MI+SMOTE+ENN', X_train_mi_smoteenn, X_test_mi_smoteenn, y_train_mi_smoteenn),\n    \n    # Boruta configurations\n    ('Boruta', X_train_boruta, X_test_boruta, y_train),\n    ('Boruta+SVMSMOTE', X_train_boruta_svmsmote, X_test_boruta_svmsmote, y_train_boruta_svmsmote),\n    ('Boruta+BorderlineSMOTE', X_train_boruta_bordersmote, X_test_boruta_bordersmote, y_train_boruta_bordersmote),\n    ('Boruta+RandomOverSampler', X_train_boruta_ros, X_test_boruta_ros, y_train_boruta_ros),\n    ('Boruta+SMOTE', X_train_boruta_smote, X_test_boruta_smote, y_train_boruta_smote),\n    ('Boruta+SMOTE+Tomek', X_train_boruta_smotetomek, X_test_boruta_smotetomek, y_train_boruta_smotetomek),\n    ('Boruta+SMOTE+ENN', X_train_boruta_smoteenn, X_test_boruta_smoteenn, y_train_boruta_smoteenn),\n    \n    # LDA configurations\n    ('LDA', X_train_lda, X_test_lda, y_train),\n    ('LDA+SVMSMOTE', X_train_lda_svmsmote, X_test_lda_svmsmote, y_train_svmsmote),\n    ('LDA+BorderlineSMOTE', X_train_lda_bordersmote, X_test_lda_bordersmote, y_train_bordersmote),\n    ('LDA+RandomOverSampler', X_train_lda_ros, X_test_lda_ros, y_train_ros),\n    ('LDA+SMOTE', X_train_lda_smote, X_test_lda_smote, y_train_smote),\n    ('LDA+SMOTE+Tomek', X_train_lda_smotetomek, X_test_lda_smotetomek, y_train_smotetomek),\n    ('LDA+SMOTE+ENN', X_train_lda_smoteenn, X_test_lda_smoteenn, y_train_smoteenn),\n    \n    # Autoencoder configurations\n    ('Autoencoder', X_train_encoded, X_test_encoded, y_train),\n    ('Autoencoder+SVMSMOTE', X_train_encoded_svmsmote, X_test_encoded_svmsmote, y_train_svmsmote),\n    ('Autoencoder+BorderlineSMOTE', X_train_encoded_bordersmote, X_test_encoded_bordersmote, y_train_bordersmote),\n    ('Autoencoder+RandomOverSampler', X_train_encoded_ros, X_test_encoded_ros, y_train_ros),\n    ('Autoencoder+SMOTE', X_train_encoded_smote, X_test_encoded_smote, y_train_smote),\n    ('Autoencoder+SMOTE+Tomek', X_train_encoded_smotetomek, X_test_encoded_smotetomek, y_train_smotetomek),\n    ('Autoencoder+SMOTE+ENN', X_train_encoded_smoteenn, X_test_encoded_smoteenn, y_train_smoteenn),\n\n    # PowerTransformer configurations\n    ('PowerTransformer', X_train_power, X_test_power, y_train),\n    ('PowerTransformer+SMOTE', X_train_power_smote, X_test_power_smote, y_train_power_smote),\n    ('PowerTransformer+SMOTE+Tomek', X_train_power_smotetomek, X_test_power_smotetomek, y_train_power_smotetomek),\n    ('PowerTransformer+SMOTE+ENN', X_train_power_smoteenn, X_test_power_smoteenn, y_train_power_smoteenn),\n\n    # PolynomialFeatures configurations\n    ('PolynomialFeatures', X_train_poly, X_test_poly, y_train),\n    ('PolynomialFeatures+SMOTE', X_train_poly_smote, X_test_poly_smote, y_train_poly_smote),\n    ('PolynomialFeatures+SMOTE+Tomek', X_train_poly_smotetomek, X_test_poly_smotetomek, y_train_poly_smotetomek),\n    ('PolynomialFeatures+SMOTE+ENN', X_train_poly_smoteenn, X_test_poly_smoteenn, y_train_poly_smoteenn),\n\n    # ============================================\n    # AdaBoost configurations\n    # ============================================\n    ('AdaBoost Normalized+SMOTE Balanced', X_train_lr, X_test_normalized, y_train_lr),\n    ('AdaBoost Normalized+SMOTE Balanced+Calibrated', X_train_lr, X_test_normalized, y_train_lr)\n]\n\n\n# Print all configurations\nprint(f\"Total configurations: {len(configurations)}\")\nfor i, (name, X_tr, X_te, y_tr) in enumerate(configurations):\n    print(f\"{i+1}. {name}: Train shape={X_tr.shape}, Test shape={X_te.shape}, Train labels={y_tr.shape}\")\n\n# Evaluate AdaBoost for each configuration\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n   \n    # Train AdaBoost\n    adb = AdaBoostClassifier(random_state=42)\n    adb.fit(X_train_cfg, y_train_cfg)\n\n    # Predictions\n    y_train_pred = adb.predict(X_train_cfg)\n    y_test_pred = adb.predict(X_test_cfg)\n    y_train_proba = adb.predict_proba(X_train_cfg)\n    y_test_proba = adb.predict_proba(X_test_cfg)\n\n    # Compute metrics\n    metrics_dict = {\n        \"Dataset\": [\"Training\", \"Test\"],\n        \"Accuracy\": [\n            metrics.accuracy_score(y_train_cfg, y_train_pred),\n            metrics.accuracy_score(y_test, y_test_pred),\n        ],\n        \"F1 Score\": [\n            metrics.f1_score(y_train_cfg, y_train_pred, average='macro'),\n            metrics.f1_score(y_test, y_test_pred, average='macro'),\n        ],\n        \"Recall\": [\n            metrics.recall_score(y_train_cfg, y_train_pred, average='macro'),\n            metrics.recall_score(y_test, y_test_pred, average='macro'),\n        ],\n        \"Precision\": [\n            metrics.precision_score(y_train_cfg, y_train_pred, average='macro'),\n            metrics.precision_score(y_test, y_test_pred, average='macro'),\n        ],\n        \"AUC-ROC\": [\n            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_proba, multi_class='ovr', average='macro'),\n            metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro'),\n        ]\n    }\n\n    df_metrics = pd.DataFrame(metrics_dict)\n\n    # Store results (assuming storeResults function exists)\n    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n    storeResults(\n        'AdaBoost',\n        name,\n        metrics.accuracy_score(y_test, y_test_pred),\n        metrics.f1_score(y_test, y_test_pred, average='macro'),\n        metrics.recall_score(y_test, y_test_pred, average='macro'),\n        metrics.precision_score(y_test, y_test_pred, average='macro'),\n        auc_score\n    )\n\n# Evaluate test accuracy and sort configurations\nresults = []\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n    adb = AdaBoostClassifier(random_state=42)\n    adb.fit(X_train_cfg, y_train_cfg)\n    y_test_pred = adb.predict(X_test_cfg)\n    test_accuracy = metrics.accuracy_score(y_test, y_test_pred)\n    results.append((name, test_accuracy))\n\n# Sort results by test accuracy descending\nresults_sorted = sorted(results, key=lambda x: x[1], reverse=True)\n\n# Print results\nprint(\"Configurations sorted by Test Accuracy (High to Low):\\n\")\nfor i, (name, acc) in enumerate(results_sorted, 1):\n    print(f\"{i}. {name}: Test Accuracy = {acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:55:41.493443Z","iopub.execute_input":"2025-10-06T08:55:41.493679Z","iopub.status.idle":"2025-10-06T08:55:55.247139Z","shell.execute_reply.started":"2025-10-06T08:55:41.493659Z","shell.execute_reply":"2025-10-06T08:55:55.246425Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## adaboost calibrated","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn import metrics\nimport pandas as pd\n\nML_Model = []\nML_Config = []\naccuracy = []\nf1_score = []\nrecall = []\nprecision = []\nauc_roc = []\n\n# -----------------------------\n# Function to store results\n# -----------------------------\ndef storeResults(model, config, a, b, c, d, e):\n    ML_Model.append(model)\n    ML_Config.append(config)\n    accuracy.append(round(a, 6))\n    f1_score.append(round(b, 6))\n    recall.append(round(c, 6))\n    precision.append(round(d, 6))\n    auc_roc.append(round(e, 6))\n\n# -----------------------------\n# Print all configurations\n# -----------------------------\nprint(f\"Total configurations: {len(configurations)}\")\nfor i, (name, X_tr, X_te, y_tr) in enumerate(configurations):\n    print(f\"{i+1}. {name}: Train shape={X_tr.shape}, Test shape={X_te.shape}, Train labels={y_tr.shape}\")\n\n# -----------------------------\n# Evaluate AdaBoost for each configuration (with calibration)\n# -----------------------------\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n   \n    # Train AdaBoost\n    adb = AdaBoostClassifier(random_state=42)\n    adb_cal = CalibratedClassifierCV(adb, cv=5, method='sigmoid')\n    adb_cal.fit(X_train_cfg, y_train_cfg)\n\n    # Predictions\n    y_train_pred = adb_cal.predict(X_train_cfg)\n    y_test_pred = adb_cal.predict(X_test_cfg)\n    y_train_proba = adb_cal.predict_proba(X_train_cfg)\n    y_test_proba = adb_cal.predict_proba(X_test_cfg)\n\n    # Compute AUC-ROC\n    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n\n    # Store results\n    storeResults(\n        'AdaBoost (Calibrated)',\n        name,\n        metrics.accuracy_score(y_test, y_test_pred),\n        metrics.f1_score(y_test, y_test_pred, average='macro'),\n        metrics.recall_score(y_test, y_test_pred, average='macro'),\n        metrics.precision_score(y_test, y_test_pred, average='macro'),\n        auc_score\n    )\n\n# -----------------------------\n# Evaluate test accuracy and sort configurations\n# -----------------------------\nresults = []\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n    adb = AdaBoostClassifier(random_state=42)\n    adb.fit(X_train_cfg, y_train_cfg)\n    y_test_pred = adb.predict(X_test_cfg)\n    test_accuracy = metrics.accuracy_score(y_test, y_test_pred)\n    results.append((name, test_accuracy))\n\n# Sort results by test accuracy descending\nresults_sorted = sorted(results, key=lambda x: x[1], reverse=True)\n\n# Print sorted results\nprint(\"Configurations sorted by Test Accuracy (High to Low):\\n\")\nfor i, (name, acc) in enumerate(results_sorted, 1):\n    print(f\"{i}. {name}: Test Accuracy = {acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:55:55.247998Z","iopub.execute_input":"2025-10-06T08:55:55.248214Z","iopub.status.idle":"2025-10-06T08:56:47.357988Z","shell.execute_reply.started":"2025-10-06T08:55:55.248196Z","shell.execute_reply":"2025-10-06T08:56:47.357095Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1. PolynomialFeatures+SMOTE+ENN: Test Accuracy = 0.9467\n2. AdaBoost Normalized+SMOTE Balanced: Test Accuracy = 0.9467\n3. AdaBoost Normalized+SMOTE Balanced+Calibrated: Test Accuracy = 0.9467\n4. LDA: Test Accuracy = 0.9333\n5. LDA+SVMSMOTE: Test Accuracy = 0.9333","metadata":{}},{"cell_type":"markdown","source":"## adaboost calibrated 2","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn import metrics\nimport pandas as pd\n\nML_Model = []\nML_Config = []\naccuracy = []\nf1_score = []\nrecall = []\nprecision = []\nauc_roc = []\n\n# -----------------------------\n# Function to store results\n# -----------------------------\ndef storeResults(model, config, a, b, c, d, e):\n    ML_Model.append(model)\n    ML_Config.append(config)\n    accuracy.append(round(a, 6))\n    f1_score.append(round(b, 6))\n    recall.append(round(c, 6))\n    precision.append(round(d, 6))\n    auc_roc.append(round(e, 6))\n\n# -----------------------------\n# Print all configurations\n# -----------------------------\nprint(f\"Total configurations: {len(configurations)}\")\nfor i, (name, X_tr, X_te, y_tr) in enumerate(configurations):\n    print(f\"{i+1}. {name}: Train shape={X_tr.shape}, Test shape={X_te.shape}, Train labels={y_tr.shape}\")\n\n# -----------------------------\n# Evaluate AdaBoost for each configuration (with calibration)\n# -----------------------------\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n   \n    # Train AdaBoost\n    adb = AdaBoostClassifier(algorithm='SAMME', n_estimators=200, learning_rate=0.5, random_state=42)\n    adb_cal = CalibratedClassifierCV(adb, cv=5, method='sigmoid')\n    adb_cal.fit(X_train_cfg, y_train_cfg)\n\n    # Predictions\n    y_train_pred = adb_cal.predict(X_train_cfg)\n    y_test_pred = adb_cal.predict(X_test_cfg)\n    y_train_proba = adb_cal.predict_proba(X_train_cfg)\n    y_test_proba = adb_cal.predict_proba(X_test_cfg)\n\n    # Compute AUC-ROC\n    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n\n    # Store results\n    storeResults(\n        'AdaBoost (Calibrated)',\n        name,\n        metrics.accuracy_score(y_test, y_test_pred),\n        metrics.f1_score(y_test, y_test_pred, average='macro'),\n        metrics.recall_score(y_test, y_test_pred, average='macro'),\n        metrics.precision_score(y_test, y_test_pred, average='macro'),\n        auc_score\n    )\n\n# -----------------------------\n# Evaluate test accuracy and sort configurations\n# -----------------------------\nresults = []\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n    adb = AdaBoostClassifier(random_state=42)\n    adb.fit(X_train_cfg, y_train_cfg)\n    y_test_pred = adb.predict(X_test_cfg)\n    test_accuracy = metrics.accuracy_score(y_test, y_test_pred)\n    results.append((name, test_accuracy))\n\n# Sort results by test accuracy descending\nresults_sorted = sorted(results, key=lambda x: x[1], reverse=True)\n\n# Print sorted results\nprint(\"Configurations sorted by Test Accuracy (High to Low):\\n\")\nfor i, (name, acc) in enumerate(results_sorted, 1):\n    print(f\"{i}. {name}: Test Accuracy = {acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:56:47.358986Z","iopub.execute_input":"2025-10-06T08:56:47.359293Z","iopub.status.idle":"2025-10-06T08:56:59.357424Z","shell.execute_reply.started":"2025-10-06T08:56:47.359259Z","shell.execute_reply":"2025-10-06T08:56:59.355366Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1. PolynomialFeatures+SMOTE+ENN: Test Accuracy = 0.9467\n2. AdaBoost Normalized+SMOTE Balanced: Test Accuracy = 0.9467\n3. AdaBoost Normalized+SMOTE Balanced+Calibrated: Test Accuracy = 0.9467\n4. LDA: Test Accuracy = 0.9333\n5. LDA+SVMSMOTE: Test Accuracy = 0.9333","metadata":{}},{"cell_type":"markdown","source":"## Gradient boosting","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn import metrics\nimport pandas as pd\n\nML_Model = []\nML_Config = []\naccuracy = []\nf1_score = []\nrecall = []\nprecision = []\nauc_roc = []\n\n# Function to store results\ndef storeResults(model, config, a, b, c, d, e):\n    ML_Model.append(model)\n    ML_Config.append(config)\n    accuracy.append(round(a, 6))\n    f1_score.append(round(b, 6))\n    recall.append(round(c, 6))\n    precision.append(round(d, 6))\n    auc_roc.append(round(e, 6))\n\n# ============================================\n# Gradient Boosting configurations\n# ============================================\n# Use the same configurations list you already have\nprint(f\"Total configurations: {len(configurations)}\")\nfor i, (name, X_tr, X_te, y_tr) in enumerate(configurations):\n    print(f\"{i+1}. {name}: Train shape={X_tr.shape}, Test shape={X_te.shape}, Train labels={y_tr.shape}\")\n\n# Evaluate Gradient Boosting for each configuration\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n   \n    # Train Gradient Boosting\n    gbc = GradientBoostingClassifier(random_state=42)\n    gbc.fit(X_train_cfg, y_train_cfg)\n\n    # Predictions\n    y_train_pred = gbc.predict(X_train_cfg)\n    y_test_pred = gbc.predict(X_test_cfg)\n    y_train_proba = gbc.predict_proba(X_train_cfg)\n    y_test_proba = gbc.predict_proba(X_test_cfg)\n\n    # Store results\n    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n    storeResults(\n        'GradientBoosting',\n        name,\n        metrics.accuracy_score(y_test, y_test_pred),\n        metrics.f1_score(y_test, y_test_pred, average='macro'),\n        metrics.recall_score(y_test, y_test_pred, average='macro'),\n        metrics.precision_score(y_test, y_test_pred, average='macro'),\n        auc_score\n    )\n\n# Evaluate test accuracy and sort configurations\nresults = []\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n    gbc = GradientBoostingClassifier(random_state=42)\n    gbc.fit(X_train_cfg, y_train_cfg)\n    y_test_pred = gbc.predict(X_test_cfg)\n    test_accuracy = metrics.accuracy_score(y_test, y_test_pred)\n    results.append((name, test_accuracy))\n\n# Sort results by test accuracy descending\nresults_sorted = sorted(results, key=lambda x: x[1], reverse=True)\n\n# Print results\nprint(\"Configurations sorted by Test Accuracy (High to Low):\\n\")\nfor i, (name, acc) in enumerate(results_sorted, 1):\n    print(f\"{i}. {name}: Test Accuracy = {acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:56:59.358030Z","iopub.status.idle":"2025-10-06T08:56:59.358381Z","shell.execute_reply.started":"2025-10-06T08:56:59.358218Z","shell.execute_reply":"2025-10-06T08:56:59.358234Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1. Boruta+SMOTE+Tomek: Test Accuracy = 0.9733\n2. PowerTransformer+SMOTE: Test Accuracy = 0.9733\n3. Original Data: Test Accuracy = 0.9600\n4. Normalized Data: Test Accuracy = 0.9600\n5. Normalized+SMOTE: Test Accuracy = 0.9600\n6. Normalized+SMOTE+Tomek: Test Accuracy = 0.9600\n7. Robust Data: Test Accuracy = 0.9600\n8. Robust+SMOTE+Tomek: Test Accuracy = 0.9600\n9. MI: Test Accuracy = 0.9600\n10. MI+SMOTE: Test Accuracy = 0.9600\n11. MI+SMOTE+Tomek: Test Accuracy = 0.9600\n12. Boruta: Test Accuracy = 0.9600\n13. Autoencoder+SMOTE+ENN: Test Accuracy = 0.9600\n14. PowerTransformer: Test Accuracy = 0.9600","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## calibrated gradient boosting calibrated","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn import metrics\nimport pandas as pd\n\nML_Model = []\nML_Config = []\naccuracy = []\nf1_score = []\nrecall = []\nprecision = []\nauc_roc = []\n\n# -----------------------------\n# Function to store results\n# -----------------------------\ndef storeResults(model, config, a, b, c, d, e):\n    ML_Model.append(model)\n    ML_Config.append(config)\n    accuracy.append(round(a, 6))\n    f1_score.append(round(b, 6))\n    recall.append(round(c, 6))\n    precision.append(round(d, 6))\n    auc_roc.append(round(e, 6))\n\n# -----------------------------\n# Print all configurations\n# -----------------------------\nprint(f\"Total configurations: {len(configurations)}\")\nfor i, (name, X_tr, X_te, y_tr) in enumerate(configurations):\n    print(f\"{i+1}. {name}: Train shape={X_tr.shape}, Test shape={X_te.shape}, Train labels={y_tr.shape}\")\n\n# -----------------------------\n# Evaluate Gradient Boosting\n# -----------------------------\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n   \n    # Train Gradient Boosting\n    gbc = GradientBoostingClassifier(random_state=42)\n    gbc.fit(X_train_cfg, y_train_cfg)\n\n    # Calibrate probabilities\n    gbc_cal = CalibratedClassifierCV(gbc, cv='prefit', method='sigmoid')\n    gbc_cal.fit(X_train_cfg, y_train_cfg)\n\n    # Predictions\n    y_train_pred = gbc_cal.predict(X_train_cfg)\n    y_test_pred = gbc_cal.predict(X_test_cfg)\n    y_train_proba = gbc_cal.predict_proba(X_train_cfg)\n    y_test_proba = gbc_cal.predict_proba(X_test_cfg)\n\n    # Compute AUC-ROC\n    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n\n    # Store results\n    storeResults(\n        'GradientBoosting',\n        name,\n        metrics.accuracy_score(y_test, y_test_pred),\n        metrics.f1_score(y_test, y_test_pred, average='macro'),\n        metrics.recall_score(y_test, y_test_pred, average='macro'),\n        metrics.precision_score(y_test, y_test_pred, average='macro'),\n        auc_score\n    )\n\n# -----------------------------\n# Evaluate test accuracy and sort configurations\n# -----------------------------\nresults = []\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n    gbc = GradientBoostingClassifier(random_state=42)\n    gbc.fit(X_train_cfg, y_train_cfg)\n    y_test_pred = gbc.predict(X_test_cfg)\n    test_accuracy = metrics.accuracy_score(y_test, y_test_pred)\n    results.append((name, test_accuracy))\n\n# Sort results by test accuracy descending\nresults_sorted = sorted(results, key=lambda x: x[1], reverse=True)\n\n# Print sorted results\nprint(\"Configurations sorted by Test Accuracy (High to Low):\\n\")\nfor i, (name, acc) in enumerate(results_sorted, 1):\n    print(f\"{i}. {name}: Test Accuracy = {acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:56:59.359749Z","iopub.status.idle":"2025-10-06T08:56:59.360094Z","shell.execute_reply.started":"2025-10-06T08:56:59.359933Z","shell.execute_reply":"2025-10-06T08:56:59.359950Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1. Boruta+SMOTE+Tomek: Test Accuracy = 0.9733\n2. PowerTransformer+SMOTE: Test Accuracy = 0.9733\n3. Original Data: Test Accuracy = 0.9600\n4. Normalized Data: Test Accuracy = 0.9600\n5. Normalized+SMOTE: Test Accuracy = 0.9600\n6. Normalized+SMOTE+Tomek: Test Accuracy = 0.9600\n7. Robust Data: Test Accuracy = 0.9600\n8. Robust+SMOTE+Tomek: Test Accuracy = 0.9600\n9. MI: Test Accuracy = 0.9600\n10. MI+SMOTE: Test Accuracy = 0.9600","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Catboost calibrated","metadata":{}},{"cell_type":"code","source":"from catboost import CatBoostClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn import metrics\nimport pandas as pd\n\nML_Model = []\nML_Config = []\naccuracy = []\nf1_score = []\nrecall = []\nprecision = []\nauc_roc = []\n\n# -----------------------------\n# Function to store results\n# -----------------------------\ndef storeResults(model, config, a, b, c, d, e):\n    ML_Model.append(model)\n    ML_Config.append(config)\n    accuracy.append(round(a, 6))\n    f1_score.append(round(b, 6))\n    recall.append(round(c, 6))\n    precision.append(round(d, 6))\n    auc_roc.append(round(e, 6))\n\n# -----------------------------\n# Print all configurations\n# -----------------------------\nprint(f\"Total configurations: {len(configurations)}\")\nfor i, (name, X_tr, X_te, y_tr) in enumerate(configurations):\n    print(f\"{i+1}. {name}: Train shape={X_tr.shape}, Test shape={X_te.shape}, Train labels={y_tr.shape}\")\n\n# -----------------------------\n# Evaluate CatBoost for each configuration\n# -----------------------------\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n   \n    # Train CatBoost\n    cbc = CatBoostClassifier(verbose=0, random_state=42)\n    cbc.fit(X_train_cfg, y_train_cfg)\n\n    # Calibrate probabilities\n    cbc_cal = CalibratedClassifierCV(cbc, cv='prefit', method='sigmoid')\n    cbc_cal.fit(X_train_cfg, y_train_cfg)\n\n    # Predictions\n    y_train_pred = cbc_cal.predict(X_train_cfg)\n    y_test_pred = cbc_cal.predict(X_test_cfg)\n    y_train_proba = cbc_cal.predict_proba(X_train_cfg)\n    y_test_proba = cbc_cal.predict_proba(X_test_cfg)\n\n    # Compute AUC-ROC\n    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n\n    # Store results\n    storeResults(\n        'CatBoost',\n        name,\n        metrics.accuracy_score(y_test, y_test_pred),\n        metrics.f1_score(y_test, y_test_pred, average='macro'),\n        metrics.recall_score(y_test, y_test_pred, average='macro'),\n        metrics.precision_score(y_test, y_test_pred, average='macro'),\n        auc_score\n    )\n\n# -----------------------------\n# Evaluate test accuracy and sort configurations\n# -----------------------------\nresults = []\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n    cbc = CatBoostClassifier(verbose=0, random_state=42)\n    cbc.fit(X_train_cfg, y_train_cfg)\n    y_test_pred = cbc.predict(X_test_cfg)\n    test_accuracy = metrics.accuracy_score(y_test, y_test_pred)\n    results.append((name, test_accuracy))\n\n# Sort results by test accuracy descending\nresults_sorted = sorted(results, key=lambda x: x[1], reverse=True)\n\n# Print sorted results\nprint(\"Configurations sorted by Test Accuracy (High to Low):\\n\")\nfor i, (name, acc) in enumerate(results_sorted, 1):\n    print(f\"{i}. {name}: Test Accuracy = {acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:56:59.361184Z","iopub.status.idle":"2025-10-06T08:56:59.361465Z","shell.execute_reply.started":"2025-10-06T08:56:59.361343Z","shell.execute_reply":"2025-10-06T08:56:59.361356Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Catboost default\n","metadata":{}},{"cell_type":"code","source":"from catboost import CatBoostClassifier\nfrom sklearn import metrics\nimport pandas as pd\n\nML_Model = []\nML_Config = []\naccuracy = []\nf1_score = []\nrecall = []\nprecision = []\nauc_roc = []\n\n# -----------------------------\n# Function to store results\n# -----------------------------\ndef storeResults(model, config, a, b, c, d, e):\n    ML_Model.append(model)\n    ML_Config.append(config)\n    accuracy.append(round(a, 6))\n    f1_score.append(round(b, 6))\n    recall.append(round(c, 6))\n    precision.append(round(d, 6))\n    auc_roc.append(round(e, 6))\n\n# -----------------------------\n# Print all configurations\n# -----------------------------\nprint(f\"Total configurations: {len(configurations)}\")\nfor i, (name, X_tr, X_te, y_tr) in enumerate(configurations):\n    print(f\"{i+1}. {name}: Train shape={X_tr.shape}, Test shape={X_te.shape}, Train labels={y_tr.shape}\")\n\n# -----------------------------\n# Evaluate CatBoost for each configuration\n# -----------------------------\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n   \n    # Train CatBoost with default parameters\n    cbc = CatBoostClassifier(verbose=0, random_state=42)\n    cbc.fit(X_train_cfg, y_train_cfg)\n    \n    # Predictions\n    y_train_pred = cbc.predict(X_train_cfg)\n    y_test_pred = cbc.predict(X_test_cfg)\n    y_train_proba = cbc.predict_proba(X_train_cfg)\n    y_test_proba = cbc.predict_proba(X_test_cfg)\n    \n    # Compute AUC-ROC\n    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n    \n    # Store results\n    storeResults(\n        'CatBoost',\n        name,\n        metrics.accuracy_score(y_test, y_test_pred),\n        metrics.f1_score(y_test, y_test_pred, average='macro'),\n        metrics.recall_score(y_test, y_test_pred, average='macro'),\n        metrics.precision_score(y_test, y_test_pred, average='macro'),\n        auc_score\n    )\n\n# -----------------------------\n# Evaluate test accuracy and sort configurations\n# -----------------------------\nresults = []\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n    cbc = CatBoostClassifier(verbose=0, random_state=42)\n    cbc.fit(X_train_cfg, y_train_cfg)\n    y_test_pred = cbc.predict(X_test_cfg)\n    test_accuracy = metrics.accuracy_score(y_test, y_test_pred)\n    results.append((name, test_accuracy))\n\n# Sort results by test accuracy descending\nresults_sorted = sorted(results, key=lambda x: x[1], reverse=True)\n\n# Print sorted results\nprint(\"\\nConfigurations sorted by Test Accuracy (High to Low):\\n\")\nfor i, (name, acc) in enumerate(results_sorted, 1):\n    print(f\"{i}. {name}: Test Accuracy = {acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:56:59.362418Z","iopub.status.idle":"2025-10-06T08:56:59.362645Z","shell.execute_reply.started":"2025-10-06T08:56:59.362538Z","shell.execute_reply":"2025-10-06T08:56:59.362548Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Catboost 1","metadata":{}},{"cell_type":"code","source":"from catboost import CatBoostClassifier\nfrom sklearn import metrics\nimport pandas as pd\n\nML_Model = []\nML_Config = []\naccuracy = []\nf1_score = []\nrecall = []\nprecision = []\nauc_roc = []\n\n# -----------------------------\n# Function to store results\n# -----------------------------\ndef storeResults(model, config, a, b, c, d, e):\n    ML_Model.append(model)\n    ML_Config.append(config)\n    accuracy.append(round(a, 6))\n    f1_score.append(round(b, 6))\n    recall.append(round(c, 6))\n    precision.append(round(d, 6))\n    auc_roc.append(round(e, 6))\n\n# -----------------------------\n# Print all configurations\n# -----------------------------\nprint(f\"Total configurations: {len(configurations)}\")\nfor i, (name, X_tr, X_te, y_tr) in enumerate(configurations):\n    print(f\"{i+1}. {name}: Train shape={X_tr.shape}, Test shape={X_te.shape}, Train labels={y_tr.shape}\")\n\n# -----------------------------\n# Evaluate CatBoost for each configuration\n# -----------------------------\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n   \n    # Train CatBoost with default parameters\n    cbc = CatBoostClassifier(\n    iterations=100,\n    depth=8,\n    learning_rate=0.05,\n    l2_leaf_reg=5,\n    bagging_temperature=0.8,\n    border_count=128,\n    eval_metric='Accuracy',\n    random_seed=42,\n    verbose=200\n)\n    cbc.fit(X_train_cfg, y_train_cfg)\n    \n    # Predictions\n    y_train_pred = cbc.predict(X_train_cfg)\n    y_test_pred = cbc.predict(X_test_cfg)\n    y_train_proba = cbc.predict_proba(X_train_cfg)\n    y_test_proba = cbc.predict_proba(X_test_cfg)\n    \n    # Compute AUC-ROC\n    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n    \n    # Store results\n    storeResults(\n        'CatBoost',\n        name,\n        metrics.accuracy_score(y_test, y_test_pred),\n        metrics.f1_score(y_test, y_test_pred, average='macro'),\n        metrics.recall_score(y_test, y_test_pred, average='macro'),\n        metrics.precision_score(y_test, y_test_pred, average='macro'),\n        auc_score\n    )\n\n# -----------------------------\n# Evaluate test accuracy and sort configurations\n# -----------------------------\nresults = []\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n    cbc = CatBoostClassifier(verbose=0, random_state=42)\n    cbc.fit(X_train_cfg, y_train_cfg)\n    y_test_pred = cbc.predict(X_test_cfg)\n    test_accuracy = metrics.accuracy_score(y_test, y_test_pred)\n    results.append((name, test_accuracy))\n\n# Sort results by test accuracy descending\nresults_sorted = sorted(results, key=lambda x: x[1], reverse=True)\n\n# Print sorted results\nprint(\"\\nConfigurations sorted by Test Accuracy (High to Low):\\n\")\nfor i, (name, acc) in enumerate(results_sorted, 1):\n    print(f\"{i}. {name}: Test Accuracy = {acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:56:59.363954Z","iopub.status.idle":"2025-10-06T08:56:59.364247Z","shell.execute_reply.started":"2025-10-06T08:56:59.364123Z","shell.execute_reply":"2025-10-06T08:56:59.364138Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Catboost 2","metadata":{}},{"cell_type":"code","source":"from catboost import CatBoostClassifier\nfrom sklearn import metrics\nimport pandas as pd\n\nML_Model = []\nML_Config = []\naccuracy = []\nf1_score = []\nrecall = []\nprecision = []\nauc_roc = []\n\n# -----------------------------\n# Function to store results\n# -----------------------------\ndef storeResults(model, config, a, b, c, d, e):\n    ML_Model.append(model)\n    ML_Config.append(config)\n    accuracy.append(round(a, 6))\n    f1_score.append(round(b, 6))\n    recall.append(round(c, 6))\n    precision.append(round(d, 6))\n    auc_roc.append(round(e, 6))\n\n# -----------------------------\n# Print all configurations\n# -----------------------------\nprint(f\"Total configurations: {len(configurations)}\")\nfor i, (name, X_tr, X_te, y_tr) in enumerate(configurations):\n    print(f\"{i+1}. {name}: Train shape={X_tr.shape}, Test shape={X_te.shape}, Train labels={y_tr.shape}\")\n\n# -----------------------------\n# Evaluate CatBoost for each configuration\n# -----------------------------\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n   \n    # Train CatBoost with default parameters\n    cbc = CatBoostClassifier(\n    iterations=150,\n    depth=10,\n    learning_rate=0.03,\n    l2_leaf_reg=3,\n\n    border_count=254,\n    bootstrap_type='Bernoulli',\n    subsample=0.9,\n    eval_metric='Accuracy',\n    random_seed=42,\n    verbose=200\n)\n\n    cbc.fit(X_train_cfg, y_train_cfg)\n    \n    # Predictions\n    y_train_pred = cbc.predict(X_train_cfg)\n    y_test_pred = cbc.predict(X_test_cfg)\n    y_train_proba = cbc.predict_proba(X_train_cfg)\n    y_test_proba = cbc.predict_proba(X_test_cfg)\n    \n    # Compute AUC-ROC\n    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n    \n    # Store results\n    storeResults(\n        'CatBoost',\n        name,\n        metrics.accuracy_score(y_test, y_test_pred),\n        metrics.f1_score(y_test, y_test_pred, average='macro'),\n        metrics.recall_score(y_test, y_test_pred, average='macro'),\n        metrics.precision_score(y_test, y_test_pred, average='macro'),\n        auc_score\n    )\n\n# -----------------------------\n# Evaluate test accuracy and sort configurations\n# -----------------------------\nresults = []\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n    cbc = CatBoostClassifier(verbose=0, random_state=42)\n    cbc.fit(X_train_cfg, y_train_cfg)\n    y_test_pred = cbc.predict(X_test_cfg)\n    test_accuracy = metrics.accuracy_score(y_test, y_test_pred)\n    results.append((name, test_accuracy))\n\n# Sort results by test accuracy descending\nresults_sorted = sorted(results, key=lambda x: x[1], reverse=True)\n\n# Print sorted results\nprint(\"\\nConfigurations sorted by Test Accuracy (High to Low):\\n\")\nfor i, (name, acc) in enumerate(results_sorted, 1):\n    print(f\"{i}. {name}: Test Accuracy = {acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:56:59.365554Z","iopub.status.idle":"2025-10-06T08:56:59.365797Z","shell.execute_reply.started":"2025-10-06T08:56:59.365680Z","shell.execute_reply":"2025-10-06T08:56:59.365690Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Catbooost 3","metadata":{}},{"cell_type":"code","source":"from catboost import CatBoostClassifier\nfrom sklearn import metrics\nimport pandas as pd\n\nML_Model = []\nML_Config = []\naccuracy = []\nf1_score = []\nrecall = []\nprecision = []\nauc_roc = []\n\n# -----------------------------\n# Function to store results\n# -----------------------------\ndef storeResults(model, config, a, b, c, d, e):\n    ML_Model.append(model)\n    ML_Config.append(config)\n    accuracy.append(round(a, 6))\n    f1_score.append(round(b, 6))\n    recall.append(round(c, 6))\n    precision.append(round(d, 6))\n    auc_roc.append(round(e, 6))\n\n# -----------------------------\n# Print all configurations\n# -----------------------------\nprint(f\"Total configurations: {len(configurations)}\")\nfor i, (name, X_tr, X_te, y_tr) in enumerate(configurations):\n    print(f\"{i+1}. {name}: Train shape={X_tr.shape}, Test shape={X_te.shape}, Train labels={y_tr.shape}\")\n\n# -----------------------------\n# Evaluate CatBoost for each configuration\n# -----------------------------\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n   \n    # Train CatBoost with default parameters\n    cbc = CatBoostClassifier(\n    iterations=120,\n    depth=6,\n    learning_rate=0.07,\n    l2_leaf_reg=8,\n    bagging_temperature=1.0,\n    border_count=64,\n    random_strength=1.5,\n    eval_metric='Accuracy',\n    random_seed=42,\n    verbose=200\n)\n\n    cbc.fit(X_train_cfg, y_train_cfg)\n    \n    # Predictions\n    y_train_pred = cbc.predict(X_train_cfg)\n    y_test_pred = cbc.predict(X_test_cfg)\n    y_train_proba = cbc.predict_proba(X_train_cfg)\n    y_test_proba = cbc.predict_proba(X_test_cfg)\n    \n    # Compute AUC-ROC\n    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n    \n    # Store results\n    storeResults(\n        'CatBoost',\n        name,\n        metrics.accuracy_score(y_test, y_test_pred),\n        metrics.f1_score(y_test, y_test_pred, average='macro'),\n        metrics.recall_score(y_test, y_test_pred, average='macro'),\n        metrics.precision_score(y_test, y_test_pred, average='macro'),\n        auc_score\n    )\n\n# -----------------------------\n# Evaluate test accuracy and sort configurations\n# -----------------------------\nresults = []\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n    cbc = CatBoostClassifier(verbose=0, random_state=42)\n    cbc.fit(X_train_cfg, y_train_cfg)\n    y_test_pred = cbc.predict(X_test_cfg)\n    test_accuracy = metrics.accuracy_score(y_test, y_test_pred)\n    results.append((name, test_accuracy))\n\n# Sort results by test accuracy descending\nresults_sorted = sorted(results, key=lambda x: x[1], reverse=True)\n\n# Print sorted results\n# Print sorted results\nprint(\"\\nConfigurations sorted by Test Accuracy (High to Low):\\n\")\nfor i, (name, acc) in enumerate(results_sorted, 1):\n    print(f\"{i}. {name}: Test Accuracy = {acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:56:59.367186Z","iopub.status.idle":"2025-10-06T08:56:59.367498Z","shell.execute_reply.started":"2025-10-06T08:56:59.367339Z","shell.execute_reply":"2025-10-06T08:56:59.367353Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Catboost 4","metadata":{}},{"cell_type":"code","source":"from catboost import CatBoostClassifier\nfrom sklearn import metrics\nimport pandas as pd\n\nML_Model = []\nML_Config = []\naccuracy = []\nf1_score = []\nrecall = []\nprecision = []\nauc_roc = []\n\n# -----------------------------\n# Function to store results\n# -----------------------------\ndef storeResults(model, config, a, b, c, d, e):\n    ML_Model.append(model)\n    ML_Config.append(config)\n    accuracy.append(round(a, 6))\n    f1_score.append(round(b, 6))\n    recall.append(round(c, 6))\n    precision.append(round(d, 6))\n    auc_roc.append(round(e, 6))\n\n# -----------------------------\n# Print all configurations\n# -----------------------------\nprint(f\"Total configurations: {len(configurations)}\")\nfor i, (name, X_tr, X_te, y_tr) in enumerate(configurations):\n    print(f\"{i+1}. {name}: Train shape={X_tr.shape}, Test shape={X_te.shape}, Train labels={y_tr.shape}\")\n\n# -----------------------------\n# Evaluate CatBoost for each configuration\n# -----------------------------\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n   \n    # Train CatBoost with default parameters\n    cbc = CatBoostClassifier(\n    iterations=100,\n    depth=5,\n    learning_rate=0.1,\n    l2_leaf_reg=4,\n    subsample=0.85,\n    border_count=128,\n    random_strength=0.8,\n    eval_metric='Accuracy',\n    random_seed=42,\n    verbose=200\n)\n\n    cbc.fit(X_train_cfg, y_train_cfg)\n    \n    # Predictions\n    y_train_pred = cbc.predict(X_train_cfg)\n    y_test_pred = cbc.predict(X_test_cfg)\n    y_train_proba = cbc.predict_proba(X_train_cfg)\n    y_test_proba = cbc.predict_proba(X_test_cfg)\n    \n    # Compute AUC-ROC\n    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n    \n    # Store results\n    storeResults(\n        'CatBoost',\n        name,\n        metrics.accuracy_score(y_test, y_test_pred),\n        metrics.f1_score(y_test, y_test_pred, average='macro'),\n        metrics.recall_score(y_test, y_test_pred, average='macro'),\n        metrics.precision_score(y_test, y_test_pred, average='macro'),\n        auc_score\n    )\n\n# -----------------------------\n# Evaluate test accuracy and sort configurations\n# -----------------------------\nresults = []\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n    cbc = CatBoostClassifier(verbose=0, random_state=42)\n    cbc.fit(X_train_cfg, y_train_cfg)\n    y_test_pred = cbc.predict(X_test_cfg)\n    test_accuracy = metrics.accuracy_score(y_test, y_test_pred)\n    results.append((name, test_accuracy))\n\n# Sort results by test accuracy descending\nresults_sorted = sorted(results, key=lambda x: x[1], reverse=True)\n\n# Print sorted results\nprint(\"\\nConfigurations sorted by Test Accuracy (High to Low):\\n\")\nfor i, (name, acc) in enumerate(results_sorted, 1):\n    print(f\"{i}. {name}: Test Accuracy = {acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:56:59.368545Z","iopub.status.idle":"2025-10-06T08:56:59.368892Z","shell.execute_reply.started":"2025-10-06T08:56:59.368700Z","shell.execute_reply":"2025-10-06T08:56:59.368716Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Catboost 5","metadata":{}},{"cell_type":"code","source":"from catboost import CatBoostClassifier\nfrom sklearn import metrics\nimport pandas as pd\n\nML_Model = []\nML_Config = []\naccuracy = []\nf1_score = []\nrecall = []\nprecision = []\nauc_roc = []\n\n# -----------------------------\n# Function to store results\n# -----------------------------\ndef storeResults(model, config, a, b, c, d, e):\n    ML_Model.append(model)\n    ML_Config.append(config)\n    accuracy.append(round(a, 6))\n    f1_score.append(round(b, 6))\n    recall.append(round(c, 6))\n    precision.append(round(d, 6))\n    auc_roc.append(round(e, 6))\n\n# -----------------------------\n# Print all configurations\n# -----------------------------\nprint(f\"Total configurations: {len(configurations)}\")\nfor i, (name, X_tr, X_te, y_tr) in enumerate(configurations):\n    print(f\"{i+1}. {name}: Train shape={X_tr.shape}, Test shape={X_te.shape}, Train labels={y_tr.shape}\")\n\n# -----------------------------\n# Evaluate CatBoost for each configuration\n# -----------------------------\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n   \n    # Train CatBoost with default parameters\n    cbc = CatBoostClassifier(\n    iterations=1500,\n    depth=9,\n    learning_rate=0.04,\n    l2_leaf_reg=6,\n    bagging_temperature=0.4,\n    subsample=0.85,\n    grow_policy='Lossguide',\n    random_strength=1.0,\n    eval_metric='Accuracy',\n    random_seed=42,\n    verbose=200,\n    max_leaves=64\n)\n\n    cbc.fit(X_train_cfg, y_train_cfg)\n    \n    # Predictions\n    y_train_pred = cbc.predict(X_train_cfg)\n    y_test_pred = cbc.predict(X_test_cfg)\n    y_train_proba = cbc.predict_proba(X_train_cfg)\n    y_test_proba = cbc.predict_proba(X_test_cfg)\n    \n    # Compute AUC-ROC\n    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n    \n    # Store results\n    storeResults(\n        'CatBoost',\n        name,\n        metrics.accuracy_score(y_test, y_test_pred),\n        metrics.f1_score(y_test, y_test_pred, average='macro'),\n        metrics.recall_score(y_test, y_test_pred, average='macro'),\n        metrics.precision_score(y_test, y_test_pred, average='macro'),\n        auc_score\n    )\n\n# -----------------------------\n# Evaluate test accuracy and sort configurations\n# -----------------------------\nresults = []\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n    cbc = CatBoostClassifier(verbose=0, random_state=42)\n    cbc.fit(X_train_cfg, y_train_cfg)\n    y_test_pred = cbc.predict(X_test_cfg)\n    test_accuracy = metrics.accuracy_score(y_test, y_test_pred)\n    results.append((name, test_accuracy))\n\n# Sort results by test accuracy descending\nresults_sorted = sorted(results, key=lambda x: x[1], reverse=True)\n\n# Print sorted results\nprint(\"\\nConfigurations sorted by Test Accuracy (High to Low):\\n\")\nfor i, (name, acc) in enumerate(results_sorted, 1):\n    print(f\"{i}. {name}: Test Accuracy = {acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:56:59.369942Z","iopub.status.idle":"2025-10-06T08:56:59.370185Z","shell.execute_reply.started":"2025-10-06T08:56:59.370082Z","shell.execute_reply":"2025-10-06T08:56:59.370092Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Lightgbm default","metadata":{}},{"cell_type":"code","source":"from lightgbm import LGBMClassifier\nfrom sklearn import metrics\nimport pandas as pd\n\nML_Model = []\nML_Config = []\naccuracy = []\nf1_score = []\nrecall = []\nprecision = []\nauc_roc = []\n\n# Function to store results\ndef storeResults(model, config, a, b, c, d, e):\n    ML_Model.append(model)\n    ML_Config.append(config)\n    accuracy.append(round(a, 6))\n    f1_score.append(round(b, 6))\n    recall.append(round(c, 6))\n    precision.append(round(d, 6))\n    auc_roc.append(round(e, 6))\n\n# ============================================\n# LightGBM configurations\n# ============================================\n# Use the same configurations list you already have\nprint(f\"Total configurations: {len(configurations)}\")\nfor i, (name, X_tr, X_te, y_tr) in enumerate(configurations):\n    print(f\"{i+1}. {name}: Train shape={X_tr.shape}, Test shape={X_te.shape}, Train labels={y_tr.shape}\")\n\n# Evaluate LightGBM for each configuration\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n   \n    # Train LightGBM\n    lgbm = LGBMClassifier(random_state=42, verbose=-1,objective='multiclass', boosting_type='gbdt',\n    metric='multi_logloss')\n    lgbm.fit(X_train_cfg, y_train_cfg)\n    \n    # Predictions\n    y_train_pred = lgbm.predict(X_train_cfg)\n    y_test_pred = lgbm.predict(X_test_cfg)\n    y_train_proba = lgbm.predict_proba(X_train_cfg)\n    y_test_proba = lgbm.predict_proba(X_test_cfg)\n    \n    # Store results\n    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n    storeResults(\n        'LightGBM',\n        name,\n        metrics.accuracy_score(y_test, y_test_pred),\n        metrics.f1_score(y_test, y_test_pred, average='macro'),\n        metrics.recall_score(y_test, y_test_pred, average='macro'),\n        metrics.precision_score(y_test, y_test_pred, average='macro'),\n        auc_score\n    )\n\n# Evaluate test accuracy and sort configurations\nresults = []\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n    lgbm = LGBMClassifier(random_state=42, verbose=-1)\n    lgbm.fit(X_train_cfg, y_train_cfg)\n    y_test_pred = lgbm.predict(X_test_cfg)\n    test_accuracy = metrics.accuracy_score(y_test, y_test_pred)\n    results.append((name, test_accuracy))\n\n# Sort results by test accuracy descending\nresults_sorted = sorted(results, key=lambda x: x[1], reverse=True)\n\n# Print results\nprint(\"Configurations sorted by Test Accuracy (High to Low):\\n\")\nfor i, (name, acc) in enumerate(results_sorted, 1):\n    print(f\"{i}. {name}: Test Accuracy = {acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:56:59.371568Z","iopub.status.idle":"2025-10-06T08:56:59.371920Z","shell.execute_reply.started":"2025-10-06T08:56:59.371737Z","shell.execute_reply":"2025-10-06T08:56:59.371751Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## lgbm 1","metadata":{}},{"cell_type":"code","source":"from lightgbm import LGBMClassifier\nfrom sklearn import metrics\nimport pandas as pd\n\nML_Model = []\nML_Config = []\naccuracy = []\nf1_score = []\nrecall = []\nprecision = []\nauc_roc = []\n\n# Function to store results\ndef storeResults(model, config, a, b, c, d, e):\n    ML_Model.append(model)\n    ML_Config.append(config)\n    accuracy.append(round(a, 6))\n    f1_score.append(round(b, 6))\n    recall.append(round(c, 6))\n    precision.append(round(d, 6))\n    auc_roc.append(round(e, 6))\n\n# ============================================\n# LightGBM configurations\n# ============================================\n# Use the same configurations list you already have\nprint(f\"Total configurations: {len(configurations)}\")\nfor i, (name, X_tr, X_te, y_tr) in enumerate(configurations):\n    print(f\"{i+1}. {name}: Train shape={X_tr.shape}, Test shape={X_te.shape}, Train labels={y_tr.shape}\")\n\n# Evaluate LightGBM for each configuration\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n   \n    # Train LightGBM\n    lgbm =  LGBMClassifier(\n    n_estimators=1200,\n    learning_rate=0.07,\n    objective='multiclass',\n    num_leaves=25,\n    max_depth=6,\n    min_child_samples=40,\n    subsample=0.7,\n    colsample_bytree=0.7,\n    reg_alpha=0.5,\n    reg_lambda=1.0,\n    boosting_type='gbdt',\n    metric='multi_logloss',\n    random_state=42\n)\n    lgbm.fit(X_train_cfg, y_train_cfg)\n    \n    # Predictions\n    y_train_pred = lgbm.predict(X_train_cfg)\n    y_test_pred = lgbm.predict(X_test_cfg)\n    y_train_proba = lgbm.predict_proba(X_train_cfg)\n    y_test_proba = lgbm.predict_proba(X_test_cfg)\n    \n    # Store results\n    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n    storeResults(\n        'LightGBM',\n        name,\n        metrics.accuracy_score(y_test, y_test_pred),\n        metrics.f1_score(y_test, y_test_pred, average='macro'),\n        metrics.recall_score(y_test, y_test_pred, average='macro'),\n        metrics.precision_score(y_test, y_test_pred, average='macro'),\n        auc_score\n    )\n\n# Evaluate test accuracy and sort configurations\nresults = []\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n    lgbm = LGBMClassifier(random_state=42, verbose=-1)\n    lgbm.fit(X_train_cfg, y_train_cfg)\n    y_test_pred = lgbm.predict(X_test_cfg)\n    test_accuracy = metrics.accuracy_score(y_test, y_test_pred)\n    results.append((name, test_accuracy))\n\n# Sort results by test accuracy descending\nresults_sorted = sorted(results, key=lambda x: x[1], reverse=True)\n\n# Print results\nprint(\"Configurations sorted by Test Accuracy (High to Low):\\n\")\nfor i, (name, acc) in enumerate(results_sorted, 1):\n    print(f\"{i}. {name}: Test Accuracy = {acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:56:59.373064Z","iopub.status.idle":"2025-10-06T08:56:59.373347Z","shell.execute_reply.started":"2025-10-06T08:56:59.373181Z","shell.execute_reply":"2025-10-06T08:56:59.373193Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## xgboost","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn import metrics\nimport pandas as pd\n\nML_Model = []\nML_Config = []\naccuracy = []\nf1_score = []\nrecall = []\nprecision = []\nauc_roc = []\n\n# Function to store results\ndef storeResults(model, config, a, b, c, d, e):\n    ML_Model.append(model)\n    ML_Config.append(config)\n    accuracy.append(round(a, 6))\n    f1_score.append(round(b, 6))\n    recall.append(round(c, 6))\n    precision.append(round(d, 6))\n    auc_roc.append(round(e, 6))\n\n# ============================================\n# XGBoost configurations\n# ============================================\n# Use the same configurations list you already have\nprint(f\"Total configurations: {len(configurations)}\")\nfor i, (name, X_tr, X_te, y_tr) in enumerate(configurations):\n    print(f\"{i+1}. {name}: Train shape={X_tr.shape}, Test shape={X_te.shape}, Train labels={y_tr.shape}\")\n\n# Evaluate XGBoost for each configuration\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n   \n    # Train XGBoost\n    xgb = XGBClassifier(random_state=42, verbosity=0)\n    xgb.fit(X_train_cfg, y_train_cfg)\n    \n    # Predictions\n    y_train_pred = xgb.predict(X_train_cfg)\n    y_test_pred = xgb.predict(X_test_cfg)\n    y_train_proba = xgb.predict_proba(X_train_cfg)\n    y_test_proba = xgb.predict_proba(X_test_cfg)\n    \n    # Store results\n    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n    storeResults(\n        'XGBoost',\n        name,\n        metrics.accuracy_score(y_test, y_test_pred),\n        metrics.f1_score(y_test, y_test_pred, average='macro'),\n        metrics.recall_score(y_test, y_test_pred, average='macro'),\n        metrics.precision_score(y_test, y_test_pred, average='macro'),\n        auc_score\n    )\n\n# Evaluate test accuracy and sort configurations\nresults = []\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n    xgb = XGBClassifier(random_state=42, verbosity=0)\n    xgb.fit(X_train_cfg, y_train_cfg)\n    y_test_pred = xgb.predict(X_test_cfg)\n    test_accuracy = metrics.accuracy_score(y_test, y_test_pred)\n    results.append((name, test_accuracy))\n\n# Sort results by test accuracy descending\nresults_sorted = sorted(results, key=lambda x: x[1], reverse=True)\n\n# Print results\nprint(\"Configurations sorted by Test Accuracy (High to Low):\\n\")\nfor i, (name, acc) in enumerate(results_sorted, 1):\n    print(f\"{i}. {name}: Test Accuracy = {acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:56:59.374380Z","iopub.status.idle":"2025-10-06T08:56:59.374684Z","shell.execute_reply.started":"2025-10-06T08:56:59.374531Z","shell.execute_reply":"2025-10-06T08:56:59.374548Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Extra trees","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn import metrics\nimport pandas as pd\n\nML_Model = []\nML_Config = []\naccuracy = []\nf1_score = []\nrecall = []\nprecision = []\nauc_roc = []\n\n# Function to store results\ndef storeResults(model, config, a, b, c, d, e):\n    ML_Model.append(model)\n    ML_Config.append(config)\n    accuracy.append(round(a, 6))\n    f1_score.append(round(b, 6))\n    recall.append(round(c, 6))\n    precision.append(round(d, 6))\n    auc_roc.append(round(e, 6))\n\n# ============================================\n# Extra Trees configurations\n# ============================================\n# Use the same configurations list you already have\nprint(f\"Total configurations: {len(configurations)}\")\nfor i, (name, X_tr, X_te, y_tr) in enumerate(configurations):\n    print(f\"{i+1}. {name}: Train shape={X_tr.shape}, Test shape={X_te.shape}, Train labels={y_tr.shape}\")\n\n# Evaluate Extra Trees for each configuration\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n   \n    # Train Extra Trees\n    etc = ExtraTreesClassifier(random_state=42)\n    etc.fit(X_train_cfg, y_train_cfg)\n    \n    # Predictions\n    y_train_pred = etc.predict(X_train_cfg)\n    y_test_pred = etc.predict(X_test_cfg)\n    y_train_proba = etc.predict_proba(X_train_cfg)\n    y_test_proba = etc.predict_proba(X_test_cfg)\n    \n    # Store results\n    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n    storeResults(\n        'ExtraTrees',\n        name,\n        metrics.accuracy_score(y_test, y_test_pred),\n        metrics.f1_score(y_test, y_test_pred, average='macro'),\n        metrics.recall_score(y_test, y_test_pred, average='macro'),\n        metrics.precision_score(y_test, y_test_pred, average='macro'),\n        auc_score\n    )\n\n# Evaluate test accuracy and sort configurations\nresults = []\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n    etc = ExtraTreesClassifier(random_state=42)\n    etc.fit(X_train_cfg, y_train_cfg)\n    y_test_pred = etc.predict(X_test_cfg)\n    test_accuracy = metrics.accuracy_score(y_test, y_test_pred)\n    results.append((name, test_accuracy))\n\n# Sort results by test accuracy descending\nresults_sorted = sorted(results, key=lambda x: x[1], reverse=True)\n\n# Print results\nprint(\"Configurations sorted by Test Accuracy (High to Low):\\n\")\nfor i, (name, acc) in enumerate(results_sorted, 1):\n    print(f\"{i}. {name}: Test Accuracy = {acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:56:59.375545Z","iopub.status.idle":"2025-10-06T08:56:59.375820Z","shell.execute_reply.started":"2025-10-06T08:56:59.375668Z","shell.execute_reply":"2025-10-06T08:56:59.375683Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## KNN","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\nimport pandas as pd\n\nML_Model = []\nML_Config = []\naccuracy = []\nf1_score = []\nrecall = []\nprecision = []\nauc_roc = []\n\n# Function to store results\ndef storeResults(model, config, a, b, c, d, e):\n    ML_Model.append(model)\n    ML_Config.append(config)\n    accuracy.append(round(a, 6))\n    f1_score.append(round(b, 6))\n    recall.append(round(c, 6))\n    precision.append(round(d, 6))\n    auc_roc.append(round(e, 6))\n\n# ============================================\n# KNN configurations\n# ============================================\n# Use the same configurations list you already have\nprint(f\"Total configurations: {len(configurations)}\")\nfor i, (name, X_tr, X_te, y_tr) in enumerate(configurations):\n    print(f\"{i+1}. {name}: Train shape={X_tr.shape}, Test shape={X_te.shape}, Train labels={y_tr.shape}\")\n\n# Evaluate KNN for each configuration\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n   \n    # Train KNN\n    knn = KNeighborsClassifier()\n    knn.fit(X_train_cfg, y_train_cfg)\n    \n    # Predictions\n    y_train_pred = knn.predict(X_train_cfg)\n    y_test_pred = knn.predict(X_test_cfg)\n    y_train_proba = knn.predict_proba(X_train_cfg)\n    y_test_proba = knn.predict_proba(X_test_cfg)\n    \n    # Store results\n    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n    storeResults(\n        'KNN',\n        name,\n        metrics.accuracy_score(y_test, y_test_pred),\n        metrics.f1_score(y_test, y_test_pred, average='macro'),\n        metrics.recall_score(y_test, y_test_pred, average='macro'),\n        metrics.precision_score(y_test, y_test_pred, average='macro'),\n        auc_score\n    )\n\n# Evaluate test accuracy and sort configurations\nresults = []\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n    knn = KNeighborsClassifier()\n    knn.fit(X_train_cfg, y_train_cfg)\n    y_test_pred = knn.predict(X_test_cfg)\n    test_accuracy = metrics.accuracy_score(y_test, y_test_pred)\n    results.append((name, test_accuracy))\n\n# Sort results by test accuracy descending\nresults_sorted = sorted(results, key=lambda x: x[1], reverse=True)\n\n# Print results\nprint(\"Configurations sorted by Test Accuracy (High to Low):\\n\")\nfor i, (name, acc) in enumerate(results_sorted, 1):\n    print(f\"{i}. {name}: Test Accuracy = {acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:56:59.376773Z","iopub.status.idle":"2025-10-06T08:56:59.377157Z","shell.execute_reply.started":"2025-10-06T08:56:59.376977Z","shell.execute_reply":"2025-10-06T08:56:59.376992Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## MLP","metadata":{}},{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nfrom sklearn import metrics\nimport pandas as pd\n\nML_Model = []\nML_Config = []\naccuracy = []\nf1_score = []\nrecall = []\nprecision = []\nauc_roc = []\n\n# Function to store results\ndef storeResults(model, config, a, b, c, d, e):\n    ML_Model.append(model)\n    ML_Config.append(config)\n    accuracy.append(round(a, 6))\n    f1_score.append(round(b, 6))\n    recall.append(round(c, 6))\n    precision.append(round(d, 6))\n    auc_roc.append(round(e, 6))\n\n# ============================================\n# MLP configurations\n# ============================================\n# Use the same configurations list you already have\nprint(f\"Total configurations: {len(configurations)}\")\nfor i, (name, X_tr, X_te, y_tr) in enumerate(configurations):\n    print(f\"{i+1}. {name}: Train shape={X_tr.shape}, Test shape={X_te.shape}, Train labels={y_tr.shape}\")\n\n# Evaluate MLP for each configuration\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n   \n    # Train MLP\n    mlp = MLPClassifier(random_state=42, max_iter=500)\n    mlp.fit(X_train_cfg, y_train_cfg)\n    \n    # Predictions\n    y_train_pred = mlp.predict(X_train_cfg)\n    y_test_pred = mlp.predict(X_test_cfg)\n    y_train_proba = mlp.predict_proba(X_train_cfg)\n    y_test_proba = mlp.predict_proba(X_test_cfg)\n    \n    # Store results\n    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n    storeResults(\n        'MLP',\n        name,\n        metrics.accuracy_score(y_test, y_test_pred),\n        metrics.f1_score(y_test, y_test_pred, average='macro'),\n        metrics.recall_score(y_test, y_test_pred, average='macro'),\n        metrics.precision_score(y_test, y_test_pred, average='macro'),\n        auc_score\n    )\n\n# Evaluate test accuracy and sort configurations\nresults = []\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n    mlp = MLPClassifier(random_state=42, max_iter=500)\n    mlp.fit(X_train_cfg, y_train_cfg)\n    y_test_pred = mlp.predict(X_test_cfg)\n    test_accuracy = metrics.accuracy_score(y_test, y_test_pred)\n    results.append((name, test_accuracy))\n\n# Sort results by test accuracy descending\nresults_sorted = sorted(results, key=lambda x: x[1], reverse=True)\n\n# Print results\nprint(\"Configurations sorted by Test Accuracy (High to Low):\\n\")\nfor i, (name, acc) in enumerate(results_sorted, 1):\n    print(f\"{i}. {name}: Test Accuracy = {acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:56:59.378543Z","iopub.status.idle":"2025-10-06T08:56:59.378816Z","shell.execute_reply.started":"2025-10-06T08:56:59.378663Z","shell.execute_reply":"2025-10-06T08:56:59.378673Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#","metadata":{}},{"cell_type":"markdown","source":"## Random forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nimport pandas as pd\n\nML_Model = []\nML_Config = []\naccuracy = []\nf1_score = []\nrecall = []\nprecision = []\nauc_roc = []\n\n# Function to store results\ndef storeResults(model, config, a, b, c, d, e):\n    ML_Model.append(model)\n    ML_Config.append(config)\n    accuracy.append(round(a, 6))\n    f1_score.append(round(b, 6))\n    recall.append(round(c, 6))\n    precision.append(round(d, 6))\n    auc_roc.append(round(e, 6))\n\n# ============================================\n# Random Forest configurations\n# ============================================\n# Use the same configurations list you already have\nprint(f\"Total configurations: {len(configurations)}\")\nfor i, (name, X_tr, X_te, y_tr) in enumerate(configurations):\n    print(f\"{i+1}. {name}: Train shape={X_tr.shape}, Test shape={X_te.shape}, Train labels={y_tr.shape}\")\n\n# Evaluate Random Forest for each configuration\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n   \n    # Train Random Forest\n    rf = RandomForestClassifier(random_state=42)\n    rf.fit(X_train_cfg, y_train_cfg)\n    \n    # Predictions\n    y_train_pred = rf.predict(X_train_cfg)\n    y_test_pred = rf.predict(X_test_cfg)\n    y_train_proba = rf.predict_proba(X_train_cfg)\n    y_test_proba = rf.predict_proba(X_test_cfg)\n    \n    # Store results\n    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n    storeResults(\n        'RandomForest',\n        name,\n        metrics.accuracy_score(y_test, y_test_pred),\n        metrics.f1_score(y_test, y_test_pred, average='macro'),\n        metrics.recall_score(y_test, y_test_pred, average='macro'),\n        metrics.precision_score(y_test, y_test_pred, average='macro'),\n        auc_score\n    )\n\n# Evaluate test accuracy and sort configurations\nresults = []\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n    rf = RandomForestClassifier(random_state=42)\n    rf.fit(X_train_cfg, y_train_cfg)\n    y_test_pred = rf.predict(X_test_cfg)\n    test_accuracy = metrics.accuracy_score(y_test, y_test_pred)\n    results.append((name, test_accuracy))\n\n# Sort results by test accuracy descending\nresults_sorted = sorted(results, key=lambda x: x[1], reverse=True)\n\n# Print results\nprint(\"Configurations sorted by Test Accuracy (High to Low):\\n\")\nfor i, (name, acc) in enumerate(results_sorted, 1):\n    print(f\"{i}. {name}: Test Accuracy = {acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:56:59.379899Z","iopub.status.idle":"2025-10-06T08:56:59.380204Z","shell.execute_reply.started":"2025-10-06T08:56:59.380048Z","shell.execute_reply":"2025-10-06T08:56:59.380062Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Best results and configurations","metadata":{}},{"cell_type":"markdown","source":"# Top 10 Model Configurations by Test Accuracy\n\n| Rank | Model                 | Configuration             | Test Accuracy |\n|------|----------------------|--------------------------|---------------|\n| 1    | Gradient Boosting     | Boruta + SMOTE + Tomek   | 0.9733        |\n| 2    | Gradient Boosting     | PowerTransformer + SMOTE | 0.9733        |\n| 3    | XGBoost               | MI + SMOTE + Tomek       | 0.9733        |\n| 4    | XGBoost               | LDA                      | 0.9733        |\n| 5    | Extra Trees           | MI                       | 0.9733        |\n| 6    | Extra Trees           | MI + SMOTE + Tomek       | 0.9733        |\n| 7    | Random Forest         | MI + SMOTE + Tomek       | 0.9733        |\n| 8    | Random Forest         | Boruta + SMOTE + Tomek   | 0.9733        |\n| 9    | Logistic Regression   | Robust + SVMSMOTE        | 0.9600        |\n| 10   | Logistic Regression   | Robust + RandomOverSampler | 0.9600      |\n\n---\n\n# Top 5 Configurations Only\n\n1. Boruta + SMOTE + Tomek  \n2. PowerTransformer + SMOTE  \n3. MI + SMOTE + Tomek  \n4. LDA  \n5. MI\n","metadata":{}},{"cell_type":"markdown","source":"# Final work","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}