{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d379875",
   "metadata": {
    "id": "Yb1fHaSMPI85",
    "papermill": {
     "duration": 0.007344,
     "end_time": "2025-10-16T04:56:01.632080",
     "exception": false,
     "start_time": "2025-10-16T04:56:01.624736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "# Importing Libraries\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b87b432f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T04:56:01.645150Z",
     "iopub.status.busy": "2025-10-16T04:56:01.644892Z",
     "iopub.status.idle": "2025-10-16T04:56:05.969732Z",
     "shell.execute_reply": "2025-10-16T04:56:05.968734Z"
    },
    "id": "_TgpA4zWnk0L",
    "outputId": "9a6af836-a31a-4227-d4e0-04e98de5aa57",
    "papermill": {
     "duration": 4.333321,
     "end_time": "2025-10-16T04:56:05.971632",
     "exception": false,
     "start_time": "2025-10-16T04:56:01.638311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boruta in /usr/local/lib/python3.11/dist-packages (0.4.3)\r\n",
      "Requirement already satisfied: category_encoders in /usr/local/lib/python3.11/dist-packages (2.7.0)\r\n",
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.0.3)\r\n",
      "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.11/dist-packages (from boruta) (1.26.4)\r\n",
      "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.11/dist-packages (from boruta) (1.2.2)\r\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.11/dist-packages (from boruta) (1.15.3)\r\n",
      "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (2.2.3)\r\n",
      "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.0.1)\r\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (0.14.5)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10.4->boruta) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10.4->boruta) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10.4->boruta) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10.4->boruta) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10.4->boruta) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10.4->boruta) (2.4.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.17.1->boruta) (1.5.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.17.1->boruta) (3.6.0)\r\n",
      "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.9.0->category_encoders) (25.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category_encoders) (1.17.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.10.4->boruta) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.10.4->boruta) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.10.4->boruta) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.10.4->boruta) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.10.4->boruta) (2024.2.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install boruta category_encoders xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac375a55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T04:56:05.986601Z",
     "iopub.status.busy": "2025-10-16T04:56:05.986375Z",
     "iopub.status.idle": "2025-10-16T04:56:43.782781Z",
     "shell.execute_reply": "2025-10-16T04:56:43.782148Z"
    },
    "id": "diEVyiMoJ4Cm",
    "papermill": {
     "duration": 37.80568,
     "end_time": "2025-10-16T04:56:43.784203",
     "exception": false,
     "start_time": "2025-10-16T04:56:05.978523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boruta in /usr/local/lib/python3.11/dist-packages (0.4.3)\r\n",
      "Requirement already satisfied: category_encoders in /usr/local/lib/python3.11/dist-packages (2.7.0)\r\n",
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.0.3)\r\n",
      "Requirement already satisfied: catboost in /usr/local/lib/python3.11/dist-packages (1.2.8)\r\n",
      "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.11/dist-packages (from boruta) (1.26.4)\r\n",
      "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.11/dist-packages (from boruta) (1.2.2)\r\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.11/dist-packages (from boruta) (1.15.3)\r\n",
      "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (2.2.3)\r\n",
      "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.0.1)\r\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (0.14.5)\r\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.21)\r\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.7.2)\r\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\r\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10.4->boruta) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10.4->boruta) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10.4->boruta) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10.4->boruta) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10.4->boruta) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10.4->boruta) (2.4.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.17.1->boruta) (1.5.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.17.1->boruta) (3.6.0)\r\n",
      "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.9.0->category_encoders) (25.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.59.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.3.0)\r\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.0.9)\r\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (8.5.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.10.4->boruta) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.10.4->boruta) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.10.4->boruta) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.10.4->boruta) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.10.4->boruta) (2024.2.0)\r\n",
      "Found existing installation: scikit-learn 1.2.2\r\n",
      "Uninstalling scikit-learn-1.2.2:\r\n",
      "  Successfully uninstalled scikit-learn-1.2.2\r\n",
      "Found existing installation: imbalanced-learn 0.13.0\r\n",
      "Uninstalling imbalanced-learn-0.13.0:\r\n",
      "  Successfully uninstalled imbalanced-learn-0.13.0\r\n",
      "\u001b[33mWARNING: Skipping scikit-learn as it is not installed.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Skipping imbalanced-learn as it is not installed.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mCollecting scikit-learn==1.4.2\r\n",
      "  Downloading scikit_learn-1.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\r\n",
      "Collecting imbalanced-learn==0.12.3\r\n",
      "  Downloading imbalanced_learn-0.12.3-py3-none-any.whl.metadata (8.3 kB)\r\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2) (1.15.3)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2) (1.5.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2) (3.6.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.4.2) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.4.2) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.4.2) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.4.2) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.4.2) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.4.2) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.5->scikit-learn==1.4.2) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.5->scikit-learn==1.4.2) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.5->scikit-learn==1.4.2) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.5->scikit-learn==1.4.2) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.5->scikit-learn==1.4.2) (2024.2.0)\r\n",
      "Downloading scikit_learn-1.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m204.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading imbalanced_learn-0.12.3-py3-none-any.whl (258 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.3/258.3 kB\u001b[0m \u001b[31m334.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: scikit-learn, imbalanced-learn\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\r\n",
      "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.4.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed imbalanced-learn-0.12.3 scikit-learn-1.4.2\r\n",
      "Requirement already satisfied: scikit-learn==1.4.2 in /usr/local/lib/python3.11/dist-packages (1.4.2)\r\n",
      "Collecting imbalanced-learn==0.12.0\r\n",
      "  Downloading imbalanced_learn-0.12.0-py3-none-any.whl.metadata (8.2 kB)\r\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2) (1.15.3)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2) (1.5.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2) (3.6.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.4.2) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.4.2) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.4.2) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.4.2) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.4.2) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.4.2) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.5->scikit-learn==1.4.2) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.5->scikit-learn==1.4.2) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.5->scikit-learn==1.4.2) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.5->scikit-learn==1.4.2) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.5->scikit-learn==1.4.2) (2024.2.0)\r\n",
      "Downloading imbalanced_learn-0.12.0-py3-none-any.whl (257 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.7/257.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: imbalanced-learn\r\n",
      "  Attempting uninstall: imbalanced-learn\r\n",
      "    Found existing installation: imbalanced-learn 0.12.3\r\n",
      "    Uninstalling imbalanced-learn-0.12.3:\r\n",
      "      Successfully uninstalled imbalanced-learn-0.12.3\r\n",
      "Successfully installed imbalanced-learn-0.12.0\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 04:56:32.291560: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760590592.536875      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1760590592.601085      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "!pip install boruta category_encoders xgboost catboost\n",
    "\n",
    "\n",
    "\n",
    "!pip uninstall -y scikit-learn imbalanced-learn\n",
    "# Step 1: Uninstall old versions (run twice to ensure cleanup)\n",
    "!pip uninstall -y scikit-learn imbalanced-learn\n",
    "\n",
    "# Step 2: Reinstall compatible latest versions\n",
    "!pip install --upgrade --no-cache-dir scikit-learn==1.4.2 imbalanced-learn==0.12.3\n",
    "!pip install scikit-learn==1.4.2 imbalanced-learn==0.12.0\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, RobustScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier, StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler, SMOTENC\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour, TomekLinks, RandomUnderSampler\n",
    "from boruta import BorutaPy\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c76c05",
   "metadata": {
    "id": "unb2qil8SoTn",
    "papermill": {
     "duration": 0.007419,
     "end_time": "2025-10-16T04:56:43.799993",
     "exception": false,
     "start_time": "2025-10-16T04:56:43.792574",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset Load & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc6552cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T04:56:43.816400Z",
     "iopub.status.busy": "2025-10-16T04:56:43.815851Z",
     "iopub.status.idle": "2025-10-16T04:56:43.887084Z",
     "shell.execute_reply": "2025-10-16T04:56:43.886437Z"
    },
    "id": "R6yAAGUeSsn3",
    "papermill": {
     "duration": 0.080857,
     "end_time": "2025-10-16T04:56:43.888376",
     "exception": false,
     "start_time": "2025-10-16T04:56:43.807519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/sleep-health-and-lifestyle-dataset/Sleep_health_and_lifestyle_dataset.csv\")\n",
    "df.fillna(\"None\", inplace=True)\n",
    "\n",
    "df_train, df_test = train_test_split(\n",
    "    df, test_size=0.2, random_state=42, stratify=df['Sleep Disorder']\n",
    ")\n",
    "\n",
    "for d in (df_train, df_test):\n",
    "    # Dividing Blood Pressure into Systolic and Diastolic BP\n",
    "    d[['Systolic BP', 'Diastolic BP']] = d['Blood Pressure'].str.split('/', expand=True).astype(int)\n",
    "    d.drop(['Person ID', 'Blood Pressure'], axis=1, inplace=True)\n",
    "\n",
    "    # Labeling less number of careers as other\n",
    "    d['Occupation'] = d['Occupation'].replace(['Manager', 'Sales Representative', 'Scientist', 'Software Engineer'], 'Other')\n",
    "\n",
    "    # Adding the average BMI for the range\n",
    "    d['BMI Category'] = d['BMI Category'].replace({'Normal':22, 'Normal Weight':22, 'Overweight':27, 'Obese':30})\n",
    "\n",
    "    # Creating Interaction features\n",
    "    eps = 1e-6\n",
    "    d['Stress_sleep_interaction'] = d['Stress Level'] / (d['Quality of Sleep'] + eps)\n",
    "    d['BMI_Activity'] = d['BMI Category'] * d['Physical Activity Level']\n",
    "    d['Sleep_Heart_ratio'] = d['Sleep Duration'] / (d['Heart Rate'] + eps)\n",
    "    d['Sleep_Steps_ratio'] = d['Sleep Duration'] / (d['Daily Steps'] + eps)\n",
    "    d['Sleep_Stress_ratio'] = d['Sleep Duration'] / (d['Stress Level'] + eps)\n",
    "    d['Pulse_Pressure'] = d['Systolic BP'] - d['Diastolic BP']\n",
    "    d['log_steps'] = np.log1p(d['Daily Steps'])\n",
    "    d['sqrt_sleep'] = np.sqrt(d['Sleep Duration'])\n",
    "    d.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    d.dropna(inplace=True)\n",
    "\n",
    "# One-hot encode Occupation on train, then align test to train columns\n",
    "df_train = pd.get_dummies(df_train, columns=['Occupation'], drop_first=False)\n",
    "df_test  = pd.get_dummies(df_test,  columns=['Occupation'], drop_first=False)\n",
    "\n",
    "# Ensure test has same dummy columns as train (add missing columns with 0)\n",
    "df_test = df_test.reindex(columns=df_train.columns, fill_value=0)\n",
    "\n",
    "# Label encode Gender using encoder fitted on train (no leakage)\n",
    "le_gender = LabelEncoder()\n",
    "df_train['Gender'] = le_gender.fit_transform(df_train['Gender'])\n",
    "# transform test using the same encoder; if unseen label appears this will raise — same behavior as original approach\n",
    "df_test['Gender'] = le_gender.transform(df_test['Gender'])\n",
    "\n",
    "# Encode target (Sleep Disorder) using encoder fitted on train only\n",
    "le_target = LabelEncoder()\n",
    "y_train = le_target.fit_transform(df_train['Sleep Disorder'])\n",
    "y_test  = le_target.transform(df_test['Sleep Disorder'])\n",
    "\n",
    "# Prepare X_train and X_test (drop target column exactly like original)\n",
    "X_train = df_train.drop('Sleep Disorder', axis=1)\n",
    "X_test  = df_test.drop('Sleep Disorder', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68313236",
   "metadata": {
    "id": "auoUKdHC731t",
    "papermill": {
     "duration": 0.007414,
     "end_time": "2025-10-16T04:56:43.903980",
     "exception": false,
     "start_time": "2025-10-16T04:56:43.896566",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Apply RobustSclaer, MI, LDA, Boruta, Autoencoder, and SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc9390bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T04:56:43.920182Z",
     "iopub.status.busy": "2025-10-16T04:56:43.919951Z",
     "iopub.status.idle": "2025-10-16T04:56:58.789652Z",
     "shell.execute_reply": "2025-10-16T04:56:58.788980Z"
    },
    "id": "b5AYEVMw8BMY",
    "outputId": "5ce8ce9f-1f9c-4b46-a3e2-396d7ef1331f",
    "papermill": {
     "duration": 14.879295,
     "end_time": "2025-10-16T04:56:58.790967",
     "exception": false,
     "start_time": "2025-10-16T04:56:43.911672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1760590613.253126      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1760590613.253833      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1760590615.978304     112 service.cc:148] XLA service 0x794a30011fb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1760590615.979181     112 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1760590615.979202     112 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1760590616.223387     112 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1760590617.140709     112 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n"
     ]
    }
   ],
   "source": [
    "#### Pipeline 1 - Robustscaler -> MI - LDA  ####\n",
    "#Normalize the data\n",
    "scaler = RobustScaler()\n",
    "X_train_robust = scaler.fit_transform(X_train)\n",
    "X_test_robust = scaler.transform(X_test)\n",
    "\n",
    "smotetomek = SMOTETomek(sampling_strategy='auto',\n",
    "                   smote=SMOTE(k_neighbors=3, random_state=42),\n",
    "                   tomek=TomekLinks(sampling_strategy='auto', n_jobs=-1),\n",
    "                   n_jobs=-1,\n",
    "                   random_state=42)\n",
    "\n",
    "X_train_robust_resample, y_train_robust_resample = smotetomek.fit_resample(X_train_robust, y_train)\n",
    "\n",
    "# Applying Mutual information\n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=5)\n",
    "X_train_mi = mi.fit_transform(X_train_robust, y_train)\n",
    "X_test_mi = mi.transform(X_test_robust)\n",
    "\n",
    "# Applying LDA\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "X_train_lda = lda.fit_transform(X_train_mi, y_train)\n",
    "X_test_lda = lda.transform(X_test_mi)\n",
    "\n",
    "# Resample MI and LDA data\n",
    "X_train_mi_res, y_train_mi_res = smotetomek.fit_resample(X_train_mi, y_train)\n",
    "X_train_lda_res, y_train_lda_res = smotetomek.fit_resample(X_train_lda, y_train)\n",
    "\n",
    "#### Pipeline 2 - MinMaxscaler -> Boruta - Autoencoder  ####\n",
    "#Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_minmax = scaler.fit_transform(X_train)\n",
    "X_test_minmax = scaler.transform(X_test)\n",
    "\n",
    "X_train_minmax_resample, y_train_minmax_resample = smotetomek.fit_resample(X_train_minmax, y_train)\n",
    "\n",
    "# RandomForest classifier\n",
    "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Applying Boruta Feature Selection\n",
    "boruta_selector = BorutaPy(rfc, n_estimators='auto', verbose=0, random_state=42)\n",
    "\n",
    "X_train_boruta = boruta_selector.fit_transform(X_train_minmax, y_train)\n",
    "X_test_boruta = boruta_selector.transform(X_test_minmax)\n",
    "\n",
    "# applying Autoencoder\n",
    "n_features = X_train_boruta.shape[1]\n",
    "input_layer = Input(shape=(n_features,))\n",
    "encoded     = Dense(32, activation='relu')(input_layer)\n",
    "bottleneck  = Dense(16, activation='relu')(encoded)\n",
    "decoded     = Dense(32, activation='relu')(bottleneck)\n",
    "output_layer= Dense(n_features, activation='sigmoid')(decoded)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "autoencoder.fit(X_train_boruta, X_train_boruta, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "# Encoder-only transform (you created these; keeping intact)\n",
    "encoder = Model(inputs=input_layer, outputs=bottleneck)\n",
    "X_train_encoded = encoder.predict(X_train_boruta)\n",
    "X_test_encoded  = encoder.predict(X_test_boruta)\n",
    "\n",
    "# Resample MI and LDA data\n",
    "X_train_boruta_res, y_train_boruta_res = smotetomek.fit_resample(X_train_boruta, y_train)\n",
    "X_train_encoded_res, y_train_encoded_res = smotetomek.fit_resample(X_train_encoded, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10891b05",
   "metadata": {
    "id": "TpEDoK5Ihy4W",
    "papermill": {
     "duration": 0.007691,
     "end_time": "2025-10-16T04:56:58.807066",
     "exception": false,
     "start_time": "2025-10-16T04:56:58.799375",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ML Model Result and parameter Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "024af123",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T04:56:58.823774Z",
     "iopub.status.busy": "2025-10-16T04:56:58.823513Z",
     "iopub.status.idle": "2025-10-16T04:56:58.831704Z",
     "shell.execute_reply": "2025-10-16T04:56:58.831169Z"
    },
    "id": "tlV5BgjKhyYX",
    "papermill": {
     "duration": 0.017765,
     "end_time": "2025-10-16T04:56:58.832667",
     "exception": false,
     "start_time": "2025-10-16T04:56:58.814902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Initialize lists to store model performance results\n",
    "ML_Model = []\n",
    "ML_Config = []\n",
    "accuracy = []\n",
    "f1_score = []\n",
    "recall = []\n",
    "precision = []\n",
    "auc_roc = []\n",
    "\n",
    "def storeResults(model, config, a, b, c, d, e, csv_file='model_performance.csv'):\n",
    "    \"\"\"\n",
    "    Store model performance results in a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    model: Name of the ML model\n",
    "    config: Configuration name (preprocessing steps applied)\n",
    "    a: Accuracy score\n",
    "    b: F1 score\n",
    "    c: Recall score\n",
    "    d: Precision score\n",
    "    e: AUC-ROC score\n",
    "    csv_file: The CSV file where the performance results will be stored\n",
    "    \"\"\"\n",
    "    # Open the CSV file in append mode\n",
    "    with open(csv_file, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        # Write the header if the file is empty (only the first time)\n",
    "        if file.tell() == 0:\n",
    "            writer.writerow(['Model', 'Configuration', 'Accuracy', 'F1 Score', 'Recall', 'Precision', 'AUC-ROC'])\n",
    "\n",
    "        # Write the model performance results to the CSV file\n",
    "        writer.writerow([model, config, round(a, 6), round(b, 6), round(c, 6), round(d, 6), round(e, 6)])\n",
    "\n",
    "    # Optionally, store in memory as well\n",
    "    ML_Model.append(model)\n",
    "    ML_Config.append(config)\n",
    "    accuracy.append(round(a, 6))\n",
    "    f1_score.append(round(b, 6))\n",
    "    recall.append(round(c, 6))\n",
    "    precision.append(round(d, 6))\n",
    "    auc_roc.append(round(e, 6))\n",
    "\n",
    "# Example usage:\n",
    "# storeResults('SVM', 'Boruta + SMOTE+Tomek', 0.92, 0.91, 0.93, 0.90, 0.94)\n",
    "\n",
    "\n",
    "best_params_dict = {}\n",
    "\n",
    "import csv\n",
    "\n",
    "def storeBestParams(config_name, best_params, classifier_name, csv_file='best_params.csv'):\n",
    "    \"\"\"\n",
    "    Store the best parameters for each classifier and configuration in a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    config_name: Name of the configuration (e.g., preprocessing applied)\n",
    "    best_params: Best hyperparameters found by the model\n",
    "    classifier_name: Name of the classifier (e.g., 'KNN', 'SVM', etc.)\n",
    "    csv_file: The CSV file where the best parameters will be stored\n",
    "    \"\"\"\n",
    "        # Check if the classifier already exists in the dictionary, if not, initialize it\n",
    "    if classifier_name not in best_params_dict:\n",
    "        best_params_dict[classifier_name] = {}\n",
    "\n",
    "    # Store the best parameters for the given configuration in the dictionary\n",
    "    best_params_dict[classifier_name][config_name] = best_params\n",
    "    # Open the CSV file in append mode\n",
    "    with open(csv_file, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        # Write the header if the file is empty (only the first time)\n",
    "        if file.tell() == 0:\n",
    "            writer.writerow(['Classifier', 'Configuration', 'Best Parameters'])\n",
    "\n",
    "        # Write the best parameters for the given classifier and configuration\n",
    "        writer.writerow([classifier_name, config_name, str(best_params)])\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# storeBestParams('Boruta + SMOTE+Tomek', {'C': 1, 'gamma': 'scale'}, 'SVM')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551622ec",
   "metadata": {
    "id": "Pg8p5Fg07Hsd",
    "papermill": {
     "duration": 0.007681,
     "end_time": "2025-10-16T04:56:58.848035",
     "exception": false,
     "start_time": "2025-10-16T04:56:58.840354",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ed90496",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T04:56:58.864635Z",
     "iopub.status.busy": "2025-10-16T04:56:58.864438Z",
     "iopub.status.idle": "2025-10-16T05:02:59.975982Z",
     "shell.execute_reply": "2025-10-16T05:02:59.975155Z"
    },
    "id": "axcfe3_47JuW",
    "outputId": "24d9a5f8-28d0-487b-888c-bca825307219",
    "papermill": {
     "duration": 361.135009,
     "end_time": "2025-10-16T05:02:59.990742",
     "exception": false,
     "start_time": "2025-10-16T04:56:58.855733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Logistic Regression with Original Data configuration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Model Performance Metrics\n",
      "Configuration Name:  Original Data\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.913043  0.895726 0.891459   0.900717 0.956114\n",
      "    Test  0.973333  0.956583 0.955556   0.962963 0.998183\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'solver': 'liblinear', 'penalty': 'l1', 'C': 100.0}\n",
      "\n",
      "Running Logistic Regression with Normalized Data with RobustScaler configuration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Model Performance Metrics\n",
      "Configuration Name:  Normalized Data with RobustScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.916388  0.901213 0.896836   0.905995 0.960341\n",
      "    Test  0.946667  0.921421 0.927146   0.916340 0.997812\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'solver': 'liblinear', 'penalty': 'l1', 'C': 88.89}\n",
      "\n",
      "Running Logistic Regression with SMOTETomek + RobustScaler configuration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Model Performance Metrics\n",
      "Configuration Name:  SMOTETomek + RobustScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.902672  0.902682 0.902759   0.908230 0.970730\n",
      "    Test  0.960000  0.943788 0.947980   0.947368 0.992627\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'solver': 'lbfgs', 'penalty': 'l2', 'C': 22.23}\n",
      "\n",
      "Running Logistic Regression with MI configuration...\n",
      "\n",
      "Logistic Regression Model Performance Metrics\n",
      "Configuration Name:  MI\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.886288  0.858664 0.858863   0.859107 0.918226\n",
      "    Test  0.946667  0.922244 0.928535   0.916667 0.973541\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'solver': 'saga', 'penalty': 'l1', 'C': 33.339999999999996}\n",
      "\n",
      "Running Logistic Regression with MI + SMOTETomek configuration...\n",
      "\n",
      "Logistic Regression Model Performance Metrics\n",
      "Configuration Name:  MI + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.846154  0.847108 0.846443   0.848502 0.936851\n",
      "    Test  0.826667  0.806257 0.845707   0.796249 0.959477\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'solver': 'saga', 'penalty': 'l2', 'C': 44.449999999999996}\n",
      "\n",
      "Running Logistic Regression with LDA configuration...\n",
      "\n",
      "Logistic Regression Model Performance Metrics\n",
      "Configuration Name:  LDA\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.846154  0.819428 0.825591   0.816838 0.912356\n",
      "    Test  0.920000  0.897154 0.913384   0.890058 0.958210\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'solver': 'lbfgs', 'penalty': 'l2', 'C': 11.12}\n",
      "\n",
      "Running Logistic Regression with LDA + SMOTETomek configuration...\n",
      "\n",
      "Logistic Regression Model Performance Metrics\n",
      "Configuration Name:  LDA + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.831740  0.833444 0.831808   0.838737 0.927248\n",
      "    Test  0.813333  0.798932 0.838131   0.798333 0.953568\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'solver': 'saga', 'penalty': 'l2', 'C': 77.78}\n",
      "\n",
      "Running Logistic Regression with Normalized Data with MinMaxScaler configuration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Model Performance Metrics\n",
      "Configuration Name:  Normalized Data with MinMaxScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.913043  0.895726 0.891459   0.900717 0.953633\n",
      "    Test  0.973333  0.956583 0.955556   0.962963 0.999294\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'solver': 'liblinear', 'penalty': 'l1', 'C': 55.559999999999995}\n",
      "\n",
      "Running Logistic Regression with SMOTETomek + MiMaxScaler configuration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Model Performance Metrics\n",
      "Configuration Name:  SMOTETomek + MiMaxScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.908397  0.908545 0.908462   0.912970 0.967040\n",
      "    Test  0.946667  0.937442 0.955051   0.925146 0.981516\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'solver': 'liblinear', 'penalty': 'l2', 'C': 22.23}\n",
      "\n",
      "Running Logistic Regression with Boruta configuration...\n",
      "\n",
      "Logistic Regression Model Performance Metrics\n",
      "Configuration Name:  Boruta\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.899666  0.874035 0.869954   0.878348 0.933605\n",
      "    Test  0.933333  0.900930 0.904924   0.904184 0.972546\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'solver': 'liblinear', 'penalty': 'l2', 'C': 100.0}\n",
      "\n",
      "Running Logistic Regression with Boruta + SMOTETomek configuration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Model Performance Metrics\n",
      "Configuration Name:  Boruta + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.868069  0.867967 0.868175   0.870618 0.955176\n",
      "    Test  0.933333  0.910146 0.919571   0.906015 0.959702\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'solver': 'saga', 'penalty': 'l1', 'C': 11.12}\n",
      "\n",
      "Running Logistic Regression with Autoencoder configuration...\n",
      "\n",
      "Logistic Regression Model Performance Metrics\n",
      "Configuration Name:  Autoencoder\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.886288  0.864413 0.858863   0.871068 0.930268\n",
      "    Test  0.946667  0.935980 0.941793   0.930810 0.973504\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'solver': 'liblinear', 'penalty': 'l2', 'C': 88.89}\n",
      "\n",
      "Running Logistic Regression with Autoencoder + SMOTETomek configuration...\n",
      "\n",
      "Logistic Regression Model Performance Metrics\n",
      "Configuration Name:  Autoencoder + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.863985  0.863959 0.864135   0.866871 0.944912\n",
      "    Test  0.893333  0.861905 0.882197   0.850000 0.965640\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'solver': 'liblinear', 'penalty': 'l1', 'C': 33.339999999999996}\n"
     ]
    }
   ],
   "source": [
    "##### import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "configurations = []\n",
    "\n",
    "configurations.append(('Original Data', X_train, X_test, y_train))\n",
    "configurations.append(('Normalized Data with RobustScaler', X_train_robust, X_test_robust, y_train))\n",
    "configurations.append(('SMOTETomek + RobustScaler', X_train_robust_resample, X_test_robust, y_train_robust_resample))\n",
    "configurations.append(('MI', X_train_mi, X_test_mi, y_train))\n",
    "configurations.append(('MI + SMOTETomek', X_train_mi_res, X_test_mi, y_train_mi_res))\n",
    "configurations.append(('LDA', X_train_lda, X_test_lda, y_train))\n",
    "configurations.append(('LDA + SMOTETomek', X_train_lda_res, X_test_lda, y_train_lda_res))\n",
    "configurations.append(('Normalized Data with MinMaxScaler', X_train_minmax, X_test_minmax, y_train))\n",
    "configurations.append(('SMOTETomek + MiMaxScaler', X_train_minmax_resample, X_test_minmax, y_train_minmax_resample))\n",
    "configurations.append(('Boruta', X_train_boruta, X_test_boruta, y_train))\n",
    "configurations.append(('Boruta + SMOTETomek', X_train_boruta_res, X_test_boruta, y_train_boruta_res))\n",
    "configurations.append(('Autoencoder', X_train_encoded, X_test_encoded, y_train))\n",
    "configurations.append(('Autoencoder + SMOTETomek', X_train_encoded_res, X_test_encoded, y_train_encoded_res))\n",
    "\n",
    "params = [\n",
    "    {\n",
    "        'solver': ['saga'],\n",
    "        'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "        'C': np.linspace(0.01, 100, 10)\n",
    "    },\n",
    "    {\n",
    "        'solver': ['lbfgs'],\n",
    "        'penalty': ['l2', 'none'],\n",
    "        'C': np.linspace(0.01, 100, 10)\n",
    "    },\n",
    "    {\n",
    "        'solver': ['liblinear'],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'C': np.linspace(0.01, 100, 10)\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
    "    print(f\"\\nRunning Logistic Regression with {name} configuration...\")\n",
    "    cv = StratifiedKFold(n_splits=8, shuffle=True, random_state=42)\n",
    "    logr = RandomizedSearchCV(LogisticRegression(max_iter=5000), params, cv=cv, n_iter=50,\n",
    "                             n_jobs=-1, scoring=['accuracy', 'f1_macro'], refit='accuracy', verbose=0)\n",
    "    # logr= GridSearchCV(LogisticRegression(), params, cv=5, n_jobs=-1, scoring='accuracy', verbose=2)\n",
    "    logr.fit(X_train_cfg, y_train_cfg)\n",
    "\n",
    "    y_train_lr = logr.predict(X_train_cfg)\n",
    "    y_test_lr = logr.predict(X_test_cfg)\n",
    "    y_train_lr_proba = logr.predict_proba(X_train_cfg)\n",
    "    y_test_lr_proba = logr.predict_proba(X_test_cfg)\n",
    "\n",
    "    metrics_dict = {\n",
    "          \"Dataset\": [\"Training\", \"Test\"],\n",
    "          \"Accuracy\": [\n",
    "              metrics.accuracy_score(y_train_cfg, y_train_lr),\n",
    "              metrics.accuracy_score(y_test, y_test_lr),\n",
    "          ],\n",
    "          \"F1 Score\": [\n",
    "              metrics.f1_score(y_train_cfg, y_train_lr, average='macro'),\n",
    "              metrics.f1_score(y_test, y_test_lr, average='macro'),\n",
    "          ],\n",
    "          \"Recall\": [\n",
    "              metrics.recall_score(y_train_cfg, y_train_lr, average='macro'),\n",
    "              metrics.recall_score(y_test, y_test_lr, average='macro'),\n",
    "          ],\n",
    "          \"Precision\": [\n",
    "              metrics.precision_score(y_train_cfg, y_train_lr, average='macro'),\n",
    "              metrics.precision_score(y_test, y_test_lr, average='macro'),\n",
    "          ],\n",
    "          \"AUC-ROC\": [\n",
    "              metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_lr_proba, multi_class='ovr', average='macro'),\n",
    "              metrics.roc_auc_score(pd.get_dummies(y_test), y_test_lr_proba, multi_class='ovr', average='macro'),\n",
    "          ]\n",
    "      }\n",
    "\n",
    "    df_metrics = pd.DataFrame(metrics_dict)\n",
    "    print(\"\\nLogistic Regression Model Performance Metrics\")\n",
    "    print(\"Configuration Name: \", name)\n",
    "    print(df_metrics.to_string(index=False))\n",
    "\n",
    "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_lr_proba, multi_class='ovr', average='macro')\n",
    "    storeResults(\n",
    "          'Logistic Regression',\n",
    "          name,\n",
    "          metrics.accuracy_score(y_test, y_test_lr),\n",
    "          metrics.f1_score(y_test, y_test_lr, average='macro'),\n",
    "          metrics.recall_score(y_test, y_test_lr, average='macro'),\n",
    "          metrics.precision_score(y_test, y_test_lr, average='macro'),\n",
    "          auc_score\n",
    "      )\n",
    "    storeBestParams(name, logr.best_params_, \"logistic regression\")\n",
    "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
    "    print(logr.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99ee246",
   "metadata": {
    "id": "BmgqNGgGKtdB",
    "papermill": {
     "duration": 0.013954,
     "end_time": "2025-10-16T05:03:00.019023",
     "exception": false,
     "start_time": "2025-10-16T05:03:00.005069",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9ec4a97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:03:00.048582Z",
     "iopub.status.busy": "2025-10-16T05:03:00.048307Z",
     "iopub.status.idle": "2025-10-16T05:03:05.531225Z",
     "shell.execute_reply": "2025-10-16T05:03:05.530364Z"
    },
    "id": "aZEKc2YMKRB8",
    "outputId": "f805fda9-b8e9-4e6d-a8a4-fbc9f41cff52",
    "papermill": {
     "duration": 5.499264,
     "end_time": "2025-10-16T05:03:05.532537",
     "exception": false,
     "start_time": "2025-10-16T05:03:00.033273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running KNN with Original Data configuration...\n",
      "\n",
      "KNearestNeighbors Model Performance Metrics\n",
      "Configuration Name:  Original Data\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.923077  0.908611 0.904117   0.913518 0.987279\n",
      "    Test  0.920000  0.895256 0.911995   0.881944 0.951633\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'weights': 'distance', 'p': 2, 'n_neighbors': 17, 'metric': 'euclidean'}\n",
      "\n",
      "Running KNN with Normalized Data with RobustScaler configuration...\n",
      "\n",
      "KNearestNeighbors Model Performance Metrics\n",
      "Configuration Name:  Normalized Data with RobustScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.923077  0.908611 0.904117   0.913518 0.987279\n",
      "    Test  0.946667  0.921421 0.927146   0.916340 0.956252\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'weights': 'distance', 'p': 2, 'n_neighbors': 17, 'metric': 'manhattan'}\n",
      "\n",
      "Running KNN with SMOTETomek + RobustScaler configuration...\n",
      "\n",
      "KNearestNeighbors Model Performance Metrics\n",
      "Configuration Name:  SMOTETomek + RobustScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.944656   0.94472 0.944641   0.944999 0.975128\n",
      "    Test  0.893333   0.85604 0.870328   0.850292 0.958432\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'weights': 'distance', 'p': 2, 'n_neighbors': 2, 'metric': 'manhattan'}\n",
      "\n",
      "Running KNN with MI configuration...\n",
      "\n",
      "KNearestNeighbors Model Performance Metrics\n",
      "Configuration Name:  MI\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.913043  0.895726 0.891459   0.900717 0.977633\n",
      "    Test  0.960000  0.943077 0.949369   0.937500 0.962026\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'weights': 'distance', 'p': 2, 'n_neighbors': 45, 'metric': 'manhattan'}\n",
      "\n",
      "Running KNN with MI + SMOTETomek configuration...\n",
      "\n",
      "KNearestNeighbors Model Performance Metrics\n",
      "Configuration Name:  MI + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.948077  0.948196 0.948179   0.948432 0.989827\n",
      "    Test  0.986667  0.978495 0.979167   0.979167 0.981784\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'weights': 'distance', 'p': 2, 'n_neighbors': 17, 'metric': 'euclidean'}\n",
      "\n",
      "Running KNN with LDA configuration...\n",
      "\n",
      "KNearestNeighbors Model Performance Metrics\n",
      "Configuration Name:  LDA\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.913043  0.895726 0.891459   0.900717 0.977633\n",
      "    Test  0.960000  0.935214 0.934722   0.936975 0.961638\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'weights': 'distance', 'p': 2, 'n_neighbors': 17, 'metric': 'manhattan'}\n",
      "\n",
      "Running KNN with LDA + SMOTETomek configuration...\n",
      "\n",
      "KNearestNeighbors Model Performance Metrics\n",
      "Configuration Name:  LDA + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.948375  0.948450 0.948417   0.948701 0.988251\n",
      "    Test  0.960000  0.935214 0.934722   0.936975 0.963809\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'weights': 'distance', 'p': 2, 'n_neighbors': 17, 'metric': 'euclidean'}\n",
      "\n",
      "Running KNN with Normalized Data with MinMaxScaler configuration...\n",
      "\n",
      "KNearestNeighbors Model Performance Metrics\n",
      "Configuration Name:  Normalized Data with MinMaxScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training   0.87291  0.850383 0.840829   0.862841 0.949174\n",
      "    Test   0.92000  0.912757 0.911995   0.914773 0.984427\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'weights': 'uniform', 'p': 2, 'n_neighbors': 17, 'metric': 'manhattan'}\n",
      "\n",
      "Running KNN with SMOTETomek + MiMaxScaler configuration...\n",
      "\n",
      "KNearestNeighbors Model Performance Metrics\n",
      "Configuration Name:  SMOTETomek + MiMaxScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.944656  0.944720 0.944641   0.944999 0.975128\n",
      "    Test  0.880000  0.854394 0.877399   0.854978 0.947550\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'weights': 'distance', 'p': 2, 'n_neighbors': 2, 'metric': 'manhattan'}\n",
      "\n",
      "Running KNN with Boruta configuration...\n",
      "\n",
      "KNearestNeighbors Model Performance Metrics\n",
      "Configuration Name:  Boruta\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.872910  0.847108 0.844301   0.854838 0.927273\n",
      "    Test  0.933333  0.920960 0.920960   0.926025 0.987187\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'weights': 'uniform', 'p': 2, 'n_neighbors': 45, 'metric': 'manhattan'}\n",
      "\n",
      "Running KNN with Boruta + SMOTETomek configuration...\n",
      "\n",
      "KNearestNeighbors Model Performance Metrics\n",
      "Configuration Name:  Boruta + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.952199  0.952301 0.952227   0.952635 0.994913\n",
      "    Test  0.880000  0.843196 0.862753   0.828976 0.954274\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'weights': 'distance', 'p': 2, 'n_neighbors': 17, 'metric': 'chebyshev'}\n",
      "\n",
      "Running KNN with Autoencoder configuration...\n",
      "\n",
      "KNearestNeighbors Model Performance Metrics\n",
      "Configuration Name:  Autoencoder\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.876254  0.846363 0.846206   0.851076 0.945696\n",
      "    Test  0.933333  0.923238 0.934217   0.917989 0.987058\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'weights': 'uniform', 'p': 2, 'n_neighbors': 17, 'metric': 'chebyshev'}\n",
      "\n",
      "Running KNN with Autoencoder + SMOTETomek configuration...\n",
      "\n",
      "KNearestNeighbors Model Performance Metrics\n",
      "Configuration Name:  Autoencoder + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.944444  0.944537 0.944410   0.944924 0.973750\n",
      "    Test  0.893333  0.873476 0.884975   0.872598 0.970731\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'weights': 'distance', 'p': 2, 'n_neighbors': 2, 'metric': 'euclidean'}\n"
     ]
    }
   ],
   "source": [
    "configurations = []\n",
    "\n",
    "configurations.append(('Original Data', X_train, X_test, y_train))\n",
    "configurations.append(('Normalized Data with RobustScaler', X_train_robust, X_test_robust, y_train))\n",
    "configurations.append(('SMOTETomek + RobustScaler', X_train_robust_resample, X_test_robust, y_train_robust_resample))\n",
    "configurations.append(('MI', X_train_mi, X_test_mi, y_train))\n",
    "configurations.append(('MI + SMOTETomek', X_train_mi_res, X_test_mi, y_train_mi_res))\n",
    "configurations.append(('LDA', X_train_lda, X_test_lda, y_train))\n",
    "configurations.append(('LDA + SMOTETomek', X_train_lda_res, X_test_lda, y_train_lda_res))\n",
    "configurations.append(('Normalized Data with MinMaxScaler', X_train_minmax, X_test_minmax, y_train))\n",
    "configurations.append(('SMOTETomek + MiMaxScaler', X_train_minmax_resample, X_test_minmax, y_train_minmax_resample))\n",
    "configurations.append(('Boruta', X_train_boruta, X_test_boruta, y_train))\n",
    "configurations.append(('Boruta + SMOTETomek', X_train_boruta_res, X_test_boruta, y_train_boruta_res))\n",
    "configurations.append(('Autoencoder', X_train_encoded, X_test_encoded, y_train))\n",
    "configurations.append(('Autoencoder + SMOTETomek', X_train_encoded_res, X_test_encoded, y_train_encoded_res))\n",
    "\n",
    "params = {\n",
    "    'n_neighbors': np.random.randint(2, 50, 3),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski'],\n",
    "    'p': np.random.randint(1, 5, 1)\n",
    "}\n",
    "\n",
    "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
    "    print(f\"\\nRunning KNN with {name} configuration...\")\n",
    "    cv = StratifiedKFold(n_splits=8, shuffle=True, random_state=42)\n",
    "    knn = RandomizedSearchCV(KNeighborsClassifier(), params, cv=cv, n_iter=50,\n",
    "                             n_jobs=-1, scoring=['accuracy', 'f1_macro'], refit='accuracy', verbose=0)\n",
    "    # knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(X_train_cfg, y_train_cfg)\n",
    "\n",
    "    y_train_knn = knn.predict(X_train_cfg)\n",
    "    y_test_knn = knn.predict(X_test_cfg)\n",
    "    y_train_knn_proba = knn.predict_proba(X_train_cfg)\n",
    "    y_test_knn_proba = knn.predict_proba(X_test_cfg)\n",
    "\n",
    "    metrics_dict = {\n",
    "          \"Dataset\": [\"Training\", \"Test\"],\n",
    "          \"Accuracy\": [\n",
    "              metrics.accuracy_score(y_train_cfg, y_train_knn),\n",
    "              metrics.accuracy_score(y_test, y_test_knn),\n",
    "          ],\n",
    "          \"F1 Score\": [\n",
    "              metrics.f1_score(y_train_cfg, y_train_knn, average='macro'),\n",
    "              metrics.f1_score(y_test, y_test_knn, average='macro'),\n",
    "          ],\n",
    "          \"Recall\": [\n",
    "              metrics.recall_score(y_train_cfg, y_train_knn, average='macro'),\n",
    "              metrics.recall_score(y_test, y_test_knn, average='macro'),\n",
    "          ],\n",
    "          \"Precision\": [\n",
    "              metrics.precision_score(y_train_cfg, y_train_knn, average='macro'),\n",
    "              metrics.precision_score(y_test, y_test_knn, average='macro'),\n",
    "          ],\n",
    "          \"AUC-ROC\": [\n",
    "              metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_knn_proba, multi_class='ovr', average='macro'),\n",
    "              metrics.roc_auc_score(pd.get_dummies(y_test), y_test_knn_proba, multi_class='ovr', average='macro'),\n",
    "          ]\n",
    "      }\n",
    "\n",
    "    df_metrics = pd.DataFrame(metrics_dict)\n",
    "    print(\"\\nKNearestNeighbors Model Performance Metrics\")\n",
    "    print(\"Configuration Name: \", name)\n",
    "    print(df_metrics.to_string(index=False))\n",
    "\n",
    "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_knn_proba, multi_class='ovr', average='macro')\n",
    "    storeResults(\n",
    "          'K-Nearest Neighbors',\n",
    "          name,\n",
    "          metrics.accuracy_score(y_test, y_test_knn),\n",
    "          metrics.f1_score(y_test, y_test_knn, average='macro'),\n",
    "          metrics.recall_score(y_test, y_test_knn, average='macro'),\n",
    "          metrics.precision_score(y_test, y_test_knn, average='macro'),\n",
    "          auc_score\n",
    "      )\n",
    "    storeBestParams(name, knn.best_params_, \"KNN\")\n",
    "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
    "    print(knn.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28854ab",
   "metadata": {
    "id": "SHcHbkJxdLvI",
    "papermill": {
     "duration": 0.014727,
     "end_time": "2025-10-16T05:03:05.562672",
     "exception": false,
     "start_time": "2025-10-16T05:03:05.547945",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e56b824",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:03:05.594157Z",
     "iopub.status.busy": "2025-10-16T05:03:05.593938Z",
     "iopub.status.idle": "2025-10-16T05:08:41.326142Z",
     "shell.execute_reply": "2025-10-16T05:08:41.325251Z"
    },
    "id": "L7NTek4OUXv5",
    "outputId": "8ff3302b-65e7-4164-8ba7-b8cc1b405e26",
    "papermill": {
     "duration": 335.75021,
     "end_time": "2025-10-16T05:08:41.327475",
     "exception": false,
     "start_time": "2025-10-16T05:03:05.577265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features to select using Boruta: 11\n",
      "\n",
      "=== Random Forest Model Performance with Hyperparameter Tuning ===\n",
      "\n",
      "Running Random Forest with Original Data configuration...\n",
      "\n",
      "Random Forest Model Performance Metrics\n",
      "Configuration Name:  Original Data\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.903010  0.881326 0.878802   0.885183 0.939791\n",
      "    Test  0.946667  0.919978 0.925758   0.921727 0.995633\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'n_estimators': 145, 'min_samples_split': 3, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.05, 'max_leaf_nodes': 30, 'max_features': 'sqrt', 'max_depth': 11, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.035500000000000004, 'bootstrap': False}\n",
      "\n",
      "Running Random Forest with Normalized Data with RobustScaler configuration...\n",
      "\n",
      "Random Forest Model Performance Metrics\n",
      "Configuration Name:  Normalized Data with RobustScaler\n",
      " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
      "Training  0.903010  0.879330 0.87533   0.884509 0.939212\n",
      "    Test  0.946667  0.913165 0.91250   0.918803 0.995633\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'n_estimators': 106, 'min_samples_split': 6, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.05, 'max_leaf_nodes': 35, 'max_features': 'sqrt', 'max_depth': 17, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.035500000000000004, 'bootstrap': False}\n",
      "\n",
      "Running Random Forest with SMOTETomek + RobustScaler configuration...\n",
      "\n",
      "Random Forest Model Performance Metrics\n",
      "Configuration Name:  SMOTETomek + RobustScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.944656  0.944773 0.944663   0.945197 0.985811\n",
      "    Test  0.960000  0.935214 0.934722   0.936975 0.997072\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'n_estimators': 118, 'min_samples_split': 5, 'min_samples_leaf': 4, 'min_impurity_decrease': 0.001, 'max_leaf_nodes': 25, 'max_features': 'sqrt', 'max_depth': 8, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.001, 'bootstrap': False}\n",
      "\n",
      "Running Random Forest with MI configuration...\n",
      "\n",
      "Random Forest Model Performance Metrics\n",
      "Configuration Name:  MI\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.892977  0.863145 0.859201   0.868244 0.937576\n",
      "    Test  0.933333  0.901219 0.906313   0.896732 0.994953\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'n_estimators': 121, 'min_samples_split': 3, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.025500000000000002, 'max_leaf_nodes': 20, 'max_features': 'sqrt', 'max_depth': 5, 'criterion': 'entropy', 'class_weight': 'balanced', 'ccp_alpha': 0.07, 'bootstrap': False}\n",
      "\n",
      "Running Random Forest with MI + SMOTETomek configuration...\n",
      "\n",
      "Random Forest Model Performance Metrics\n",
      "Configuration Name:  MI + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.928846  0.929044 0.928908   0.929568 0.981982\n",
      "    Test  0.933333  0.900738 0.906313   0.895833 0.993489\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'n_estimators': 115, 'min_samples_split': 4, 'min_samples_leaf': 5, 'min_impurity_decrease': 0.001, 'max_leaf_nodes': 25, 'max_features': 'sqrt', 'max_depth': 11, 'criterion': 'entropy', 'class_weight': 'balanced', 'ccp_alpha': 0.001, 'bootstrap': False}\n",
      "\n",
      "Running Random Forest with LDA configuration...\n",
      "\n",
      "Random Forest Model Performance Metrics\n",
      "Configuration Name:  LDA\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.892977  0.870114 0.859201   0.882923 0.924196\n",
      "    Test  0.960000  0.935214 0.934722   0.936975 0.977123\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'n_estimators': 100, 'min_samples_split': 4, 'min_samples_leaf': 5, 'min_impurity_decrease': 0.025500000000000002, 'max_leaf_nodes': 35, 'max_features': 'sqrt', 'max_depth': 14, 'criterion': 'gini', 'class_weight': 'balanced', 'ccp_alpha': 0.001, 'bootstrap': False}\n",
      "\n",
      "Running Random Forest with LDA + SMOTETomek configuration...\n",
      "\n",
      "Random Forest Model Performance Metrics\n",
      "Configuration Name:  LDA + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.912046  0.912033 0.912095   0.913168 0.973510\n",
      "    Test  0.933333  0.899482 0.904924   0.896825 0.988674\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'n_estimators': 106, 'min_samples_split': 3, 'min_samples_leaf': 4, 'min_impurity_decrease': 0.001, 'max_leaf_nodes': 20, 'max_features': 'sqrt', 'max_depth': 8, 'criterion': 'gini', 'class_weight': 'balanced', 'ccp_alpha': 0.001, 'bootstrap': False}\n",
      "\n",
      "Running Random Forest with Normalized Data with MinMaxScaler configuration...\n",
      "\n",
      "Random Forest Model Performance Metrics\n",
      "Configuration Name:  Normalized Data with MinMaxScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.903010  0.881326 0.878802   0.885183 0.940314\n",
      "    Test  0.946667  0.919978 0.925758   0.921727 0.995633\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'n_estimators': 139, 'min_samples_split': 4, 'min_samples_leaf': 4, 'min_impurity_decrease': 0.05, 'max_leaf_nodes': 25, 'max_features': 'sqrt', 'max_depth': 8, 'criterion': 'entropy', 'class_weight': 'balanced', 'ccp_alpha': 0.035500000000000004, 'bootstrap': False}\n",
      "\n",
      "Running Random Forest with SMOTETomek + MiMaxScaler configuration...\n",
      "\n",
      "Random Forest Model Performance Metrics\n",
      "Configuration Name:  SMOTETomek + MiMaxScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.944656  0.944773 0.944663   0.945197 0.985609\n",
      "    Test  0.973333  0.956583 0.955556   0.962963 0.995254\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'n_estimators': 100, 'min_samples_split': 3, 'min_samples_leaf': 3, 'min_impurity_decrease': 0.001, 'max_leaf_nodes': 25, 'max_features': 'sqrt', 'max_depth': 14, 'criterion': 'entropy', 'class_weight': 'balanced', 'ccp_alpha': 0.001, 'bootstrap': False}\n",
      "\n",
      "Running Random Forest with Boruta configuration...\n",
      "\n",
      "Random Forest Model Performance Metrics\n",
      "Configuration Name:  Boruta\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.909699  0.895558 0.899969   0.892800 0.976456\n",
      "    Test  0.946667  0.930810 0.941793   0.922222 0.997123\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'n_estimators': 106, 'min_samples_split': 5, 'min_samples_leaf': 3, 'min_impurity_decrease': 0.001, 'max_leaf_nodes': 25, 'max_features': 'sqrt', 'max_depth': 14, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.001, 'bootstrap': False}\n",
      "\n",
      "Running Random Forest with Boruta + SMOTETomek configuration...\n",
      "\n",
      "Random Forest Model Performance Metrics\n",
      "Configuration Name:  Boruta + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training   0.93499  0.935150 0.935062   0.935820 0.984351\n",
      "    Test   0.96000  0.950418 0.962626   0.940741 0.994867\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'n_estimators': 136, 'min_samples_split': 4, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.001, 'max_leaf_nodes': 25, 'max_features': 'sqrt', 'max_depth': 8, 'criterion': 'entropy', 'class_weight': 'balanced', 'ccp_alpha': 0.001, 'bootstrap': False}\n",
      "\n",
      "Running Random Forest with Autoencoder configuration...\n",
      "\n",
      "Random Forest Model Performance Metrics\n",
      "Configuration Name:  Autoencoder\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.913043  0.895726 0.891459   0.900717 0.969602\n",
      "    Test  0.960000  0.952182 0.962626   0.947368 0.992536\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'n_estimators': 103, 'min_samples_split': 3, 'min_samples_leaf': 3, 'min_impurity_decrease': 0.001, 'max_leaf_nodes': 35, 'max_features': 'sqrt', 'max_depth': 5, 'criterion': 'gini', 'class_weight': 'balanced', 'ccp_alpha': 0.001, 'bootstrap': False}\n",
      "\n",
      "Running Random Forest with Autoencoder + SMOTETomek configuration...\n",
      "\n",
      "Random Forest Model Performance Metrics\n",
      "Configuration Name:  Autoencoder + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.923372  0.923661 0.923447   0.925023 0.985622\n",
      "    Test  0.933333  0.916266 0.934217   0.901961 0.991803\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'n_estimators': 103, 'min_samples_split': 3, 'min_samples_leaf': 5, 'min_impurity_decrease': 0.001, 'max_leaf_nodes': 20, 'max_features': 'sqrt', 'max_depth': 14, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.001, 'bootstrap': False}\n"
     ]
    }
   ],
   "source": [
    "configurations = []\n",
    "\n",
    "configurations.append(('Original Data', X_train, X_test, y_train))\n",
    "configurations.append(('Normalized Data with RobustScaler', X_train_robust, X_test_robust, y_train))\n",
    "configurations.append(('SMOTETomek + RobustScaler', X_train_robust_resample, X_test_robust, y_train_robust_resample))\n",
    "configurations.append(('MI', X_train_mi, X_test_mi, y_train))\n",
    "configurations.append(('MI + SMOTETomek', X_train_mi_res, X_test_mi, y_train_mi_res))\n",
    "configurations.append(('LDA', X_train_lda, X_test_lda, y_train))\n",
    "configurations.append(('LDA + SMOTETomek', X_train_lda_res, X_test_lda, y_train_lda_res))\n",
    "configurations.append(('Normalized Data with MinMaxScaler', X_train_minmax, X_test_minmax, y_train))\n",
    "configurations.append(('SMOTETomek + MiMaxScaler', X_train_minmax_resample, X_test_minmax, y_train_minmax_resample))\n",
    "configurations.append(('Boruta', X_train_boruta, X_test_boruta, y_train))\n",
    "configurations.append(('Boruta + SMOTETomek', X_train_boruta_res, X_test_boruta, y_train_boruta_res))\n",
    "configurations.append(('Autoencoder', X_train_encoded, X_test_encoded, y_train))\n",
    "configurations.append(('Autoencoder + SMOTETomek', X_train_encoded_res, X_test_encoded, y_train_encoded_res))\n",
    "\n",
    "selected_features = boruta_selector.support_\n",
    "optimal_features = sum(selected_features)\n",
    "print(f\"Optimal number of features to select using Boruta: {optimal_features}\")\n",
    "\n",
    "# Step 4: Random Forest + GridSearchCV\n",
    "print(\"\\n=== Random Forest Model Performance with Hyperparameter Tuning ===\")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': range(100, 160, 3),\n",
    "    'max_depth': range(2, 20, 3),\n",
    "    'min_samples_split': range(3, 7, 1),\n",
    "    'min_samples_leaf': range(3, 7, 1),\n",
    "    'max_features': ['sqrt'],\n",
    "    'bootstrap': [False],\n",
    "    'class_weight': ['balanced'],\n",
    "    'max_leaf_nodes': range(20, 40, 5),\n",
    "    'min_impurity_decrease': np.linspace(0.001, 0.05, 3),\n",
    "    'ccp_alpha': np.linspace(0.001, 0.07, 3),\n",
    "    'criterion': ['gini', 'entropy', 'log_loss']\n",
    "}\n",
    "\n",
    "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
    "    print(f\"\\nRunning Random Forest with {name} configuration...\")\n",
    "    cv = StratifiedKFold(n_splits=8, shuffle=True, random_state=42)\n",
    "    rf = RandomizedSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=cv, n_jobs=-1,\n",
    "                            n_iter=50, scoring=['accuracy', 'f1_macro'], refit='accuracy', verbose=0)\n",
    "    # rf = RandomForestClassifier(random_state=42)\n",
    "    rf.fit(X_train_cfg, y_train_cfg)\n",
    "\n",
    "    y_train_rf = rf.predict(X_train_cfg)\n",
    "    y_test_rf = rf.predict(X_test_cfg)\n",
    "    y_train_rf_proba = rf.predict_proba(X_train_cfg)\n",
    "    y_test_rf_proba = rf.predict_proba(X_test_cfg)\n",
    "\n",
    "    metrics_dict = {\n",
    "          \"Dataset\": [\"Training\", \"Test\"],\n",
    "          \"Accuracy\": [\n",
    "              metrics.accuracy_score(y_train_cfg, y_train_rf),\n",
    "              metrics.accuracy_score(y_test, y_test_rf),\n",
    "          ],\n",
    "          \"F1 Score\": [\n",
    "              metrics.f1_score(y_train_cfg, y_train_rf, average='macro'),\n",
    "              metrics.f1_score(y_test, y_test_rf, average='macro'),\n",
    "          ],\n",
    "          \"Recall\": [\n",
    "              metrics.recall_score(y_train_cfg, y_train_rf, average='macro'),\n",
    "              metrics.recall_score(y_test, y_test_rf, average='macro'),\n",
    "          ],\n",
    "          \"Precision\": [\n",
    "              metrics.precision_score(y_train_cfg, y_train_rf, average='macro'),\n",
    "              metrics.precision_score(y_test, y_test_rf, average='macro'),\n",
    "          ],\n",
    "          \"AUC-ROC\": [\n",
    "              metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_rf_proba, multi_class='ovr', average='macro'),\n",
    "              metrics.roc_auc_score(pd.get_dummies(y_test), y_test_rf_proba, multi_class='ovr', average='macro'),\n",
    "          ]\n",
    "      }\n",
    "\n",
    "    df_metrics = pd.DataFrame(metrics_dict)\n",
    "    print(\"\\nRandom Forest Model Performance Metrics\")\n",
    "    print(\"Configuration Name: \", name)\n",
    "    print(df_metrics.to_string(index=False))\n",
    "\n",
    "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_rf_proba, multi_class='ovr', average='macro')\n",
    "    storeResults(\n",
    "          'Random Forest',\n",
    "          name,\n",
    "          metrics.accuracy_score(y_test, y_test_rf),\n",
    "          metrics.f1_score(y_test, y_test_rf, average='macro'),\n",
    "          metrics.recall_score(y_test, y_test_rf, average='macro'),\n",
    "          metrics.precision_score(y_test, y_test_rf, average='macro'),\n",
    "          auc_score\n",
    "      )\n",
    "    storeBestParams(name, rf.best_params_,  \"Random forest\")\n",
    "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
    "    print(rf.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5730a44d",
   "metadata": {
    "id": "X_F-w3vDsLHX",
    "papermill": {
     "duration": 0.015187,
     "end_time": "2025-10-16T05:08:41.358922",
     "exception": false,
     "start_time": "2025-10-16T05:08:41.343735",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# XGBoost\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10fdb40f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:08:41.391210Z",
     "iopub.status.busy": "2025-10-16T05:08:41.390971Z",
     "iopub.status.idle": "2025-10-16T05:13:35.743607Z",
     "shell.execute_reply": "2025-10-16T05:13:35.742791Z"
    },
    "id": "4gFupPmNsedu",
    "outputId": "7ab26068-9511-4605-8a5a-832cec824c90",
    "papermill": {
     "duration": 294.386415,
     "end_time": "2025-10-16T05:13:35.761103",
     "exception": false,
     "start_time": "2025-10-16T05:08:41.374688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features to select using Boruta: 11\n",
      "\n",
      "=== XGBoost Model Performance with Hyperparameter Tuning ===\n",
      "\n",
      "Running XGBoost with Original Data configuration...\n",
      "\n",
      "XGBoost Model Performance Metrics\n",
      "Configuration Name:  Original Data\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.916388  0.901116 0.896836   0.906622 0.984194\n",
      "    Test  0.973333  0.956944 0.956944   0.956944 0.993471\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 0.8, 'reg_lambda': 0.1, 'reg_alpha': 0.1, 'n_estimators': 260, 'min_child_weight': 1, 'max_depth': 32, 'max_delta_step': 5, 'learning_rate': 0.033400000000000006, 'gamma': 0.1, 'colsample_bytree': 0.3, 'colsample_bynode': 0.8, 'colsample_bylevel': 1.0, 'booster': 'gbtree'}\n",
      "\n",
      "Running XGBoost with Normalized Data with RobustScaler configuration...\n",
      "\n",
      "XGBoost Model Performance Metrics\n",
      "Configuration Name:  Normalized Data with RobustScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.916388  0.901116 0.896836   0.906622 0.974428\n",
      "    Test  0.973333  0.956944 0.956944   0.956944 0.997830\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 0.5, 'reg_lambda': 0.1, 'reg_alpha': 0.1, 'n_estimators': 200, 'min_child_weight': 3, 'max_depth': 22, 'max_delta_step': 0, 'learning_rate': 0.0889, 'gamma': 0.1, 'colsample_bytree': 0.3, 'colsample_bynode': 0.8, 'colsample_bylevel': 1.0, 'booster': 'gbtree'}\n",
      "\n",
      "Running XGBoost with SMOTETomek + RobustScaler configuration...\n",
      "\n",
      "XGBoost Model Performance Metrics\n",
      "Configuration Name:  SMOTETomek + RobustScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.950382  0.950447 0.950411   0.950778 0.987095\n",
      "    Test  0.960000  0.935214 0.934722   0.936975 0.993118\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 1.0, 'reg_lambda': 0.1, 'reg_alpha': 0.1, 'n_estimators': 280, 'min_child_weight': 7, 'max_depth': 32, 'max_delta_step': 5, 'learning_rate': 0.033400000000000006, 'gamma': 0.0, 'colsample_bytree': 0.8, 'colsample_bynode': 0.8, 'colsample_bylevel': 1.0, 'booster': 'gbtree'}\n",
      "\n",
      "Running XGBoost with MI configuration...\n",
      "\n",
      "XGBoost Model Performance Metrics\n",
      "Configuration Name:  MI\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.899666  0.874065 0.869954   0.878567 0.937677\n",
      "    Test  0.946667  0.913889 0.913889   0.913889 0.993471\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 0.4, 'reg_lambda': 0.1, 'reg_alpha': 0.1, 'n_estimators': 120, 'min_child_weight': 9, 'max_depth': 2, 'max_delta_step': 5, 'learning_rate': 0.044500000000000005, 'gamma': 0.0, 'colsample_bytree': 0.8, 'colsample_bynode': 0.8, 'colsample_bylevel': 1.0, 'booster': 'gbtree'}\n",
      "\n",
      "Running XGBoost with MI + SMOTETomek configuration...\n",
      "\n",
      "XGBoost Model Performance Metrics\n",
      "Configuration Name:  MI + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.932692  0.932775 0.932717   0.932963 0.974982\n",
      "    Test  0.933333  0.900738 0.906313   0.895833 0.988743\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 1.0, 'reg_lambda': 0.1, 'reg_alpha': 0.1, 'n_estimators': 320, 'min_child_weight': 9, 'max_depth': 2, 'max_delta_step': 0, 'learning_rate': 0.044500000000000005, 'gamma': 0.0, 'colsample_bytree': 0.8, 'colsample_bynode': 0.8, 'colsample_bylevel': 1.0, 'booster': 'gbtree'}\n",
      "\n",
      "Running XGBoost with LDA configuration...\n",
      "\n",
      "XGBoost Model Performance Metrics\n",
      "Configuration Name:  LDA\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.896321  0.875590 0.864578   0.888955 0.952942\n",
      "    Test  0.946667  0.913165 0.912500   0.918803 0.993385\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 0.7000000000000001, 'reg_lambda': 0.1, 'reg_alpha': 0.1, 'n_estimators': 260, 'min_child_weight': 8, 'max_depth': 22, 'max_delta_step': 0, 'learning_rate': 0.033400000000000006, 'gamma': 0.0, 'colsample_bytree': 0.3, 'colsample_bynode': 0.8, 'colsample_bylevel': 1.0, 'booster': 'gbtree'}\n",
      "\n",
      "Running XGBoost with LDA + SMOTETomek configuration...\n",
      "\n",
      "XGBoost Model Performance Metrics\n",
      "Configuration Name:  LDA + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.913958  0.913900 0.913999   0.915604 0.976025\n",
      "    Test  0.946667  0.913165 0.912500   0.918803 0.991955\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 0.6, 'reg_lambda': 0.1, 'reg_alpha': 0.1, 'n_estimators': 250, 'min_child_weight': 7, 'max_depth': 42, 'max_delta_step': 5, 'learning_rate': 0.0889, 'gamma': 0.0, 'colsample_bytree': 0.8, 'colsample_bynode': 0.6, 'colsample_bylevel': 1.0, 'booster': 'gbtree'}\n",
      "\n",
      "Running XGBoost with Normalized Data with MinMaxScaler configuration...\n",
      "\n",
      "XGBoost Model Performance Metrics\n",
      "Configuration Name:  Normalized Data with MinMaxScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.916388  0.901116 0.896836   0.906622 0.980467\n",
      "    Test  0.973333  0.956944 0.956944   0.956944 0.997141\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 0.5, 'reg_lambda': 0.1, 'reg_alpha': 0.1, 'n_estimators': 290, 'min_child_weight': 2, 'max_depth': 12, 'max_delta_step': 5, 'learning_rate': 0.0889, 'gamma': 0.1, 'colsample_bytree': 0.3, 'colsample_bynode': 0.6, 'colsample_bylevel': 1.0, 'booster': 'gbtree'}\n",
      "\n",
      "Running XGBoost with SMOTETomek + MiMaxScaler configuration...\n",
      "\n",
      "XGBoost Model Performance Metrics\n",
      "Configuration Name:  SMOTETomek + MiMaxScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.950382  0.950447 0.950411   0.950778 0.991158\n",
      "    Test  0.973333  0.956944 0.956944   0.956944 0.993067\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 0.6, 'reg_lambda': 0.1, 'reg_alpha': 0.1, 'n_estimators': 360, 'min_child_weight': 4, 'max_depth': 22, 'max_delta_step': 5, 'learning_rate': 0.06670000000000001, 'gamma': 0.1, 'colsample_bytree': 0.8, 'colsample_bynode': 0.8, 'colsample_bylevel': 1.0, 'booster': 'gbtree'}\n",
      "\n",
      "Running XGBoost with Boruta configuration...\n",
      "\n",
      "XGBoost Model Performance Metrics\n",
      "Configuration Name:  Boruta\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.906355  0.884919 0.880707   0.889368 0.958314\n",
      "    Test  0.960000  0.935214 0.934722   0.936975 0.997123\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 0.4, 'reg_lambda': 0.1, 'reg_alpha': 0.1, 'n_estimators': 120, 'min_child_weight': 2, 'max_depth': 12, 'max_delta_step': 5, 'learning_rate': 0.06670000000000001, 'gamma': 0.0, 'colsample_bytree': 0.3, 'colsample_bynode': 0.6, 'colsample_bylevel': 1.0, 'booster': 'gbtree'}\n",
      "\n",
      "Running XGBoost with Boruta + SMOTETomek configuration...\n",
      "\n",
      "XGBoost Model Performance Metrics\n",
      "Configuration Name:  Boruta + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.950287  0.950371 0.950344   0.950691 0.991087\n",
      "    Test  0.973333  0.956944 0.956944   0.956944 0.992326\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 0.8, 'reg_lambda': 0.1, 'reg_alpha': 0.1, 'n_estimators': 150, 'min_child_weight': 3, 'max_depth': 22, 'max_delta_step': 5, 'learning_rate': 0.0889, 'gamma': 0.1, 'colsample_bytree': 0.8, 'colsample_bynode': 0.8, 'colsample_bylevel': 1.0, 'booster': 'gbtree'}\n",
      "\n",
      "Running XGBoost with Autoencoder configuration...\n",
      "\n",
      "XGBoost Model Performance Metrics\n",
      "Configuration Name:  Autoencoder\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.916388  0.899151 0.893364   0.906086 0.981431\n",
      "    Test  0.933333  0.922849 0.920960   0.926025 0.992914\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 0.9, 'reg_lambda': 0.1, 'reg_alpha': 0.1, 'n_estimators': 50, 'min_child_weight': 1, 'max_depth': 32, 'max_delta_step': 5, 'learning_rate': 0.0223, 'gamma': 0.05, 'colsample_bytree': 0.3, 'colsample_bynode': 0.8, 'colsample_bylevel': 1.0, 'booster': 'gbtree'}\n",
      "\n",
      "Running XGBoost with Autoencoder + SMOTETomek configuration...\n",
      "\n",
      "XGBoost Model Performance Metrics\n",
      "Configuration Name:  Autoencoder + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.952107   0.95219 0.952139   0.952500 0.993746\n",
      "    Test  0.946667   0.93081 0.941793   0.922222 0.986185\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 0.5, 'reg_lambda': 0.1, 'reg_alpha': 0.1, 'n_estimators': 490, 'min_child_weight': 3, 'max_depth': 22, 'max_delta_step': 0, 'learning_rate': 0.0889, 'gamma': 0.1, 'colsample_bytree': 0.3, 'colsample_bynode': 0.8, 'colsample_bylevel': 1.0, 'booster': 'gbtree'}\n"
     ]
    }
   ],
   "source": [
    "configurations = []\n",
    "\n",
    "configurations.append(('Original Data', X_train, X_test, y_train))\n",
    "configurations.append(('Normalized Data with RobustScaler', X_train_robust, X_test_robust, y_train))\n",
    "configurations.append(('SMOTETomek + RobustScaler', X_train_robust_resample, X_test_robust, y_train_robust_resample))\n",
    "configurations.append(('MI', X_train_mi, X_test_mi, y_train))\n",
    "configurations.append(('MI + SMOTETomek', X_train_mi_res, X_test_mi, y_train_mi_res))\n",
    "configurations.append(('LDA', X_train_lda, X_test_lda, y_train))\n",
    "configurations.append(('LDA + SMOTETomek', X_train_lda_res, X_test_lda, y_train_lda_res))\n",
    "configurations.append(('Normalized Data with MinMaxScaler', X_train_minmax, X_test_minmax, y_train))\n",
    "configurations.append(('SMOTETomek + MiMaxScaler', X_train_minmax_resample, X_test_minmax, y_train_minmax_resample))\n",
    "configurations.append(('Boruta', X_train_boruta, X_test_boruta, y_train))\n",
    "configurations.append(('Boruta + SMOTETomek', X_train_boruta_res, X_test_boruta, y_train_boruta_res))\n",
    "configurations.append(('Autoencoder', X_train_encoded, X_test_encoded, y_train))\n",
    "configurations.append(('Autoencoder + SMOTETomek', X_train_encoded_res, X_test_encoded, y_train_encoded_res))\n",
    "\n",
    "selected_features = boruta_selector.support_\n",
    "optimal_features = sum(selected_features)\n",
    "print(f\"Optimal number of features to select using Boruta: {optimal_features}\")\n",
    "\n",
    "# Step 4: XGBoost + GridSearchCV\n",
    "print(\"\\n=== XGBoost Model Performance with Hyperparameter Tuning ===\")\n",
    "\n",
    "param_grid = {\n",
    "    'booster': ['gbtree'],  # Specify booster type\n",
    "    'learning_rate': np.linspace(0.0001, 0.1, 10),  # Learning rate tuning\n",
    "    'n_estimators': range(50, 500, 10),  # Number of estimators (trees)\n",
    "    'max_depth': range(2, 50, 10),  # Depth of trees\n",
    "    'min_child_weight': range(1, 10, 1),  # Minimum weight of children\n",
    "    'gamma': np.linspace(0, 0.1, 3),  # Gamma (for pruning)\n",
    "    'subsample': np.linspace(0.1, 1, 10),  # Subsample ratio\n",
    "    'colsample_bytree': [0.3, 0.8],  # Subsample ratio for tree\n",
    "    'colsample_bylevel': [1.0],  # Subsample ratio for level\n",
    "    'colsample_bynode': [0.6, 0.8],  # Subsample ratio for node\n",
    "    'max_delta_step': [0, 5],  # Maximum delta step for optimization\n",
    "    \n",
    "    'reg_alpha': np.linspace(0.1, 1, 1),  # L1 regularization\n",
    "    'reg_lambda': np.linspace(0.1, 1, 1) # L2 regularization\n",
    "   # 'scale_pos_weight': [1, 2, 5],  # Handling class imbalance\n",
    "\n",
    "    # Remove booster-specific parameters if using gbtree\n",
    "    # 'sample_type': [\"weighted\"], \n",
    "    # 'normalize_type': [\"tree\", \"forest\"],\n",
    "    # 'rate_drop': [0, 0.1],\n",
    "    # 'skip_drop': [0, 0.1]\n",
    "}\n",
    "\n",
    "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
    "    print(f\"\\nRunning XGBoost with {name} configuration...\")\n",
    "    cv = StratifiedKFold(n_splits=8, shuffle=True, random_state=42)\n",
    "    xgb = RandomizedSearchCV(XGBClassifier(), param_grid, n_iter=50, cv=cv,\n",
    "                             n_jobs=-1, scoring=['accuracy', 'f1_macro'], refit='accuracy', verbose=0)\n",
    "    # xgb = XGBClassifier()\n",
    "    xgb.fit(X_train_cfg, y_train_cfg)\n",
    "\n",
    "    y_train_xg = xgb.predict(X_train_cfg)\n",
    "    y_test_xg = xgb.predict(X_test_cfg)\n",
    "    y_train_xg_proba = xgb.predict_proba(X_train_cfg)\n",
    "    y_test_xg_proba = xgb.predict_proba(X_test_cfg)\n",
    "\n",
    "    metrics_dict = {\n",
    "          \"Dataset\": [\"Training\", \"Test\"],\n",
    "          \"Accuracy\": [\n",
    "              metrics.accuracy_score(y_train_cfg, y_train_xg),\n",
    "              metrics.accuracy_score(y_test, y_test_xg),\n",
    "          ],\n",
    "          \"F1 Score\": [\n",
    "              metrics.f1_score(y_train_cfg, y_train_xg, average='macro'),\n",
    "              metrics.f1_score(y_test, y_test_xg, average='macro'),\n",
    "          ],\n",
    "          \"Recall\": [\n",
    "              metrics.recall_score(y_train_cfg, y_train_xg, average='macro'),\n",
    "              metrics.recall_score(y_test, y_test_xg, average='macro'),\n",
    "          ],\n",
    "          \"Precision\": [\n",
    "              metrics.precision_score(y_train_cfg, y_train_xg, average='macro'),\n",
    "              metrics.precision_score(y_test, y_test_xg, average='macro'),\n",
    "          ],\n",
    "          \"AUC-ROC\": [\n",
    "              metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_xg_proba, multi_class='ovr', average='macro'),\n",
    "              metrics.roc_auc_score(pd.get_dummies(y_test), y_test_xg_proba, multi_class='ovr', average='macro'),\n",
    "          ]\n",
    "      }\n",
    "\n",
    "    df_metrics = pd.DataFrame(metrics_dict)\n",
    "    print(\"\\nXGBoost Model Performance Metrics\")\n",
    "    print(\"Configuration Name: \", name)\n",
    "    print(df_metrics.to_string(index=False))\n",
    "\n",
    "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_xg_proba, multi_class='ovr', average='macro')\n",
    "    storeResults(\n",
    "        'XGBoost Model',\n",
    "        name,\n",
    "        metrics.accuracy_score(y_test, y_test_xg),\n",
    "        metrics.f1_score(y_test, y_test_xg, average='macro'),\n",
    "        metrics.recall_score(y_test, y_test_xg, average='macro'),\n",
    "        metrics.precision_score(y_test, y_test_xg, average='macro'),\n",
    "        auc_score\n",
    "    )\n",
    "    storeBestParams(name, xgb.best_params_, \"xgboost\")\n",
    "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
    "    print(xgb.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7064fb14",
   "metadata": {
    "papermill": {
     "duration": 0.016135,
     "end_time": "2025-10-16T05:13:35.793519",
     "exception": false,
     "start_time": "2025-10-16T05:13:35.777384",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# xgboost 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94f2a459",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:13:35.827422Z",
     "iopub.status.busy": "2025-10-16T05:13:35.826701Z",
     "iopub.status.idle": "2025-10-16T09:07:28.027848Z",
     "shell.execute_reply": "2025-10-16T09:07:28.027010Z"
    },
    "papermill": {
     "duration": 14032.237251,
     "end_time": "2025-10-16T09:07:28.046899",
     "exception": false,
     "start_time": "2025-10-16T05:13:35.809648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features to select using Boruta: 11\n",
      "\n",
      "=== XGBoost2 Model Performance with Hyperparameter Tuning ===\n",
      "\n",
      "Running XGBoost2 with Original Data configuration...\n",
      "\n",
      "XGBoost2 Model Performance Metrics\n",
      "Configuration Name:  Original Data\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.916388  0.901116 0.896836   0.906622 0.978982\n",
      "    Test  0.960000  0.935214 0.934722   0.936975 0.996400\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 0.6, 'n_estimators': 350, 'min_child_weight': 3, 'max_depth': 22, 'learning_rate': 0.033400000000000006, 'gamma': 0, 'colsample_bytree': 1.0, 'booster': 'gbtree'}\n",
      "\n",
      "Running XGBoost2 with Normalized Data with RobustScaler configuration...\n",
      "\n",
      "XGBoost2 Model Performance Metrics\n",
      "Configuration Name:  Normalized Data with RobustScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.916388  0.901116 0.896836   0.906622 0.980616\n",
      "    Test  0.973333  0.956944 0.956944   0.956944 0.996383\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 0.6, 'n_estimators': 450, 'min_child_weight': 1, 'max_depth': 32, 'learning_rate': 0.0112, 'gamma': 0.1, 'colsample_bytree': 0.6, 'booster': 'gbtree'}\n",
      "\n",
      "Running XGBoost2 with SMOTETomek + RobustScaler configuration...\n",
      "\n",
      "XGBoost2 Model Performance Metrics\n",
      "Configuration Name:  SMOTETomek + RobustScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.950382  0.950447 0.950411   0.950778 0.990530\n",
      "    Test  0.960000  0.935214 0.934722   0.936975 0.994229\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 0.6, 'skip_drop': 0.5, 'rate_drop': 0.5, 'n_estimators': 250, 'min_child_weight': 3, 'max_depth': 22, 'learning_rate': 0.0889, 'gamma': 0, 'colsample_bytree': 0.8, 'booster': 'dart'}\n",
      "\n",
      "Running XGBoost2 with MI configuration...\n",
      "\n",
      "XGBoost2 Model Performance Metrics\n",
      "Configuration Name:  MI\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.899666  0.874035 0.869954   0.878348 0.951255\n",
      "    Test  0.933333  0.892024 0.891667   0.893557 0.994212\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 0.8, 'skip_drop': 0.3, 'rate_drop': 0.5, 'n_estimators': 300, 'min_child_weight': 3, 'max_depth': 12, 'learning_rate': 0.0223, 'gamma': 0.5, 'colsample_bytree': 0.8, 'booster': 'dart'}\n",
      "\n",
      "Running XGBoost2 with MI + SMOTETomek configuration...\n",
      "\n",
      "XGBoost2 Model Performance Metrics\n",
      "Configuration Name:  MI + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.948077  0.948196 0.948179   0.948432 0.984822\n",
      "    Test  0.986667  0.978495 0.979167   0.979167 0.994212\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 0.8, 'skip_drop': 0.5, 'rate_drop': 0.1, 'n_estimators': 200, 'min_child_weight': 3, 'max_depth': 42, 'learning_rate': 0.1, 'gamma': 0.1, 'colsample_bytree': 0.6, 'booster': 'dart'}\n",
      "\n",
      "Running XGBoost2 with LDA configuration...\n",
      "\n",
      "XGBoost2 Model Performance Metrics\n",
      "Configuration Name:  LDA\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.899666  0.876778 0.869954   0.884381 0.957215\n",
      "    Test  0.946667  0.913165 0.912500   0.918803 0.994496\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 0.6, 'skip_drop': 0.3, 'rate_drop': 0.1, 'n_estimators': 450, 'min_child_weight': 3, 'max_depth': 22, 'learning_rate': 0.0889, 'gamma': 0.5, 'colsample_bytree': 0.8, 'booster': 'dart'}\n",
      "\n",
      "Running XGBoost2 with LDA + SMOTETomek configuration...\n",
      "\n",
      "XGBoost2 Model Performance Metrics\n",
      "Configuration Name:  LDA + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.906310  0.906428 0.906380   0.908255 0.970291\n",
      "    Test  0.946667  0.921421 0.927146   0.916340 0.993506\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 1.0, 'skip_drop': 0.5, 'rate_drop': 0.5, 'n_estimators': 300, 'min_child_weight': 1, 'max_depth': 12, 'learning_rate': 0.055600000000000004, 'gamma': 1, 'colsample_bytree': 1.0, 'booster': 'dart'}\n",
      "\n",
      "Running XGBoost2 with Normalized Data with MinMaxScaler configuration...\n",
      "\n",
      "XGBoost2 Model Performance Metrics\n",
      "Configuration Name:  Normalized Data with MinMaxScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.916388  0.901213 0.896836   0.905995 0.980680\n",
      "    Test  0.973333  0.956944 0.956944   0.956944 0.997123\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 0.6, 'skip_drop': 0.3, 'rate_drop': 0.3, 'n_estimators': 300, 'min_child_weight': 1, 'max_depth': 12, 'learning_rate': 0.044500000000000005, 'gamma': 0.1, 'colsample_bytree': 0.6, 'booster': 'dart'}\n",
      "\n",
      "Running XGBoost2 with SMOTETomek + MiMaxScaler configuration...\n",
      "\n",
      "XGBoost2 Model Performance Metrics\n",
      "Configuration Name:  SMOTETomek + MiMaxScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.950382  0.950447 0.950411   0.950778 0.989847\n",
      "    Test  0.960000  0.935214 0.934722   0.936975 0.992696\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 1.0, 'skip_drop': 0.3, 'rate_drop': 0.1, 'n_estimators': 300, 'min_child_weight': 3, 'max_depth': 22, 'learning_rate': 0.07780000000000001, 'gamma': 0.1, 'colsample_bytree': 0.6, 'booster': 'dart'}\n",
      "\n",
      "Running XGBoost2 with Boruta configuration...\n",
      "\n",
      "XGBoost2 Model Performance Metrics\n",
      "Configuration Name:  Boruta\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.916388  0.901116 0.896836   0.906622 0.981029\n",
      "    Test  0.933333  0.892473 0.893056   0.893056 0.991758\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 1.0, 'n_estimators': 400, 'min_child_weight': 1, 'max_depth': 2, 'learning_rate': 0.055600000000000004, 'gamma': 0, 'colsample_bytree': 0.8, 'booster': 'gbtree'}\n",
      "\n",
      "Running XGBoost2 with Boruta + SMOTETomek configuration...\n",
      "\n",
      "XGBoost2 Model Performance Metrics\n",
      "Configuration Name:  Boruta + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.950287  0.950371 0.950344   0.950691 0.990774\n",
      "    Test  0.946667  0.925926 0.915278   0.939476 0.991955\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 0.8, 'n_estimators': 300, 'min_child_weight': 3, 'max_depth': 22, 'learning_rate': 0.055600000000000004, 'gamma': 0.1, 'colsample_bytree': 0.6, 'booster': 'gbtree'}\n",
      "\n",
      "Running XGBoost2 with Autoencoder configuration...\n",
      "\n",
      "XGBoost2 Model Performance Metrics\n",
      "Configuration Name:  Autoencoder\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.913043  0.895726 0.891459   0.900717 0.978815\n",
      "    Test  0.933333  0.922849 0.920960   0.926025 0.991837\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 0.6, 'n_estimators': 450, 'min_child_weight': 1, 'max_depth': 22, 'learning_rate': 0.033400000000000006, 'gamma': 1, 'colsample_bytree': 0.8, 'booster': 'gbtree'}\n",
      "\n",
      "Running XGBoost2 with Autoencoder + SMOTETomek configuration...\n",
      "\n",
      "XGBoost2 Model Performance Metrics\n",
      "Configuration Name:  Autoencoder + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.952107  0.952190 0.952139   0.952500 0.994077\n",
      "    Test  0.960000  0.943643 0.949369   0.938562 0.986867\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 0.6, 'n_estimators': 250, 'min_child_weight': 3, 'max_depth': 12, 'learning_rate': 0.0889, 'gamma': 0, 'colsample_bytree': 1.0, 'booster': 'gbtree'}\n"
     ]
    }
   ],
   "source": [
    "configurations = []\n",
    "\n",
    "configurations.append(('Original Data', X_train, X_test, y_train))\n",
    "configurations.append(('Normalized Data with RobustScaler', X_train_robust, X_test_robust, y_train))\n",
    "configurations.append(('SMOTETomek + RobustScaler', X_train_robust_resample, X_test_robust, y_train_robust_resample))\n",
    "configurations.append(('MI', X_train_mi, X_test_mi, y_train))\n",
    "configurations.append(('MI + SMOTETomek', X_train_mi_res, X_test_mi, y_train_mi_res))\n",
    "configurations.append(('LDA', X_train_lda, X_test_lda, y_train))\n",
    "configurations.append(('LDA + SMOTETomek', X_train_lda_res, X_test_lda, y_train_lda_res))\n",
    "configurations.append(('Normalized Data with MinMaxScaler', X_train_minmax, X_test_minmax, y_train))\n",
    "configurations.append(('SMOTETomek + MiMaxScaler', X_train_minmax_resample, X_test_minmax, y_train_minmax_resample))\n",
    "configurations.append(('Boruta', X_train_boruta, X_test_boruta, y_train))\n",
    "configurations.append(('Boruta + SMOTETomek', X_train_boruta_res, X_test_boruta, y_train_boruta_res))\n",
    "configurations.append(('Autoencoder', X_train_encoded, X_test_encoded, y_train))\n",
    "configurations.append(('Autoencoder + SMOTETomek', X_train_encoded_res, X_test_encoded, y_train_encoded_res))\n",
    "\n",
    "selected_features = boruta_selector.support_\n",
    "optimal_features = sum(selected_features)\n",
    "print(f\"Optimal number of features to select using Boruta: {optimal_features}\")\n",
    "\n",
    "# Step 4: XGBoost + GridSearchCV\n",
    "print(\"\\n=== XGBoost2 Model Performance with Hyperparameter Tuning ===\")\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'booster': ['gbtree'],\n",
    "        'learning_rate': np.linspace(0.0001, 0.1, 10),\n",
    "        'n_estimators': range(50, 500, 50),  # Reduced step size for faster search\n",
    "        'max_depth': range(2, 50, 10),\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'gamma': [0, 0.1, 0.5, 1],\n",
    "        'min_child_weight': [1, 3, 5]\n",
    "    },\n",
    "    {\n",
    "        'booster': ['dart'],\n",
    "        'learning_rate': np.linspace(0.0001, 0.1, 10),\n",
    "        'n_estimators': range(50, 500, 50),\n",
    "        'max_depth': range(2, 50, 10),\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'gamma': [0, 0.1, 0.5, 1],\n",
    "        'min_child_weight': [1, 3, 5],\n",
    "        'rate_drop': [0.1, 0.3, 0.5],\n",
    "        'skip_drop': [0.1, 0.3, 0.5]\n",
    "    },\n",
    "    {\n",
    "        'booster': ['gblinear'],\n",
    "        'learning_rate': np.linspace(0.0001, 0.1, 10),\n",
    "        'n_estimators': range(50, 500, 50),\n",
    "        'reg_alpha': [0, 0.01, 0.1, 1],\n",
    "        'reg_lambda': [0, 0.01, 0.1, 1]\n",
    "        # No max_depth for gblinear (it's a linear model)\n",
    "    }\n",
    "]\n",
    "\n",
    "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
    "    print(f\"\\nRunning XGBoost2 with {name} configuration...\")\n",
    "    cv = StratifiedKFold(n_splits=8, shuffle=True, random_state=42)\n",
    "    xgb = RandomizedSearchCV(XGBClassifier(), param_grid, n_iter=50, cv=cv,\n",
    "                             n_jobs=-1, scoring=['accuracy', 'f1_macro'], refit='accuracy', verbose=0)\n",
    "    # xgb = XGBClassifier()\n",
    "    xgb.fit(X_train_cfg, y_train_cfg)\n",
    "\n",
    "    y_train_xg = xgb.predict(X_train_cfg)\n",
    "    y_test_xg = xgb.predict(X_test_cfg)\n",
    "    y_train_xg_proba = xgb.predict_proba(X_train_cfg)\n",
    "    y_test_xg_proba = xgb.predict_proba(X_test_cfg)\n",
    "\n",
    "    metrics_dict = {\n",
    "          \"Dataset\": [\"Training\", \"Test\"],\n",
    "          \"Accuracy\": [\n",
    "              metrics.accuracy_score(y_train_cfg, y_train_xg),\n",
    "              metrics.accuracy_score(y_test, y_test_xg),\n",
    "          ],\n",
    "          \"F1 Score\": [\n",
    "              metrics.f1_score(y_train_cfg, y_train_xg, average='macro'),\n",
    "              metrics.f1_score(y_test, y_test_xg, average='macro'),\n",
    "          ],\n",
    "          \"Recall\": [\n",
    "              metrics.recall_score(y_train_cfg, y_train_xg, average='macro'),\n",
    "              metrics.recall_score(y_test, y_test_xg, average='macro'),\n",
    "          ],\n",
    "          \"Precision\": [\n",
    "              metrics.precision_score(y_train_cfg, y_train_xg, average='macro'),\n",
    "              metrics.precision_score(y_test, y_test_xg, average='macro'),\n",
    "          ],\n",
    "          \"AUC-ROC\": [\n",
    "              metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_xg_proba, multi_class='ovr', average='macro'),\n",
    "              metrics.roc_auc_score(pd.get_dummies(y_test), y_test_xg_proba, multi_class='ovr', average='macro'),\n",
    "          ]\n",
    "      }\n",
    "\n",
    "    df_metrics = pd.DataFrame(metrics_dict)\n",
    "    print(\"\\nXGBoost2 Model Performance Metrics\")\n",
    "    print(\"Configuration Name: \", name)\n",
    "    print(df_metrics.to_string(index=False))\n",
    "\n",
    "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_xg_proba, multi_class='ovr', average='macro')\n",
    "    storeResults(\n",
    "        'XGBoost2 Model',\n",
    "        name,\n",
    "        metrics.accuracy_score(y_test, y_test_xg),\n",
    "        metrics.f1_score(y_test, y_test_xg, average='macro'),\n",
    "        metrics.recall_score(y_test, y_test_xg, average='macro'),\n",
    "        metrics.precision_score(y_test, y_test_xg, average='macro'),\n",
    "        auc_score\n",
    "    )\n",
    "    storeBestParams(name, xgb.best_params_, \"XGboost2\")\n",
    "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
    "    print(xgb.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ed2f10",
   "metadata": {
    "papermill": {
     "duration": 0.01662,
     "end_time": "2025-10-16T09:07:28.080639",
     "exception": false,
     "start_time": "2025-10-16T09:07:28.064019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd3b2530",
   "metadata": {
    "id": "RQWUpTh_uU8b",
    "papermill": {
     "duration": 0.016496,
     "end_time": "2025-10-16T09:07:28.113685",
     "exception": false,
     "start_time": "2025-10-16T09:07:28.097189",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9346daf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T09:07:28.149012Z",
     "iopub.status.busy": "2025-10-16T09:07:28.148118Z",
     "iopub.status.idle": "2025-10-16T09:23:17.217077Z",
     "shell.execute_reply": "2025-10-16T09:23:17.216345Z"
    },
    "id": "2XoccNFtuYDy",
    "outputId": "a926b7f7-c01e-4e17-b3b0-95198769b9d8",
    "papermill": {
     "duration": 949.105999,
     "end_time": "2025-10-16T09:23:17.236454",
     "exception": false,
     "start_time": "2025-10-16T09:07:28.130455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features to select using Boruta: 11\n",
      "\n",
      "=== Gradient Boosting Model Performance with Hyperparameter Tuning ===\n",
      "\n",
      "Running Gradient Boosting with Original Data configuration...\n",
      "\n",
      "Gradien Boosting Model Performance Metrics\n",
      "Configuration Name:  Original Data\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.892977  0.864521 0.859201   0.873098 0.937592\n",
      "    Test  0.973333  0.963280 0.958333   0.971759 0.998588\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'warm_start': False, 'verbose': 0, 'validation_fraction': 0.1, 'tol': 0.037750000000000006, 'subsample': 0.1, 'n_iter_no_change': None, 'n_estimators': 370, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 5, 'min_samples_leaf': 8, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': None, 'max_features': 'sqrt', 'max_depth': 35, 'loss': 'log_loss', 'learning_rate': 0.0889, 'init': None, 'criterion': 'friedman_mse', 'ccp_alpha': 0.01625}\n",
      "\n",
      "Running Gradient Boosting with Normalized Data with RobustScaler configuration...\n",
      "\n",
      "Gradien Boosting Model Performance Metrics\n",
      "Configuration Name:  Normalized Data with RobustScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.892977  0.865938 0.859201   0.873452  0.93751\n",
      "    Test  0.960000  0.941774 0.936111   0.948148  0.99603\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'warm_start': False, 'verbose': 0, 'validation_fraction': 0.1, 'tol': 0.05, 'subsample': 0.1, 'n_iter_no_change': None, 'n_estimators': 250, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 17, 'min_samples_leaf': 5, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': None, 'max_features': 'sqrt', 'max_depth': 35, 'loss': 'log_loss', 'learning_rate': 0.055600000000000004, 'init': None, 'criterion': 'friedman_mse', 'ccp_alpha': 0.01625}\n",
      "\n",
      "Running Gradient Boosting with SMOTETomek + RobustScaler configuration...\n",
      "\n",
      "Gradien Boosting Model Performance Metrics\n",
      "Configuration Name:  SMOTETomek + RobustScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.908397  0.908217 0.908396   0.909471 0.969403\n",
      "    Test  0.960000  0.943788 0.947980   0.947368 0.992713\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'warm_start': False, 'verbose': 0, 'validation_fraction': 0.1, 'tol': 0.05, 'subsample': 0.1, 'n_iter_no_change': None, 'n_estimators': 80, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 14, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': None, 'max_features': 'sqrt', 'max_depth': 50, 'loss': 'log_loss', 'learning_rate': 0.0889, 'init': None, 'criterion': 'friedman_mse', 'ccp_alpha': 0.005}\n",
      "\n",
      "Running Gradient Boosting with MI configuration...\n",
      "\n",
      "Gradien Boosting Model Performance Metrics\n",
      "Configuration Name:  MI\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.889632  0.860440 0.853825   0.868151 0.942143\n",
      "    Test  0.946667  0.913889 0.913889   0.913889 0.996030\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'warm_start': False, 'verbose': 0, 'validation_fraction': 0.1, 'tol': 0.037750000000000006, 'subsample': 0.5, 'n_iter_no_change': None, 'n_estimators': 110, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 17, 'min_samples_leaf': 5, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': None, 'max_features': 'sqrt', 'max_depth': 45, 'loss': 'log_loss', 'learning_rate': 0.033400000000000006, 'init': None, 'criterion': 'friedman_mse', 'ccp_alpha': 0.005}\n",
      "\n",
      "Running Gradient Boosting with MI + SMOTETomek configuration...\n",
      "\n",
      "Gradien Boosting Model Performance Metrics\n",
      "Configuration Name:  MI + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.926923  0.927020 0.927003   0.927400 0.978182\n",
      "    Test  0.933333  0.900738 0.906313   0.895833 0.993118\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'warm_start': False, 'verbose': 0, 'validation_fraction': 0.1, 'tol': 0.001, 'subsample': 0.1, 'n_iter_no_change': None, 'n_estimators': 260, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 14, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': None, 'max_features': 'sqrt', 'max_depth': 55, 'loss': 'log_loss', 'learning_rate': 0.06670000000000001, 'init': None, 'criterion': 'friedman_mse', 'ccp_alpha': 0.005}\n",
      "\n",
      "Running Gradient Boosting with LDA configuration...\n",
      "\n",
      "Gradien Boosting Model Performance Metrics\n",
      "Configuration Name:  LDA\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.892977  0.865953 0.859201   0.873321 0.936208\n",
      "    Test  0.960000  0.935484 0.936111   0.936111 0.980328\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'warm_start': False, 'verbose': 0, 'validation_fraction': 0.1, 'tol': 0.001, 'subsample': 0.1, 'n_iter_no_change': None, 'n_estimators': 110, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 14, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': None, 'max_features': 'sqrt', 'max_depth': 55, 'loss': 'log_loss', 'learning_rate': 0.044500000000000005, 'init': None, 'criterion': 'friedman_mse', 'ccp_alpha': 0.01625}\n",
      "\n",
      "Running Gradient Boosting with LDA + SMOTETomek configuration...\n",
      "\n",
      "Gradien Boosting Model Performance Metrics\n",
      "Configuration Name:  LDA + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.894837  0.894461 0.894952   0.898360 0.971771\n",
      "    Test  0.933333  0.899482 0.904924   0.896825 0.985763\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'warm_start': False, 'verbose': 0, 'validation_fraction': 0.1, 'tol': 0.001, 'subsample': 0.1, 'n_iter_no_change': None, 'n_estimators': 180, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 11, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': None, 'max_features': 'sqrt', 'max_depth': 20, 'loss': 'log_loss', 'learning_rate': 0.055600000000000004, 'init': None, 'criterion': 'friedman_mse', 'ccp_alpha': 0.005}\n",
      "\n",
      "Running Gradient Boosting with Normalized Data with MinMaxScaler configuration...\n",
      "\n",
      "Gradien Boosting Model Performance Metrics\n",
      "Configuration Name:  Normalized Data with MinMaxScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.906355  0.886332 0.880707   0.893085 0.939414\n",
      "    Test  0.946667  0.920002 0.912500   0.937037 0.993153\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'warm_start': False, 'verbose': 0, 'validation_fraction': 0.1, 'tol': 0.013250000000000001, 'subsample': 0.1, 'n_iter_no_change': None, 'n_estimators': 250, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 14, 'min_samples_leaf': 5, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': None, 'max_features': 'sqrt', 'max_depth': 25, 'loss': 'log_loss', 'learning_rate': 0.0889, 'init': None, 'criterion': 'friedman_mse', 'ccp_alpha': 0.01625}\n",
      "\n",
      "Running Gradient Boosting with SMOTETomek + MiMaxScaler configuration...\n",
      "\n",
      "Gradien Boosting Model Performance Metrics\n",
      "Configuration Name:  SMOTETomek + MiMaxScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.937023  0.937165 0.937055   0.938022 0.983975\n",
      "    Test  0.960000  0.935214 0.934722   0.936975 0.994531\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'warm_start': False, 'verbose': 0, 'validation_fraction': 0.1, 'tol': 0.05, 'subsample': 0.1, 'n_iter_no_change': None, 'n_estimators': 330, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 11, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': None, 'max_features': 'sqrt', 'max_depth': 35, 'loss': 'log_loss', 'learning_rate': 0.07780000000000001, 'init': None, 'criterion': 'friedman_mse', 'ccp_alpha': 0.005}\n",
      "\n",
      "Running Gradient Boosting with Boruta configuration...\n",
      "\n",
      "Gradien Boosting Model Performance Metrics\n",
      "Configuration Name:  Boruta\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training   0.90301  0.879499 0.875330   0.883903 0.959661\n",
      "    Test   0.96000  0.935214 0.934722   0.936975 0.994583\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'warm_start': False, 'verbose': 0, 'validation_fraction': 0.1, 'tol': 0.025500000000000002, 'subsample': 0.1, 'n_iter_no_change': None, 'n_estimators': 380, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 2, 'min_samples_leaf': 8, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': None, 'max_features': 'sqrt', 'max_depth': 30, 'loss': 'log_loss', 'learning_rate': 0.07780000000000001, 'init': None, 'criterion': 'friedman_mse', 'ccp_alpha': 0.005}\n",
      "\n",
      "Running Gradient Boosting with Boruta + SMOTETomek configuration...\n",
      "\n",
      "Gradien Boosting Model Performance Metrics\n",
      "Configuration Name:  Boruta + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.942639  0.942726 0.942659   0.942865 0.982861\n",
      "    Test  0.933333  0.901219 0.907702   0.902116 0.989182\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'warm_start': False, 'verbose': 0, 'validation_fraction': 0.1, 'tol': 0.037750000000000006, 'subsample': 0.1, 'n_iter_no_change': None, 'n_estimators': 250, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 2, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': None, 'max_features': 'sqrt', 'max_depth': 30, 'loss': 'log_loss', 'learning_rate': 0.1, 'init': None, 'criterion': 'friedman_mse', 'ccp_alpha': 0.005}\n",
      "\n",
      "Running Gradient Boosting with Autoencoder configuration...\n",
      "\n",
      "Gradien Boosting Model Performance Metrics\n",
      "Configuration Name:  Autoencoder\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.886288  0.864928 0.848449   0.885055 0.918317\n",
      "    Test  0.920000  0.907279 0.900126   0.915278 0.981255\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'warm_start': False, 'verbose': 0, 'validation_fraction': 0.1, 'tol': 0.025500000000000002, 'subsample': 0.1, 'n_iter_no_change': None, 'n_estimators': 350, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 11, 'min_samples_leaf': 5, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': None, 'max_features': 'sqrt', 'max_depth': 20, 'loss': 'log_loss', 'learning_rate': 0.1, 'init': None, 'criterion': 'friedman_mse', 'ccp_alpha': 0.05}\n",
      "\n",
      "Running Gradient Boosting with Autoencoder + SMOTETomek configuration...\n",
      "\n",
      "Gradien Boosting Model Performance Metrics\n",
      "Configuration Name:  Autoencoder + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training   0.93295  0.933031 0.933015   0.933488 0.989887\n",
      "    Test   0.92000  0.905288 0.900126   0.911111 0.989478\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'warm_start': False, 'verbose': 0, 'validation_fraction': 0.1, 'tol': 0.001, 'subsample': 0.1, 'n_iter_no_change': None, 'n_estimators': 330, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 8, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': None, 'max_features': 'sqrt', 'max_depth': 25, 'loss': 'log_loss', 'learning_rate': 0.07780000000000001, 'init': None, 'criterion': 'friedman_mse', 'ccp_alpha': 0.005}\n"
     ]
    }
   ],
   "source": [
    "configurations = []\n",
    "\n",
    "configurations.append(('Original Data', X_train, X_test, y_train))\n",
    "configurations.append(('Normalized Data with RobustScaler', X_train_robust, X_test_robust, y_train))\n",
    "configurations.append(('SMOTETomek + RobustScaler', X_train_robust_resample, X_test_robust, y_train_robust_resample))\n",
    "configurations.append(('MI', X_train_mi, X_test_mi, y_train))\n",
    "configurations.append(('MI + SMOTETomek', X_train_mi_res, X_test_mi, y_train_mi_res))\n",
    "configurations.append(('LDA', X_train_lda, X_test_lda, y_train))\n",
    "configurations.append(('LDA + SMOTETomek', X_train_lda_res, X_test_lda, y_train_lda_res))\n",
    "configurations.append(('Normalized Data with MinMaxScaler', X_train_minmax, X_test_minmax, y_train))\n",
    "configurations.append(('SMOTETomek + MiMaxScaler', X_train_minmax_resample, X_test_minmax, y_train_minmax_resample))\n",
    "configurations.append(('Boruta', X_train_boruta, X_test_boruta, y_train))\n",
    "configurations.append(('Boruta + SMOTETomek', X_train_boruta_res, X_test_boruta, y_train_boruta_res))\n",
    "configurations.append(('Autoencoder', X_train_encoded, X_test_encoded, y_train))\n",
    "configurations.append(('Autoencoder + SMOTETomek', X_train_encoded_res, X_test_encoded, y_train_encoded_res))\n",
    "\n",
    "selected_features = boruta_selector.support_\n",
    "optimal_features = sum(selected_features)\n",
    "print(f\"Optimal number of features to select using Boruta: {optimal_features}\")\n",
    "\n",
    "# configurations.append(('SMOTETomek', X_train_resample, X_test_normalized, y_train_resample))\n",
    "\n",
    "# Step 4: Gradient Boosting + GridSearchCV\n",
    "print(\"\\n=== Gradient Boosting Model Performance with Hyperparameter Tuning ===\")\n",
    "\n",
    "param_grid = {\n",
    "    'loss': ['log_loss'],\n",
    "    'learning_rate': np.linspace(0.0001, 0.1, 10),\n",
    "    'n_estimators': range(40, 400, 10),\n",
    "    'subsample': np.linspace(0.1, 0.9, 3),\n",
    "    'max_depth': range(20, 60, 5),\n",
    "    'init': [None],\n",
    "    'max_leaf_nodes': [None],\n",
    "    'min_samples_split': range(2, 20, 3),\n",
    "    'min_samples_leaf': range(2, 10, 3),\n",
    "    'min_weight_fraction_leaf': [0.0],\n",
    "    'min_impurity_decrease': [0.0],\n",
    "    'validation_fraction': [0.1],\n",
    "    'n_iter_no_change': [None],\n",
    "    'tol': np.linspace(0.001, 0.05, 5),\n",
    "    'ccp_alpha': np.linspace(0.005, 0.05, 5),\n",
    "    'max_features': ['sqrt'],\n",
    "    'verbose': [0],\n",
    "    'warm_start': [False],\n",
    "    'criterion': ['friedman_mse'],\n",
    "    # 'random_state': [0]\n",
    "}\n",
    "\n",
    "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
    "    print(f\"\\nRunning Gradient Boosting with {name} configuration...\")\n",
    "    cv = StratifiedKFold(n_splits=8, shuffle=True, random_state=42)\n",
    "    gbc = RandomizedSearchCV(GradientBoostingClassifier(), param_grid, cv=cv,\n",
    "                             n_iter=50, n_jobs=-1, scoring=['accuracy', 'f1_macro'], refit='accuracy', verbose=0)\n",
    "    # gbc = GradientBoostingClassifier(random_state=42)\n",
    "    gbc.fit(X_train_cfg, y_train_cfg)\n",
    "\n",
    "    y_train_gb = gbc.predict(X_train_cfg)\n",
    "    y_test_gb = gbc.predict(X_test_cfg)\n",
    "    y_train_gb_proba = gbc.predict_proba(X_train_cfg)\n",
    "    y_test_gb_proba = gbc.predict_proba(X_test_cfg)\n",
    "\n",
    "    metrics_dict = {\n",
    "          \"Dataset\": [\"Training\", \"Test\"],\n",
    "          \"Accuracy\": [\n",
    "              metrics.accuracy_score(y_train_cfg, y_train_gb),\n",
    "              metrics.accuracy_score(y_test, y_test_gb),\n",
    "          ],\n",
    "          \"F1 Score\": [\n",
    "              metrics.f1_score(y_train_cfg, y_train_gb, average='macro'),\n",
    "              metrics.f1_score(y_test, y_test_gb, average='macro'),\n",
    "          ],\n",
    "          \"Recall\": [\n",
    "              metrics.recall_score(y_train_cfg, y_train_gb, average='macro'),\n",
    "              metrics.recall_score(y_test, y_test_gb, average='macro'),\n",
    "          ],\n",
    "          \"Precision\": [\n",
    "              metrics.precision_score(y_train_cfg, y_train_gb, average='macro'),\n",
    "              metrics.precision_score(y_test, y_test_gb, average='macro'),\n",
    "          ],\n",
    "          \"AUC-ROC\": [\n",
    "              metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_gb_proba, multi_class='ovr', average='macro'),\n",
    "              metrics.roc_auc_score(pd.get_dummies(y_test), y_test_gb_proba, multi_class='ovr', average='macro'),\n",
    "          ]\n",
    "      }\n",
    "\n",
    "    df_metrics = pd.DataFrame(metrics_dict)\n",
    "    print(\"\\nGradien Boosting Model Performance Metrics\")\n",
    "    print(\"Configuration Name: \", name)\n",
    "    print(df_metrics.to_string(index=False))\n",
    "\n",
    "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_gb_proba, multi_class='ovr', average='macro')\n",
    "    storeResults(\n",
    "          'Gradient Boosting',\n",
    "          name,\n",
    "          metrics.accuracy_score(y_test, y_test_gb),\n",
    "          metrics.f1_score(y_test, y_test_gb, average='macro'),\n",
    "          metrics.recall_score(y_test, y_test_gb, average='macro'),\n",
    "          metrics.precision_score(y_test, y_test_gb, average='macro'),\n",
    "          auc_score\n",
    "      )\n",
    "    storeBestParams(name, gbc.best_params_, \"Gradient Boosting\")\n",
    "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
    "    print(gbc.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a18091",
   "metadata": {
    "id": "EtjUT-NQw64A",
    "papermill": {
     "duration": 0.017123,
     "end_time": "2025-10-16T09:23:17.271396",
     "exception": false,
     "start_time": "2025-10-16T09:23:17.254273",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab5ba507",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T09:23:17.307438Z",
     "iopub.status.busy": "2025-10-16T09:23:17.307234Z",
     "iopub.status.idle": "2025-10-16T09:38:06.235169Z",
     "shell.execute_reply": "2025-10-16T09:38:06.234262Z"
    },
    "id": "01gypP_5xEKE",
    "outputId": "01d28060-45f6-43e9-ba3d-33677fc4ad63",
    "papermill": {
     "duration": 888.970198,
     "end_time": "2025-10-16T09:38:06.259139",
     "exception": false,
     "start_time": "2025-10-16T09:23:17.288941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features to select using Boruta: 11\n",
      "\n",
      "=== Extra Trees Model Performance with Hyperparameter Tuning ===\n",
      "\n",
      "Running Extra Trees with Original Data configuration...\n",
      "\n",
      "ExtraTrees Model Performance Metrics\n",
      "Configuration Name:  Original Data\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.916388  0.901213 0.896836   0.905995 0.982110\n",
      "    Test  0.960000  0.935214 0.934722   0.936975 0.997123\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'oob_score': False, 'n_estimators': 378, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 4, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': 54, 'max_features': 'sqrt', 'max_depth': 33, 'criterion': 'log_loss', 'class_weight': None, 'ccp_alpha': 0.001, 'bootstrap': False}\n",
      "\n",
      "Running Extra Trees with Normalized Data with RobustScaler configuration...\n",
      "\n",
      "ExtraTrees Model Performance Metrics\n",
      "Configuration Name:  Normalized Data with RobustScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.909699  0.890223 0.886083   0.895565 0.977681\n",
      "    Test  0.973333  0.956583 0.955556   0.962963 0.998570\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'oob_score': True, 'n_estimators': 495, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 3, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': 46, 'max_features': 'log2', 'max_depth': 42, 'criterion': 'log_loss', 'class_weight': None, 'ccp_alpha': 0.001, 'bootstrap': True}\n",
      "\n",
      "Running Extra Trees with SMOTETomek + RobustScaler configuration...\n",
      "\n",
      "ExtraTrees Model Performance Metrics\n",
      "Configuration Name:  SMOTETomek + RobustScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.942748  0.942922 0.942770   0.943729  0.98805\n",
      "    Test  0.960000  0.935214 0.934722   0.936975  0.99603\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'oob_score': False, 'n_estimators': 300, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 4, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': 46, 'max_features': 'sqrt', 'max_depth': 45, 'criterion': 'log_loss', 'class_weight': None, 'ccp_alpha': 0.001, 'bootstrap': False}\n",
      "\n",
      "Running Extra Trees with MI configuration...\n",
      "\n",
      "ExtraTrees Model Performance Metrics\n",
      "Configuration Name:  MI\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training   0.90301  0.879499 0.875330   0.883903 0.967877\n",
      "    Test   0.96000  0.935214 0.934722   0.936975 0.996400\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'oob_score': False, 'n_estimators': 391, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 2, 'min_samples_leaf': 3, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': 46, 'max_features': 'log2', 'max_depth': 42, 'criterion': 'log_loss', 'class_weight': None, 'ccp_alpha': 0.001, 'bootstrap': False}\n",
      "\n",
      "Running Extra Trees with MI + SMOTETomek configuration...\n",
      "\n",
      "ExtraTrees Model Performance Metrics\n",
      "Configuration Name:  MI + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.884615  0.884742 0.884538   0.888926 0.967298\n",
      "    Test  0.946667  0.913889 0.913889   0.913889 0.993420\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'oob_score': True, 'n_estimators': 456, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 4, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': 54, 'max_features': 'log2', 'max_depth': 42, 'criterion': 'gini', 'class_weight': None, 'ccp_alpha': 0.0059, 'bootstrap': True}\n",
      "\n",
      "Running Extra Trees with LDA configuration...\n",
      "\n",
      "ExtraTrees Model Performance Metrics\n",
      "Configuration Name:  LDA\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.892977  0.870114 0.859201   0.882923 0.944198\n",
      "    Test  0.960000  0.935214 0.934722   0.936975 0.986452\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'oob_score': False, 'n_estimators': 339, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 3, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': 38, 'max_features': 'sqrt', 'max_depth': 39, 'criterion': 'log_loss', 'class_weight': None, 'ccp_alpha': 0.0157, 'bootstrap': True}\n",
      "\n",
      "Running Extra Trees with LDA + SMOTETomek configuration...\n",
      "\n",
      "ExtraTrees Model Performance Metrics\n",
      "Configuration Name:  LDA + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training   0.89675  0.896923 0.896769   0.900863 0.970117\n",
      "    Test   0.96000  0.935484 0.936111   0.936111 0.994264\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'oob_score': False, 'n_estimators': 417, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 2, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': 46, 'max_features': 'log2', 'max_depth': 30, 'criterion': 'log_loss', 'class_weight': None, 'ccp_alpha': 0.0059, 'bootstrap': False}\n",
      "\n",
      "Running Extra Trees with Normalized Data with MinMaxScaler configuration...\n",
      "\n",
      "ExtraTrees Model Performance Metrics\n",
      "Configuration Name:  Normalized Data with MinMaxScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.909699  0.890223 0.886083   0.895565 0.977117\n",
      "    Test  0.960000  0.934392 0.933333   0.947368 0.998570\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'oob_score': False, 'n_estimators': 469, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 4, 'min_samples_leaf': 3, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': 38, 'max_features': 'log2', 'max_depth': 48, 'criterion': 'log_loss', 'class_weight': None, 'ccp_alpha': 0.001, 'bootstrap': False}\n",
      "\n",
      "Running Extra Trees with SMOTETomek + MiMaxScaler configuration...\n",
      "\n",
      "ExtraTrees Model Performance Metrics\n",
      "Configuration Name:  SMOTETomek + MiMaxScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.925573  0.925700 0.925616   0.928120 0.981725\n",
      "    Test  0.946667  0.922095 0.925758   0.933333 0.994143\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'oob_score': False, 'n_estimators': 313, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 2, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': 30, 'max_features': 'sqrt', 'max_depth': 42, 'criterion': 'gini', 'class_weight': None, 'ccp_alpha': 0.001, 'bootstrap': True}\n",
      "\n",
      "Running Extra Trees with Boruta configuration...\n",
      "\n",
      "ExtraTrees Model Performance Metrics\n",
      "Configuration Name:  Boruta\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.913043  0.895726 0.891459   0.900717 0.977839\n",
      "    Test  0.960000  0.935214 0.934722   0.936975 0.998553\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'oob_score': False, 'n_estimators': 443, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 2, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': 30, 'max_features': 'sqrt', 'max_depth': 33, 'criterion': 'log_loss', 'class_weight': None, 'ccp_alpha': 0.001, 'bootstrap': False}\n",
      "\n",
      "Running Extra Trees with Boruta + SMOTETomek configuration...\n",
      "\n",
      "ExtraTrees Model Performance Metrics\n",
      "Configuration Name:  Boruta + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.933078  0.933177 0.933157   0.934644 0.980696\n",
      "    Test  0.986667  0.978405 0.977778   0.980392 0.996718\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'oob_score': False, 'n_estimators': 365, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 2, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': 54, 'max_features': 'log2', 'max_depth': 33, 'criterion': 'gini', 'class_weight': None, 'ccp_alpha': 0.001, 'bootstrap': False}\n",
      "\n",
      "Running Extra Trees with Autoencoder configuration...\n",
      "\n",
      "ExtraTrees Model Performance Metrics\n",
      "Configuration Name:  Autoencoder\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.909699  0.892394 0.889555   0.896043 0.977230\n",
      "    Test  0.933333  0.922849 0.920960   0.926025 0.989539\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'oob_score': False, 'n_estimators': 313, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 2, 'min_samples_leaf': 3, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': 38, 'max_features': 'log2', 'max_depth': 48, 'criterion': 'log_loss', 'class_weight': None, 'ccp_alpha': 0.001, 'bootstrap': False}\n",
      "\n",
      "Running Extra Trees with Autoencoder + SMOTETomek configuration...\n",
      "\n",
      "ExtraTrees Model Performance Metrics\n",
      "Configuration Name:  Autoencoder + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.888889  0.888732 0.889139   0.895416 0.969626\n",
      "    Test  0.920000  0.910525 0.913384   0.910422 0.992687\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'oob_score': False, 'n_estimators': 365, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 2, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': 38, 'max_features': 'sqrt', 'max_depth': 45, 'criterion': 'log_loss', 'class_weight': None, 'ccp_alpha': 0.0108, 'bootstrap': False}\n"
     ]
    }
   ],
   "source": [
    "configurations = []\n",
    "configurations.append(('Original Data', X_train, X_test, y_train))\n",
    "configurations.append(('Normalized Data with RobustScaler', X_train_robust, X_test_robust, y_train))\n",
    "configurations.append(('SMOTETomek + RobustScaler', X_train_robust_resample, X_test_robust, y_train_robust_resample))\n",
    "configurations.append(('MI', X_train_mi, X_test_mi, y_train))\n",
    "configurations.append(('MI + SMOTETomek', X_train_mi_res, X_test_mi, y_train_mi_res))\n",
    "configurations.append(('LDA', X_train_lda, X_test_lda, y_train))\n",
    "configurations.append(('LDA + SMOTETomek', X_train_lda_res, X_test_lda, y_train_lda_res))\n",
    "configurations.append(('Normalized Data with MinMaxScaler', X_train_minmax, X_test_minmax, y_train))\n",
    "configurations.append(('SMOTETomek + MiMaxScaler', X_train_minmax_resample, X_test_minmax, y_train_minmax_resample))\n",
    "configurations.append(('Boruta', X_train_boruta, X_test_boruta, y_train))\n",
    "configurations.append(('Boruta + SMOTETomek', X_train_boruta_res, X_test_boruta, y_train_boruta_res))\n",
    "configurations.append(('Autoencoder', X_train_encoded, X_test_encoded, y_train))\n",
    "configurations.append(('Autoencoder + SMOTETomek', X_train_encoded_res, X_test_encoded, y_train_encoded_res))\n",
    "\n",
    "selected_features = boruta_selector.support_\n",
    "optimal_features = sum(selected_features)\n",
    "print(f\"Optimal number of features to select using Boruta: {optimal_features}\")\n",
    "\n",
    "# configurations.append(('SMOTETomek', X_train_resample, X_test_normalized, y_train_resample))\n",
    "\n",
    "# Step 4: Extra Trees + GridSearchCV\n",
    "print(\"\\n=== Extra Trees Model Performance with Hyperparameter Tuning ===\")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': range(300, 500, 13),\n",
    "    'max_depth': range(30, 50, 3),\n",
    "    'max_leaf_nodes': range(30, 60, 8),\n",
    "    'min_samples_split': [2, 3, 4],\n",
    "    'min_samples_leaf': [2, 3, 4],\n",
    "    'min_weight_fraction_leaf': [0.0],\n",
    "    'min_impurity_decrease': [0.0],\n",
    "    'ccp_alpha': np.linspace(0.001, 0.05, 11),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'class_weight': [None],\n",
    "    'bootstrap': [True, False],\n",
    "    'oob_score': [True, False],\n",
    "    'criterion': ['gini', 'log_loss'],\n",
    "    # 'random_state': range(2, 10, 1),\n",
    "}\n",
    "\n",
    "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
    "    print(f\"\\nRunning Extra Trees with {name} configuration...\")\n",
    "    cv = StratifiedKFold(n_splits=8, shuffle=True, random_state=42)\n",
    "    etc = RandomizedSearchCV(ExtraTreesClassifier(), param_grid, cv=cv, n_iter=50,\n",
    "                             n_jobs=-1, scoring=[\"accuracy\", \"f1_macro\"], refit='accuracy', verbose=0)\n",
    "    # etc = ExtraTreesClassifier(random_state=42)\n",
    "    etc.fit(X_train_cfg, y_train_cfg)\n",
    "\n",
    "    y_train_et = etc.predict(X_train_cfg)\n",
    "    y_test_et = etc.predict(X_test_cfg)\n",
    "    y_train_et_proba = etc.predict_proba(X_train_cfg)\n",
    "    y_test_et_proba = etc.predict_proba(X_test_cfg)\n",
    "\n",
    "    metrics_dict = {\n",
    "          \"Dataset\": [\"Training\", \"Test\"],\n",
    "          \"Accuracy\": [\n",
    "              metrics.accuracy_score(y_train_cfg, y_train_et),\n",
    "              metrics.accuracy_score(y_test, y_test_et),\n",
    "          ],\n",
    "          \"F1 Score\": [\n",
    "              metrics.f1_score(y_train_cfg, y_train_et, average='macro'),\n",
    "              metrics.f1_score(y_test, y_test_et, average='macro'),\n",
    "          ],\n",
    "          \"Recall\": [\n",
    "              metrics.recall_score(y_train_cfg, y_train_et, average='macro'),\n",
    "              metrics.recall_score(y_test, y_test_et, average='macro'),\n",
    "          ],\n",
    "          \"Precision\": [\n",
    "              metrics.precision_score(y_train_cfg, y_train_et, average='macro'),\n",
    "              metrics.precision_score(y_test, y_test_et, average='macro'),\n",
    "          ],\n",
    "          \"AUC-ROC\": [\n",
    "              metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_et_proba, multi_class='ovr', average='macro'),\n",
    "              metrics.roc_auc_score(pd.get_dummies(y_test), y_test_et_proba, multi_class='ovr', average='macro'),\n",
    "          ]\n",
    "      }\n",
    "\n",
    "    df_metrics = pd.DataFrame(metrics_dict)\n",
    "    print(\"\\nExtraTrees Model Performance Metrics\")\n",
    "    print(\"Configuration Name: \", name)\n",
    "    print(df_metrics.to_string(index=False))\n",
    "\n",
    "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_et_proba, multi_class='ovr', average='macro')\n",
    "    storeResults(\n",
    "          'Extra Trees',\n",
    "          name,\n",
    "          metrics.accuracy_score(y_test, y_test_et),\n",
    "          metrics.f1_score(y_test, y_test_et, average='macro'),\n",
    "          metrics.recall_score(y_test, y_test_et, average='macro'),\n",
    "          metrics.precision_score(y_test, y_test_et, average='macro'),\n",
    "          auc_score\n",
    "      )\n",
    "    storeBestParams(name, etc.best_params_, 'Extra Trees')\n",
    "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
    "    print(etc.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0b3c9d",
   "metadata": {
    "id": "gSVf2NIHes4c",
    "papermill": {
     "duration": 0.020804,
     "end_time": "2025-10-16T09:38:06.300978",
     "exception": false,
     "start_time": "2025-10-16T09:38:06.280174",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ADABoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2094ef4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T09:38:06.341188Z",
     "iopub.status.busy": "2025-10-16T09:38:06.340929Z",
     "iopub.status.idle": "2025-10-16T09:54:39.016252Z",
     "shell.execute_reply": "2025-10-16T09:54:39.015226Z"
    },
    "id": "lZClculzemGl",
    "outputId": "f4d44b8b-eada-4a35-e132-30767320017b",
    "papermill": {
     "duration": 992.717936,
     "end_time": "2025-10-16T09:54:39.038612",
     "exception": false,
     "start_time": "2025-10-16T09:38:06.320676",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features to select using Boruta: 11\n",
      "\n",
      "=== AdaBoost Model Performance with Hyperparameter Tuning ===\n",
      "\n",
      "Running AdaBoost with Original Data configuration...\n",
      "\n",
      "AdaBoost Model Performance Metrics\n",
      "Configuration Name:  Original Data\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.916388  0.901116 0.896836   0.906622 0.985429\n",
      "    Test  0.933333  0.892473 0.893056   0.893056 0.977374\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'random_state': 33, 'n_estimators': 191, 'learning_rate': 0.03, 'estimator__min_samples_split': 3, 'estimator__max_depth': 5, 'algorithm': 'SAMME'}\n",
      "\n",
      "Running AdaBoost with Normalized Data with RobustScaler configuration...\n",
      "\n",
      "AdaBoost Model Performance Metrics\n",
      "Configuration Name:  Normalized Data with RobustScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.916388  0.901213 0.896836   0.905995 0.982728\n",
      "    Test  0.946667  0.913889 0.915278   0.917367 0.986676\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'random_state': 37, 'n_estimators': 412, 'learning_rate': 0.01, 'estimator__min_samples_split': 3, 'estimator__max_depth': 5, 'algorithm': 'SAMME'}\n",
      "\n",
      "Running AdaBoost with SMOTETomek + RobustScaler configuration...\n",
      "\n",
      "AdaBoost Model Performance Metrics\n",
      "Configuration Name:  SMOTETomek + RobustScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.950382  0.950447 0.950411   0.950778 0.993582\n",
      "    Test  0.946667  0.913889 0.915278   0.917367 0.973895\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'random_state': 45, 'n_estimators': 152, 'learning_rate': 0.05, 'estimator__min_samples_split': 4, 'estimator__max_depth': 5, 'algorithm': 'SAMME'}\n",
      "\n",
      "Running AdaBoost with MI configuration...\n",
      "\n",
      "AdaBoost Model Performance Metrics\n",
      "Configuration Name:  MI\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.913043  0.895602 0.891459   0.901612 0.967874\n",
      "    Test  0.973333  0.956944 0.956944   0.956944 0.997847\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'random_state': 39, 'n_estimators': 178, 'learning_rate': 0.01, 'estimator__min_samples_split': 2, 'estimator__max_depth': 5, 'algorithm': 'SAMME'}\n",
      "\n",
      "Running AdaBoost with MI + SMOTETomek configuration...\n",
      "\n",
      "AdaBoost Model Performance Metrics\n",
      "Configuration Name:  MI + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.948077  0.948196 0.948179   0.948432 0.989413\n",
      "    Test  0.973333  0.956944 0.956944   0.956944 0.976668\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'random_state': 27, 'n_estimators': 191, 'learning_rate': 0.03, 'estimator__min_samples_split': 3, 'estimator__max_depth': 8, 'algorithm': 'SAMME'}\n",
      "\n",
      "Running AdaBoost with LDA configuration...\n",
      "\n",
      "AdaBoost Model Performance Metrics\n",
      "Configuration Name:  LDA\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.913043  0.895726 0.891459   0.900717 0.976137\n",
      "    Test  0.933333  0.892473 0.893056   0.893056 0.947315\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'random_state': 43, 'n_estimators': 321, 'learning_rate': 0.05, 'estimator__min_samples_split': 3, 'estimator__max_depth': 8, 'algorithm': 'SAMME'}\n",
      "\n",
      "Running AdaBoost with LDA + SMOTETomek configuration...\n",
      "\n",
      "AdaBoost Model Performance Metrics\n",
      "Configuration Name:  LDA + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.946463  0.946564 0.946491   0.946895 0.988655\n",
      "    Test  0.933333  0.892473 0.893056   0.893056 0.901651\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'random_state': 59, 'n_estimators': 100, 'learning_rate': 0.05, 'estimator__min_samples_split': 2, 'estimator__max_depth': 8, 'algorithm': 'SAMME'}\n",
      "\n",
      "Running AdaBoost with Normalized Data with MinMaxScaler configuration...\n",
      "\n",
      "AdaBoost Model Performance Metrics\n",
      "Configuration Name:  Normalized Data with MinMaxScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.919732  0.905135 0.902212   0.908882 0.985491\n",
      "    Test  0.920000  0.879905 0.885480   0.877124 0.978787\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'random_state': 41, 'n_estimators': 269, 'learning_rate': 0.03, 'estimator__min_samples_split': 3, 'estimator__max_depth': 5, 'algorithm': 'SAMME'}\n",
      "\n",
      "Running AdaBoost with SMOTETomek + MiMaxScaler configuration...\n",
      "\n",
      "AdaBoost Model Performance Metrics\n",
      "Configuration Name:  SMOTETomek + MiMaxScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.952290  0.952344 0.952315   0.952573 0.993703\n",
      "    Test  0.906667  0.877124 0.892551   0.876190 0.965007\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'random_state': 43, 'n_estimators': 334, 'learning_rate': 0.03, 'estimator__min_samples_split': 2, 'estimator__max_depth': 5, 'algorithm': 'SAMME'}\n",
      "\n",
      "Running AdaBoost with Boruta configuration...\n",
      "\n",
      "AdaBoost Model Performance Metrics\n",
      "Configuration Name:  Boruta\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.916388  0.901213 0.896836   0.905995 0.983338\n",
      "    Test  0.933333  0.892473 0.893056   0.893056 0.982671\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'random_state': 33, 'n_estimators': 256, 'learning_rate': 0.03, 'estimator__min_samples_split': 4, 'estimator__max_depth': 5, 'algorithm': 'SAMME'}\n",
      "\n",
      "Running AdaBoost with Boruta + SMOTETomek configuration...\n",
      "\n",
      "AdaBoost Model Performance Metrics\n",
      "Configuration Name:  Boruta + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.948375  0.948427 0.948417   0.948510 0.990584\n",
      "    Test  0.920000  0.890556 0.885480   0.896296 0.945554\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'random_state': 35, 'n_estimators': 438, 'learning_rate': 0.05, 'estimator__min_samples_split': 2, 'estimator__max_depth': 5, 'algorithm': 'SAMME'}\n",
      "\n",
      "Running AdaBoost with Autoencoder configuration...\n",
      "\n",
      "AdaBoost Model Performance Metrics\n",
      "Configuration Name:  Autoencoder\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.913043  0.897206 0.891459   0.903674 0.963741\n",
      "    Test  0.946667  0.937796 0.941793   0.936693 0.986409\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'random_state': 20, 'n_estimators': 295, 'learning_rate': 0.03, 'estimator__min_samples_split': 3, 'estimator__max_depth': 2, 'algorithm': 'SAMME'}\n",
      "\n",
      "Running AdaBoost with Autoencoder + SMOTETomek configuration...\n",
      "\n",
      "AdaBoost Model Performance Metrics\n",
      "Configuration Name:  Autoencoder + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.952107  0.952190 0.952139   0.952500 0.992938\n",
      "    Test  0.920000  0.898827 0.900126   0.898737 0.941482\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'random_state': 48, 'n_estimators': 477, 'learning_rate': 0.05, 'estimator__min_samples_split': 4, 'estimator__max_depth': 8, 'algorithm': 'SAMME'}\n"
     ]
    }
   ],
   "source": [
    "configurations = []\n",
    "\n",
    "configurations.append(('Original Data', X_train, X_test, y_train))\n",
    "configurations.append(('Normalized Data with RobustScaler', X_train_robust, X_test_robust, y_train))\n",
    "configurations.append(('SMOTETomek + RobustScaler', X_train_robust_resample, X_test_robust, y_train_robust_resample))\n",
    "configurations.append(('MI', X_train_mi, X_test_mi, y_train))\n",
    "configurations.append(('MI + SMOTETomek', X_train_mi_res, X_test_mi, y_train_mi_res))\n",
    "configurations.append(('LDA', X_train_lda, X_test_lda, y_train))\n",
    "configurations.append(('LDA + SMOTETomek', X_train_lda_res, X_test_lda, y_train_lda_res))\n",
    "configurations.append(('Normalized Data with MinMaxScaler', X_train_minmax, X_test_minmax, y_train))\n",
    "configurations.append(('SMOTETomek + MiMaxScaler', X_train_minmax_resample, X_test_minmax, y_train_minmax_resample))\n",
    "configurations.append(('Boruta', X_train_boruta, X_test_boruta, y_train))\n",
    "configurations.append(('Boruta + SMOTETomek', X_train_boruta_res, X_test_boruta, y_train_boruta_res))\n",
    "configurations.append(('Autoencoder', X_train_encoded, X_test_encoded, y_train))\n",
    "configurations.append(('Autoencoder + SMOTETomek', X_train_encoded_res, X_test_encoded, y_train_encoded_res))\n",
    "\n",
    "selected_features = boruta_selector.support_\n",
    "optimal_features = sum(selected_features)\n",
    "print(f\"Optimal number of features to select using Boruta: {optimal_features}\")\n",
    "\n",
    "# Step 4: AdaBoost + GridSearchCV\n",
    "print(\"\\n=== AdaBoost Model Performance with Hyperparameter Tuning ===\")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': range(100, 500, 13), #[50, 150],\n",
    "    'algorithm': ['SAMME'], \n",
    "    'learning_rate': np.linspace(0.01, 0.05, 3), #[0.005, 0.5, 0.03, 0.003],\n",
    "    'estimator__max_depth': range(2, 10, 3), #[5, 20],\n",
    "    'estimator__min_samples_split': range(1, 5, 1), #[8],\n",
    "    'random_state': range(20, 60) #[42, 1234]\n",
    "}\n",
    "\n",
    "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
    "    print(f\"\\nRunning AdaBoost with {name} configuration...\")\n",
    "    cv = StratifiedKFold(n_splits=8, shuffle=True, random_state=42)\n",
    "    adb = RandomizedSearchCV(AdaBoostClassifier(estimator=DecisionTreeClassifier()), param_grid, cv=cv, n_iter=50, n_jobs=-1,\n",
    "                             scoring=['accuracy', 'f1_macro'], refit='accuracy', verbose=0)\n",
    "    # adb = AdaBoostClassifier(random_state=42)\n",
    "    adb.fit(X_train_cfg, y_train_cfg)\n",
    "\n",
    "    y_train_ad = adb.predict(X_train_cfg)\n",
    "    y_test_ad = adb.predict(X_test_cfg)\n",
    "    y_train_ad_proba = adb.predict_proba(X_train_cfg)\n",
    "    y_test_ad_proba = adb.predict_proba(X_test_cfg)\n",
    "\n",
    "    metrics_dict = {\n",
    "          \"Dataset\": [\"Training\", \"Test\"],\n",
    "          \"Accuracy\": [\n",
    "              metrics.accuracy_score(y_train_cfg, y_train_ad),\n",
    "              metrics.accuracy_score(y_test, y_test_ad),\n",
    "          ],\n",
    "          \"F1 Score\": [\n",
    "              metrics.f1_score(y_train_cfg, y_train_ad, average='macro'),\n",
    "              metrics.f1_score(y_test, y_test_ad, average='macro'),\n",
    "          ],\n",
    "          \"Recall\": [\n",
    "              metrics.recall_score(y_train_cfg, y_train_ad, average='macro'),\n",
    "              metrics.recall_score(y_test, y_test_ad, average='macro'),\n",
    "          ],\n",
    "          \"Precision\": [\n",
    "              metrics.precision_score(y_train_cfg, y_train_ad, average='macro'),\n",
    "              metrics.precision_score(y_test, y_test_ad, average='macro'),\n",
    "          ],\n",
    "          \"AUC-ROC\": [\n",
    "              metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_ad_proba, multi_class='ovr', average='macro'),\n",
    "              metrics.roc_auc_score(pd.get_dummies(y_test), y_test_ad_proba, multi_class='ovr', average='macro'),\n",
    "          ]\n",
    "      }\n",
    "\n",
    "    df_metrics = pd.DataFrame(metrics_dict)\n",
    "    print(\"\\nAdaBoost Model Performance Metrics\")\n",
    "    print(\"Configuration Name: \", name)\n",
    "    print(df_metrics.to_string(index=False))\n",
    "\n",
    "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_ad_proba, multi_class='ovr', average='macro')\n",
    "    storeResults(\n",
    "          'AdaBoost',\n",
    "          name,\n",
    "          metrics.accuracy_score(y_test, y_test_ad),\n",
    "          metrics.f1_score(y_test, y_test_ad, average='macro'),\n",
    "          metrics.recall_score(y_test, y_test_ad, average='macro'),\n",
    "          metrics.precision_score(y_test, y_test_ad, average='macro'),\n",
    "          auc_score\n",
    "      )\n",
    "    storeBestParams(name, adb.best_params_, 'AdaBoost')\n",
    "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
    "    print(adb.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c86f10",
   "metadata": {
    "id": "_TA7vnOBtT3P",
    "papermill": {
     "duration": 0.020181,
     "end_time": "2025-10-16T09:54:39.079619",
     "exception": false,
     "start_time": "2025-10-16T09:54:39.059438",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a99d143",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T09:54:39.125288Z",
     "iopub.status.busy": "2025-10-16T09:54:39.124962Z",
     "iopub.status.idle": "2025-10-16T09:55:02.733564Z",
     "shell.execute_reply": "2025-10-16T09:55:02.732697Z"
    },
    "id": "ScDMjCP9tTZH",
    "outputId": "9ba4bfd7-9b30-4b67-859b-bb6c24c5d98a",
    "papermill": {
     "duration": 23.633348,
     "end_time": "2025-10-16T09:55:02.734940",
     "exception": false,
     "start_time": "2025-10-16T09:54:39.101592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running MLP Classifier with Original Data configuration...\n",
      "\\MLP Model Performance Metrics\n",
      "Configuration Name:  Original Data\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.585284  0.246132 0.333333   0.195095 0.500000\n",
      "    Test  0.613333  0.324786 0.375000   0.534247 0.537142\n",
      "\n",
      "Running MLP Classifier with Normalized Data with RobustScaler configuration...\n",
      "\\MLP Model Performance Metrics\n",
      "Configuration Name:  Normalized Data with RobustScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.909699  0.890223 0.886083   0.895565 0.942245\n",
      "    Test  0.960000  0.934392 0.933333   0.947368 0.995289\n",
      "\n",
      "Running MLP Classifier with SMOTETomek + RobustScaler configuration...\n",
      "\\MLP Model Performance Metrics\n",
      "Configuration Name:  SMOTETomek + RobustScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.896947  0.896847 0.897011   0.902984 0.968769\n",
      "    Test  0.946667  0.922095 0.925758   0.933333 0.987494\n",
      "\n",
      "Running MLP Classifier with MI configuration...\n",
      "\\MLP Model Performance Metrics\n",
      "Configuration Name:  MI\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.892977  0.867615 0.866144   0.869575 0.931066\n",
      "    Test  0.960000  0.935484 0.936111   0.936111 0.992343\n",
      "\n",
      "Running MLP Classifier with MI + SMOTETomek configuration...\n",
      "\\MLP Model Performance Metrics\n",
      "Configuration Name:  MI + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.875000  0.875549 0.875014   0.878677 0.953866\n",
      "    Test  0.933333  0.909485 0.920960   0.903704 0.988269\n",
      "\n",
      "Running MLP Classifier with LDA configuration...\n",
      "\\MLP Model Performance Metrics\n",
      "Configuration Name:  LDA\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.876254  0.847506 0.846206   0.849279 0.921789\n",
      "    Test  0.933333  0.901219 0.906313   0.896732 0.960597\n",
      "\n",
      "Running MLP Classifier with LDA + SMOTETomek configuration...\n",
      "\\MLP Model Performance Metrics\n",
      "Configuration Name:  LDA + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.866157  0.866384 0.866094   0.868435 0.933098\n",
      "    Test  0.933333  0.908915 0.920960   0.899510 0.946429\n",
      "\n",
      "Running MLP Classifier with Normalized Data with MinMaxScaler configuration...\n",
      "\\MLP Model Performance Metrics\n",
      "Configuration Name:  Normalized Data with MinMaxScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.892977  0.865410 0.862673   0.868924 0.935370\n",
      "    Test  0.920000  0.896172 0.911995   0.885380 0.992536\n",
      "\n",
      "Running MLP Classifier with SMOTETomek + MiMaxScaler configuration...\n",
      "\\MLP Model Performance Metrics\n",
      "Configuration Name:  SMOTETomek + MiMaxScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.900763  0.901008 0.900821   0.905106 0.962670\n",
      "    Test  0.920000  0.895571 0.910606   0.894994 0.967487\n",
      "\n",
      "Running MLP Classifier with Boruta configuration...\n",
      "\\MLP Model Performance Metrics\n",
      "Configuration Name:  Boruta\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.879599  0.850637 0.851582   0.853635 0.929609\n",
      "    Test  0.906667  0.875230 0.892551   0.863426 0.985306\n",
      "\n",
      "Running MLP Classifier with Boruta + SMOTETomek configuration...\n",
      "\\MLP Model Performance Metrics\n",
      "Configuration Name:  Boruta + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.868069  0.868365 0.868131   0.871040 0.952110\n",
      "    Test  0.893333  0.855028 0.870328   0.843137 0.970637\n",
      "\n",
      "Running MLP Classifier with Autoencoder configuration...\n",
      "\\MLP Model Performance Metrics\n",
      "Configuration Name:  Autoencoder\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.879599  0.850637 0.851582   0.853635 0.925640\n",
      "    Test  0.920000  0.898827 0.900126   0.898737 0.984233\n",
      "\n",
      "Running MLP Classifier with Autoencoder + SMOTETomek configuration...\n",
      "\\MLP Model Performance Metrics\n",
      "Configuration Name:  Autoencoder + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.854406  0.854573 0.854435   0.858020 0.940167\n",
      "    Test  0.933333  0.916762 0.934217   0.902778 0.980482\n"
     ]
    }
   ],
   "source": [
    "configurations = []\n",
    "\n",
    "configurations.append(('Original Data', X_train, X_test, y_train))\n",
    "configurations.append(('Normalized Data with RobustScaler', X_train_robust, X_test_robust, y_train))\n",
    "configurations.append(('SMOTETomek + RobustScaler', X_train_robust_resample, X_test_robust, y_train_robust_resample))\n",
    "configurations.append(('MI', X_train_mi, X_test_mi, y_train))\n",
    "configurations.append(('MI + SMOTETomek', X_train_mi_res, X_test_mi, y_train_mi_res))\n",
    "configurations.append(('LDA', X_train_lda, X_test_lda, y_train))\n",
    "configurations.append(('LDA + SMOTETomek', X_train_lda_res, X_test_lda, y_train_lda_res))\n",
    "configurations.append(('Normalized Data with MinMaxScaler', X_train_minmax, X_test_minmax, y_train))\n",
    "configurations.append(('SMOTETomek + MiMaxScaler', X_train_minmax_resample, X_test_minmax, y_train_minmax_resample))\n",
    "configurations.append(('Boruta', X_train_boruta, X_test_boruta, y_train))\n",
    "configurations.append(('Boruta + SMOTETomek', X_train_boruta_res, X_test_boruta, y_train_boruta_res))\n",
    "configurations.append(('Autoencoder', X_train_encoded, X_test_encoded, y_train))\n",
    "configurations.append(('Autoencoder + SMOTETomek', X_train_encoded_res, X_test_encoded, y_train_encoded_res))\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),\n",
    "    activation='relu',\n",
    "    solver='sgd',\n",
    "    alpha=0.01,\n",
    "    batch_size='auto',\n",
    "    learning_rate='constant',\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "verbose=False)\n",
    "\n",
    "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
    "    print(f\"\\nRunning MLP Classifier with {name} configuration...\")\n",
    "    mlp.fit(X_train_cfg, y_train_cfg)\n",
    "\n",
    "    y_train_mlp = mlp.predict(X_train_cfg)\n",
    "    y_test_mlp = mlp.predict(X_test_cfg)\n",
    "    y_train_mlp_proba = mlp.predict_proba(X_train_cfg)\n",
    "    y_test_mlp_proba = mlp.predict_proba(X_test_cfg)\n",
    "\n",
    "    metrics_dict = {\n",
    "          \"Dataset\": [\"Training\", \"Test\"],\n",
    "          \"Accuracy\": [\n",
    "              metrics.accuracy_score(y_train_cfg, y_train_mlp),\n",
    "              metrics.accuracy_score(y_test, y_test_mlp),\n",
    "          ],\n",
    "          \"F1 Score\": [\n",
    "              metrics.f1_score(y_train_cfg, y_train_mlp, average='macro'),\n",
    "              metrics.f1_score(y_test, y_test_mlp, average='macro'),\n",
    "          ],\n",
    "          \"Recall\": [\n",
    "              metrics.recall_score(y_train_cfg, y_train_mlp, average='macro'),\n",
    "              metrics.recall_score(y_test, y_test_mlp, average='macro'),\n",
    "          ],\n",
    "          \"Precision\": [\n",
    "              metrics.precision_score(y_train_cfg, y_train_mlp, average='macro'),\n",
    "              metrics.precision_score(y_test, y_test_mlp, average='macro'),\n",
    "          ],\n",
    "          \"AUC-ROC\": [\n",
    "              metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_mlp_proba, multi_class='ovr', average='macro'),\n",
    "              metrics.roc_auc_score(pd.get_dummies(y_test), y_test_mlp_proba, multi_class='ovr', average='macro'),\n",
    "          ]\n",
    "      }\n",
    "\n",
    "    df_metrics = pd.DataFrame(metrics_dict)\n",
    "    print(\"\\MLP Model Performance Metrics\")\n",
    "    print(\"Configuration Name: \", name)\n",
    "    print(df_metrics.to_string(index=False))\n",
    "\n",
    "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_mlp_proba, multi_class='ovr', average='macro')\n",
    "    storeResults(\n",
    "          'MLP Classifier',\n",
    "          name,\n",
    "          metrics.accuracy_score(y_test, y_test_mlp),\n",
    "          metrics.f1_score(y_test, y_test_mlp, average='macro'),\n",
    "          metrics.recall_score(y_test, y_test_mlp, average='macro'),\n",
    "          metrics.precision_score(y_test, y_test_mlp, average='macro'),\n",
    "          auc_score\n",
    "      )\n",
    "    # print(\"Best hyperparameters found by GridSearchCV:\")\n",
    "    # print(mlp.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9b46e8",
   "metadata": {
    "id": "kMyoLU1_fzc4",
    "papermill": {
     "duration": 0.021246,
     "end_time": "2025-10-16T09:55:02.778179",
     "exception": false,
     "start_time": "2025-10-16T09:55:02.756933",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13e99b2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T09:55:02.821836Z",
     "iopub.status.busy": "2025-10-16T09:55:02.821538Z",
     "iopub.status.idle": "2025-10-16T10:06:00.902513Z",
     "shell.execute_reply": "2025-10-16T10:06:00.901805Z"
    },
    "id": "Hd9mJYCzkVFz",
    "outputId": "7ad7a082-601f-44e4-f4b1-1ef5a95c5a47",
    "papermill": {
     "duration": 658.125963,
     "end_time": "2025-10-16T10:06:00.925545",
     "exception": false,
     "start_time": "2025-10-16T09:55:02.799582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features to select using Boruta: 11\n",
      "\n",
      "=== LightGBM Model Performance with Hyperparameter Tuning ===\n",
      "\n",
      "Running LightGBM with Original Data configuration...\n",
      "\n",
      "LightGBM Model Performance Metrics\n",
      "Configuration Name:  Original Data\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.916388  0.901213 0.896836   0.905995 0.964426\n",
      "    Test  0.973333  0.956944 0.956944   0.956944 0.997830\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 0.3875, 'reg_lambda': 0.1, 'reg_alpha': 0.1, 'num_leaves': 20, 'n_estimators': 490, 'min_child_samples': 2, 'max_depth': 7, 'learning_rate': 0.01, 'colsample_bytree': 0.05}\n",
      "\n",
      "Running LightGBM with Normalized Data with RobustScaler configuration...\n",
      "\n",
      "LightGBM Model Performance Metrics\n",
      "Configuration Name:  Normalized Data with RobustScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.916388  0.901116 0.896836   0.906622 0.974745\n",
      "    Test  0.960000  0.935214 0.934722   0.936975 0.996383\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 0.3875, 'reg_lambda': 0.1, 'reg_alpha': 0.1, 'num_leaves': 40, 'n_estimators': 325, 'min_child_samples': 77, 'max_depth': 13, 'learning_rate': 0.2, 'colsample_bytree': 0.275}\n",
      "\n",
      "Running LightGBM with SMOTETomek + RobustScaler configuration...\n",
      "\n",
      "LightGBM Model Performance Metrics\n",
      "Configuration Name:  SMOTETomek + RobustScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.950382  0.950447 0.950411   0.950778 0.990503\n",
      "    Test  0.960000  0.943077 0.949369   0.937500 0.993723\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 0.5, 'reg_lambda': 0.1, 'reg_alpha': 0.1, 'num_leaves': 125, 'n_estimators': 210, 'min_child_samples': 98, 'max_depth': 9, 'learning_rate': 0.2, 'colsample_bytree': 0.3875}\n",
      "\n",
      "Running LightGBM with MI configuration...\n",
      "\n",
      "LightGBM Model Performance Metrics\n",
      "Configuration Name:  MI\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.913043  0.895602 0.891459   0.901612 0.972029\n",
      "    Test  0.973333  0.956944 0.956944   0.956944 0.996770\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 0.3875, 'reg_lambda': 0.1, 'reg_alpha': 0.1, 'num_leaves': 105, 'n_estimators': 280, 'min_child_samples': 5, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.3875}\n",
      "\n",
      "Running LightGBM with MI + SMOTETomek configuration...\n",
      "\n",
      "LightGBM Model Performance Metrics\n",
      "Configuration Name:  MI + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.948077  0.948196 0.948179   0.948432 0.987700\n",
      "    Test  0.986667  0.978495 0.979167   0.979167 0.989234\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 0.3875, 'reg_lambda': 0.1, 'reg_alpha': 0.1, 'num_leaves': 100, 'n_estimators': 185, 'min_child_samples': 53, 'max_depth': 7, 'learning_rate': 0.2, 'colsample_bytree': 0.5}\n",
      "\n",
      "Running LightGBM with LDA configuration...\n",
      "\n",
      "LightGBM Model Performance Metrics\n",
      "Configuration Name:  LDA\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.892977  0.870114 0.859201   0.882923 0.944515\n",
      "    Test  0.960000  0.935214 0.934722   0.936975 0.981008\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 0.3875, 'reg_lambda': 0.1, 'reg_alpha': 0.1, 'num_leaves': 130, 'n_estimators': 135, 'min_child_samples': 92, 'max_depth': 3, 'learning_rate': 0.2, 'colsample_bytree': 0.1625}\n",
      "\n",
      "Running LightGBM with LDA + SMOTETomek configuration...\n",
      "\n",
      "LightGBM Model Performance Metrics\n",
      "Configuration Name:  LDA + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.948375  0.948450 0.948417   0.948701 0.986987\n",
      "    Test  0.906667  0.867778 0.877904   0.862963 0.966212\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 0.3875, 'reg_lambda': 0.1, 'reg_alpha': 0.1, 'num_leaves': 50, 'n_estimators': 290, 'min_child_samples': 20, 'max_depth': 5, 'learning_rate': 0.2, 'colsample_bytree': 0.05}\n",
      "\n",
      "Running LightGBM with Normalized Data with MinMaxScaler configuration...\n",
      "\n",
      "LightGBM Model Performance Metrics\n",
      "Configuration Name:  Normalized Data with MinMaxScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.916388  0.901116 0.896836   0.906622 0.966021\n",
      "    Test  0.973333  0.956944 0.956944   0.956944 0.996753\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 0.3875, 'reg_lambda': 0.1, 'reg_alpha': 0.1, 'num_leaves': 25, 'n_estimators': 250, 'min_child_samples': 68, 'max_depth': 9, 'learning_rate': 0.2, 'colsample_bytree': 0.05}\n",
      "\n",
      "Running LightGBM with SMOTETomek + MiMaxScaler configuration...\n",
      "\n",
      "LightGBM Model Performance Metrics\n",
      "Configuration Name:  SMOTETomek + MiMaxScaler\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.954198  0.954270 0.954209   0.954497 0.994696\n",
      "    Test  0.946667  0.921421 0.927146   0.916340 0.980414\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 0.1625, 'reg_lambda': 0.1, 'reg_alpha': 0.1, 'num_leaves': 45, 'n_estimators': 120, 'min_child_samples': 5, 'max_depth': 3, 'learning_rate': 0.2, 'colsample_bytree': 0.3875}\n",
      "\n",
      "Running LightGBM with Boruta configuration...\n",
      "\n",
      "LightGBM Model Performance Metrics\n",
      "Configuration Name:  Boruta\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.916388  0.901116 0.896836   0.906622 0.977833\n",
      "    Test  0.960000  0.935214 0.934722   0.936975 0.991301\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 0.275, 'reg_lambda': 0.1, 'reg_alpha': 0.1, 'num_leaves': 25, 'n_estimators': 320, 'min_child_samples': 65, 'max_depth': 7, 'learning_rate': 0.2, 'colsample_bytree': 0.5}\n",
      "\n",
      "Running LightGBM with Boruta + SMOTETomek configuration...\n",
      "\n",
      "LightGBM Model Performance Metrics\n",
      "Configuration Name:  Boruta + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.950287  0.950383 0.950322   0.950608 0.989493\n",
      "    Test  0.933333  0.901219 0.907702   0.902116 0.988795\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 0.275, 'reg_lambda': 0.1, 'reg_alpha': 0.1, 'num_leaves': 120, 'n_estimators': 340, 'min_child_samples': 47, 'max_depth': 3, 'learning_rate': 0.2, 'colsample_bytree': 0.05}\n",
      "\n",
      "Running LightGBM with Autoencoder configuration...\n",
      "\n",
      "LightGBM Model Performance Metrics\n",
      "Configuration Name:  Autoencoder\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.919732  0.904644 0.898740   0.911284 0.981426\n",
      "    Test  0.920000  0.900126 0.900126   0.900126 0.976740\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 0.5, 'reg_lambda': 0.1, 'reg_alpha': 0.1, 'num_leaves': 110, 'n_estimators': 235, 'min_child_samples': 74, 'max_depth': 3, 'learning_rate': 0.2, 'colsample_bytree': 0.1625}\n",
      "\n",
      "Running LightGBM with Autoencoder + SMOTETomek configuration...\n",
      "\n",
      "LightGBM Model Performance Metrics\n",
      "Configuration Name:  Autoencoder + SMOTETomek\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.934866  0.935053 0.934875   0.935602 0.989884\n",
      "    Test  0.933333  0.916762 0.934217   0.902778 0.987324\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'subsample': 0.5, 'reg_lambda': 0.1, 'reg_alpha': 0.1, 'num_leaves': 55, 'n_estimators': 430, 'min_child_samples': 32, 'max_depth': 13, 'learning_rate': 0.01, 'colsample_bytree': 0.5}\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set LightGBM verbosity to suppress training logs\n",
    "os.environ['LIGHTGBM_VERBOSITY'] = '-1'\n",
    "\n",
    "configurations = []\n",
    "\n",
    "\n",
    "configurations.append(('Original Data', X_train, X_test, y_train))\n",
    "configurations.append(('Normalized Data with RobustScaler', X_train_robust, X_test_robust, y_train))\n",
    "configurations.append(('SMOTETomek + RobustScaler', X_train_robust_resample, X_test_robust, y_train_robust_resample))\n",
    "configurations.append(('MI', X_train_mi, X_test_mi, y_train))\n",
    "configurations.append(('MI + SMOTETomek', X_train_mi_res, X_test_mi, y_train_mi_res))\n",
    "configurations.append(('LDA', X_train_lda, X_test_lda, y_train))\n",
    "configurations.append(('LDA + SMOTETomek', X_train_lda_res, X_test_lda, y_train_lda_res))\n",
    "configurations.append(('Normalized Data with MinMaxScaler', X_train_minmax, X_test_minmax, y_train))\n",
    "configurations.append(('SMOTETomek + MiMaxScaler', X_train_minmax_resample, X_test_minmax, y_train_minmax_resample))\n",
    "configurations.append(('Boruta', X_train_boruta, X_test_boruta, y_train))\n",
    "configurations.append(('Boruta + SMOTETomek', X_train_boruta_res, X_test_boruta, y_train_boruta_res))\n",
    "configurations.append(('Autoencoder', X_train_encoded, X_test_encoded, y_train))\n",
    "configurations.append(('Autoencoder + SMOTETomek', X_train_encoded_res, X_test_encoded, y_train_encoded_res))\n",
    "\n",
    "selected_features = boruta_selector.support_\n",
    "optimal_features = sum(selected_features)\n",
    "print(f\"Optimal number of features to select using Boruta: {optimal_features}\")\n",
    "\n",
    "# Step 4: LightGBM + GridSearchCV\n",
    "print(\"\\n=== LightGBM Model Performance with Hyperparameter Tuning ===\")\n",
    "\n",
    "param_grid = {\n",
    "    'num_leaves': range(20, 150, 5),               # Number of leaves in the tree\n",
    "    'max_depth': range(3, 15, 2),                 # Maximum depth of the tree\n",
    "    'learning_rate': np.linspace(0.01, 0.2, 2),         # Learning rate\n",
    "    'n_estimators': range(50, 500, 5),            # Number of boosting iterations\n",
    "    'min_child_samples': range(2, 100, 3),       # Minimum data in a leaf\n",
    "    'subsample': np.linspace(0.05, 0.5, 5),              # Fraction of data to be used for training\n",
    "    'colsample_bytree': np.linspace(0.05, 0.5, 5),       # Fraction of features to be used for training\n",
    "    'reg_alpha': np.linspace(0.1, 1, 1),\n",
    "    'reg_lambda': np.linspace(0.1, 1, 1),\n",
    "}\n",
    "\n",
    "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
    "    print(f\"\\nRunning LightGBM with {name} configuration...\")\n",
    "    cv = StratifiedKFold(n_splits=8, shuffle=True, random_state=42)\n",
    "    lgbm = RandomizedSearchCV(LGBMClassifier(verbose=-1), param_grid, cv=cv, n_iter=50, n_jobs=-1,\n",
    "                             scoring=['accuracy', 'f1_macro'], refit='accuracy', verbose=0)\n",
    "    lgbm.fit(X_train_cfg, y_train_cfg)\n",
    "\n",
    "    y_train_lg = lgbm.predict(X_train_cfg)\n",
    "    y_test_lg = lgbm.predict(X_test_cfg)\n",
    "    y_train_lg_proba = lgbm.predict_proba(X_train_cfg)\n",
    "    y_test_lg_proba = lgbm.predict_proba(X_test_cfg)\n",
    "\n",
    "    metrics_dict = {\n",
    "          \"Dataset\": [\"Training\", \"Test\"],\n",
    "          \"Accuracy\": [\n",
    "              metrics.accuracy_score(y_train_cfg, y_train_lg),\n",
    "              metrics.accuracy_score(y_test, y_test_lg),\n",
    "          ],\n",
    "          \"F1 Score\": [\n",
    "              metrics.f1_score(y_train_cfg, y_train_lg, average='macro'),\n",
    "              metrics.f1_score(y_test, y_test_lg, average='macro'),\n",
    "          ],\n",
    "          \"Recall\": [\n",
    "              metrics.recall_score(y_train_cfg, y_train_lg, average='macro'),\n",
    "              metrics.recall_score(y_test, y_test_lg, average='macro'),\n",
    "          ],\n",
    "          \"Precision\": [\n",
    "              metrics.precision_score(y_train_cfg, y_train_lg, average='macro'),\n",
    "              metrics.precision_score(y_test, y_test_lg, average='macro'),\n",
    "          ],\n",
    "          \"AUC-ROC\": [\n",
    "              metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_lg_proba, multi_class='ovr', average='macro'),\n",
    "              metrics.roc_auc_score(pd.get_dummies(y_test), y_test_lg_proba, multi_class='ovr', average='macro'),\n",
    "          ]\n",
    "      }\n",
    "\n",
    "    df_metrics = pd.DataFrame(metrics_dict)\n",
    "    print(\"\\nLightGBM Model Performance Metrics\")\n",
    "    print(\"Configuration Name: \", name)\n",
    "    print(df_metrics.to_string(index=False))\n",
    "\n",
    "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_lg_proba, multi_class='ovr', average='macro')\n",
    "    storeResults(\n",
    "          'LightGBM',\n",
    "          name,\n",
    "          metrics.accuracy_score(y_test, y_test_lg),\n",
    "          metrics.f1_score(y_test, y_test_lg, average='macro'),\n",
    "          metrics.recall_score(y_test, y_test_lg, average='macro'),\n",
    "          metrics.precision_score(y_test, y_test_lg, average='macro'),\n",
    "          auc_score\n",
    "      )\n",
    "    storeBestParams(name, lgbm.best_params_, 'LightGBM')\n",
    "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
    "    print(lgbm.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad38f0c",
   "metadata": {
    "id": "cGw9YygPuPPr",
    "papermill": {
     "duration": 0.019847,
     "end_time": "2025-10-16T10:06:00.965302",
     "exception": false,
     "start_time": "2025-10-16T10:06:00.945455",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Stacking classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f517ebe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T10:06:01.005808Z",
     "iopub.status.busy": "2025-10-16T10:06:01.005521Z",
     "iopub.status.idle": "2025-10-16T10:06:01.013508Z",
     "shell.execute_reply": "2025-10-16T10:06:01.012929Z"
    },
    "id": "TPcypVO3uSfZ",
    "outputId": "cc09783d-3609-4860-8d62-0caa313e10f4",
    "papermill": {
     "duration": 0.029677,
     "end_time": "2025-10-16T10:06:01.014544",
     "exception": false,
     "start_time": "2025-10-16T10:06:00.984867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nconfigurations = []\\nconfigurations.append((\\'Original Data\\', X_train, X_test, y_train))\\nconfigurations.append((\\'Normalized Data with RobustScaler\\', X_train_robust, X_test_robust, y_train))\\nconfigurations.append((\\'SMOTETomek + RobustScaler\\', X_train_robust_resample, X_test_robust, y_train_robust_resample))\\nconfigurations.append((\\'MI\\', X_train_mi, X_test_mi, y_train))\\nconfigurations.append((\\'MI + SMOTETomek\\', X_train_mi_res, X_test_mi, y_train_mi_res))\\nconfigurations.append((\\'LDA\\', X_train_lda, X_test_lda, y_train))\\nconfigurations.append((\\'LDA + SMOTETomek\\', X_train_lda_res, X_test_lda, y_train_lda_res))\\nconfigurations.append((\\'Normalized Data with MinMaxScaler\\', X_train_minmax, X_test_minmax, y_train))\\nconfigurations.append((\\'SMOTETomek + MiMaxScaler\\', X_train_minmax_resample, X_test_minmax, y_train_minmax_resample))\\nconfigurations.append((\\'Boruta\\', X_train_boruta, X_test_boruta, y_train))\\nconfigurations.append((\\'Boruta + SMOTETomek\\', X_train_boruta_res, X_test_boruta, y_train_boruta_res))\\nconfigurations.append((\\'Autoencoder\\', X_train_encoded, X_test_encoded, y_train))\\nconfigurations.append((\\'Autoencoder + SMOTETomek\\', X_train_encoded_res, X_test_encoded, y_train_encoded_res))\\n\\nxgb_params = {\\'subsample\\': np.float64(0.1), \\'skip_drop\\': 0, \\'scale_pos_weight\\': 2, \\'sample_type\\': \\'weighted\\',\\n              \\'reg_lambda\\': np.float64(0.1), \\'reg_alpha\\': np.float64(0.1), \\'rate_drop\\': 0.1,\\n              \\'normalize_type\\': \\'forest\\', \\'n_estimators\\': 410, \\'min_child_weight\\': 1, \\'max_depth\\': 2,\\n              \\'max_delta_step\\': 5, \\'learning_rate\\': np.float64(0.07780000000000001), \\'gamma\\': np.float64(0.05),\\n              \\'estimator__n_estimators\\': 150, \\'colsample_bytree\\': 0.3, \\'colsample_bynode\\': 0.8,\\n              \\'colsample_bylevel\\': 1.0, \\'booster\\': \\'gbtree\\'}\\n\\nrf_params = {\\'n_estimators\\': 142, \\'min_samples_split\\': 4, \\'min_samples_leaf\\': 5,\\n             \\'min_impurity_decrease\\': np.float64(0.001), \\'max_leaf_nodes\\': 25, \\'max_features\\': \\'sqrt\\',\\n             \\'max_depth\\': 14, \\'criterion\\': \\'entropy\\', \\'class_weight\\': \\'balanced\\', \\'ccp_alpha\\': np.float64(0.001),\\n             \\'bootstrap\\': False}\\n\\ngbc_params = {\\'warm_start\\': False, \\'verbose\\': 0, \\'validation_fraction\\': 0.1, \\'tol\\': np.float64(0.05),\\n              \\'subsample\\': np.float64(0.1), \\'n_iter_no_change\\': None, \\'n_estimators\\': 380,\\n              \\'min_weight_fraction_leaf\\': 0.0, \\'min_samples_split\\': 14, \\'min_samples_leaf\\': 5,\\n              \\'min_impurity_decrease\\': 0.0, \\'max_leaf_nodes\\': None, \\'max_features\\': \\'sqrt\\', \\'max_depth\\': 50, \\'loss\\': \\'log_loss\\',\\n              \\'learning_rate\\': np.float64(0.1), \\'init\\': None, \\'criterion\\': \\'friedman_mse\\', \\'ccp_alpha\\': np.float64(0.005)}\\n\\nknn_params = {\\'weights\\': \\'distance\\', \\'p\\': np.int64(1), \\'n_neighbors\\': np.int64(11), \\'metric\\': \\'euclidean\\'}\\n\\nlogr_params = {\\'solver\\': \\'liblinear\\', \\'penalty\\': \\'l1\\', \\'max_iter\\': 970, \\'C\\': np.float64(66.67)}\\n\\netc_params = {\\'oob_score\\': True, \\'n_estimators\\': 482, \\'min_weight_fraction_leaf\\': 0.0, \\'min_samples_split\\': 3,\\n              \\'min_samples_leaf\\': 2, \\'min_impurity_decrease\\': 0.0, \\'max_leaf_nodes\\': 38, \\'max_features\\': \\'log2\\',\\n              \\'max_depth\\': 48, \\'criterion\\': \\'log_loss\\', \\'class_weight\\': None, \\'ccp_alpha\\': np.float64(0.001),\\n              \\'bootstrap\\': True}\\nbase_dt = DecisionTreeClassifier(max_depth=5, min_samples_split=4)\\n# adb_params = {\\'random_state\\': 54, \\'n_estimators\\': 165, \\'learning_rate\\': np.float64(0.01)}\\n\\nestimators = [\\n    (\\'xgb\\', XGBClassifier(**xgb_params)),\\n    (\\'rf\\', RandomForestClassifier(**rf_params)),\\n    (\\'gbc\\', GradientBoostingClassifier(**gbc_params)),\\n    (\\'knn\\', KNeighborsClassifier(**knn_params)),\\n    (\\'etc\\', ExtraTreesClassifier(**etc_params)),\\n    (\\'adb\\', AdaBoostClassifier(estimator=base_dt, random_state=54, n_estimators=165, learning_rate=0.01)),\\n    (\\'logr\\', LogisticRegression(**logr_params))\\n]\\n\\nlog_reg_params = {\\'solver\\': \\'liblinear\\', \\'penalty\\': \\'l2\\', \\'max_iter\\': 816, \\'C\\': np.float64(0.01)}\\nstacking_clf = StackingClassifier(\\n    estimators=estimators,\\n    final_estimator=LogisticRegression(**log_reg_params),\\n    cv = 5,\\n    stack_method = \"predict_proba\",\\n    n_jobs=-1\\n    )\\n\\nfor name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\\n    print(f\"\\nRunning Stacking Classifier with {name} configuration...\")\\n    stacking_clf.fit(X_train_cfg, y_train_cfg)\\n\\n    y_train_stc = stacking_clf.predict(X_train_cfg)\\n    y_test_stc = stacking_clf.predict(X_test_cfg)\\n    y_train_stc_proba = stacking_clf.predict_proba(X_train_cfg)\\n    y_test_stc_proba = stacking_clf.predict_proba(X_test_cfg)\\n\\n    metrics_dict = {\\n          \"Dataset\": [\"Training\", \"Test\"],\\n          \"Accuracy\": [\\n              metrics.accuracy_score(y_train_cfg, y_train_stc),\\n              metrics.accuracy_score(y_test, y_test_stc),\\n          ],\\n          \"F1 Score\": [\\n              metrics.f1_score(y_train_cfg, y_train_stc, average=\\'macro\\'),\\n              metrics.f1_score(y_test, y_test_stc, average=\\'macro\\'),\\n          ],\\n          \"Recall\": [\\n              metrics.recall_score(y_train_cfg, y_train_stc, average=\\'macro\\'),\\n              metrics.recall_score(y_test, y_test_stc, average=\\'macro\\'),\\n          ],\\n          \"Precision\": [\\n              metrics.precision_score(y_train_cfg, y_train_stc, average=\\'macro\\'),\\n              metrics.precision_score(y_test, y_test_stc, average=\\'macro\\'),\\n          ],\\n          \"AUC-ROC\": [\\n              metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_stc_proba, multi_class=\\'ovr\\', average=\\'macro\\'),\\n              metrics.roc_auc_score(pd.get_dummies(y_test), y_test_stc_proba, multi_class=\\'ovr\\', average=\\'macro\\'),\\n          ]\\n      }\\n\\n    df_metrics = pd.DataFrame(metrics_dict)\\n    print(\"\\\\Stacking Classifier Performance Metrics\")\\n    print(\"Configuration Name: \", name)\\n    print(df_metrics.to_string(index=False))\\n\\n    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_stc_proba, multi_class=\\'ovr\\', average=\\'macro\\')\\n    storeResults(\\n          \\'Stacking Classifier\\',\\n          name,\\n          metrics.accuracy_score(y_test, y_test_stc),\\n          metrics.f1_score(y_test, y_test_stc, average=\\'macro\\'),\\n          metrics.recall_score(y_test, y_test_stc, average=\\'macro\\'),\\n          metrics.precision_score(y_test, y_test_stc, average=\\'macro\\'),\\n          auc_score\\n      )\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "configurations = []\n",
    "configurations.append(('Original Data', X_train, X_test, y_train))\n",
    "configurations.append(('Normalized Data with RobustScaler', X_train_robust, X_test_robust, y_train))\n",
    "configurations.append(('SMOTETomek + RobustScaler', X_train_robust_resample, X_test_robust, y_train_robust_resample))\n",
    "configurations.append(('MI', X_train_mi, X_test_mi, y_train))\n",
    "configurations.append(('MI + SMOTETomek', X_train_mi_res, X_test_mi, y_train_mi_res))\n",
    "configurations.append(('LDA', X_train_lda, X_test_lda, y_train))\n",
    "configurations.append(('LDA + SMOTETomek', X_train_lda_res, X_test_lda, y_train_lda_res))\n",
    "configurations.append(('Normalized Data with MinMaxScaler', X_train_minmax, X_test_minmax, y_train))\n",
    "configurations.append(('SMOTETomek + MiMaxScaler', X_train_minmax_resample, X_test_minmax, y_train_minmax_resample))\n",
    "configurations.append(('Boruta', X_train_boruta, X_test_boruta, y_train))\n",
    "configurations.append(('Boruta + SMOTETomek', X_train_boruta_res, X_test_boruta, y_train_boruta_res))\n",
    "configurations.append(('Autoencoder', X_train_encoded, X_test_encoded, y_train))\n",
    "configurations.append(('Autoencoder + SMOTETomek', X_train_encoded_res, X_test_encoded, y_train_encoded_res))\n",
    "\n",
    "xgb_params = {'subsample': np.float64(0.1), 'skip_drop': 0, 'scale_pos_weight': 2, 'sample_type': 'weighted',\n",
    "              'reg_lambda': np.float64(0.1), 'reg_alpha': np.float64(0.1), 'rate_drop': 0.1,\n",
    "              'normalize_type': 'forest', 'n_estimators': 410, 'min_child_weight': 1, 'max_depth': 2,\n",
    "              'max_delta_step': 5, 'learning_rate': np.float64(0.07780000000000001), 'gamma': np.float64(0.05),\n",
    "              'estimator__n_estimators': 150, 'colsample_bytree': 0.3, 'colsample_bynode': 0.8,\n",
    "              'colsample_bylevel': 1.0, 'booster': 'gbtree'}\n",
    "\n",
    "rf_params = {'n_estimators': 142, 'min_samples_split': 4, 'min_samples_leaf': 5,\n",
    "             'min_impurity_decrease': np.float64(0.001), 'max_leaf_nodes': 25, 'max_features': 'sqrt',\n",
    "             'max_depth': 14, 'criterion': 'entropy', 'class_weight': 'balanced', 'ccp_alpha': np.float64(0.001),\n",
    "             'bootstrap': False}\n",
    "\n",
    "gbc_params = {'warm_start': False, 'verbose': 0, 'validation_fraction': 0.1, 'tol': np.float64(0.05),\n",
    "              'subsample': np.float64(0.1), 'n_iter_no_change': None, 'n_estimators': 380,\n",
    "              'min_weight_fraction_leaf': 0.0, 'min_samples_split': 14, 'min_samples_leaf': 5,\n",
    "              'min_impurity_decrease': 0.0, 'max_leaf_nodes': None, 'max_features': 'sqrt', 'max_depth': 50, 'loss': 'log_loss',\n",
    "              'learning_rate': np.float64(0.1), 'init': None, 'criterion': 'friedman_mse', 'ccp_alpha': np.float64(0.005)}\n",
    "\n",
    "knn_params = {'weights': 'distance', 'p': np.int64(1), 'n_neighbors': np.int64(11), 'metric': 'euclidean'}\n",
    "\n",
    "logr_params = {'solver': 'liblinear', 'penalty': 'l1', 'max_iter': 970, 'C': np.float64(66.67)}\n",
    "\n",
    "etc_params = {'oob_score': True, 'n_estimators': 482, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 3,\n",
    "              'min_samples_leaf': 2, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': 38, 'max_features': 'log2',\n",
    "              'max_depth': 48, 'criterion': 'log_loss', 'class_weight': None, 'ccp_alpha': np.float64(0.001),\n",
    "              'bootstrap': True}\n",
    "base_dt = DecisionTreeClassifier(max_depth=5, min_samples_split=4)\n",
    "# adb_params = {'random_state': 54, 'n_estimators': 165, 'learning_rate': np.float64(0.01)}\n",
    "\n",
    "estimators = [\n",
    "    ('xgb', XGBClassifier(**xgb_params)),\n",
    "    ('rf', RandomForestClassifier(**rf_params)),\n",
    "    ('gbc', GradientBoostingClassifier(**gbc_params)),\n",
    "    ('knn', KNeighborsClassifier(**knn_params)),\n",
    "    ('etc', ExtraTreesClassifier(**etc_params)),\n",
    "    ('adb', AdaBoostClassifier(estimator=base_dt, random_state=54, n_estimators=165, learning_rate=0.01)),\n",
    "    ('logr', LogisticRegression(**logr_params))\n",
    "]\n",
    "\n",
    "log_reg_params = {'solver': 'liblinear', 'penalty': 'l2', 'max_iter': 816, 'C': np.float64(0.01)}\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LogisticRegression(**log_reg_params),\n",
    "    cv = 5,\n",
    "    stack_method = \"predict_proba\",\n",
    "    n_jobs=-1\n",
    "    )\n",
    "\n",
    "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
    "    print(f\"\\nRunning Stacking Classifier with {name} configuration...\")\n",
    "    stacking_clf.fit(X_train_cfg, y_train_cfg)\n",
    "\n",
    "    y_train_stc = stacking_clf.predict(X_train_cfg)\n",
    "    y_test_stc = stacking_clf.predict(X_test_cfg)\n",
    "    y_train_stc_proba = stacking_clf.predict_proba(X_train_cfg)\n",
    "    y_test_stc_proba = stacking_clf.predict_proba(X_test_cfg)\n",
    "\n",
    "    metrics_dict = {\n",
    "          \"Dataset\": [\"Training\", \"Test\"],\n",
    "          \"Accuracy\": [\n",
    "              metrics.accuracy_score(y_train_cfg, y_train_stc),\n",
    "              metrics.accuracy_score(y_test, y_test_stc),\n",
    "          ],\n",
    "          \"F1 Score\": [\n",
    "              metrics.f1_score(y_train_cfg, y_train_stc, average='macro'),\n",
    "              metrics.f1_score(y_test, y_test_stc, average='macro'),\n",
    "          ],\n",
    "          \"Recall\": [\n",
    "              metrics.recall_score(y_train_cfg, y_train_stc, average='macro'),\n",
    "              metrics.recall_score(y_test, y_test_stc, average='macro'),\n",
    "          ],\n",
    "          \"Precision\": [\n",
    "              metrics.precision_score(y_train_cfg, y_train_stc, average='macro'),\n",
    "              metrics.precision_score(y_test, y_test_stc, average='macro'),\n",
    "          ],\n",
    "          \"AUC-ROC\": [\n",
    "              metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_stc_proba, multi_class='ovr', average='macro'),\n",
    "              metrics.roc_auc_score(pd.get_dummies(y_test), y_test_stc_proba, multi_class='ovr', average='macro'),\n",
    "          ]\n",
    "      }\n",
    "\n",
    "    df_metrics = pd.DataFrame(metrics_dict)\n",
    "    print(\"\\Stacking Classifier Performance Metrics\")\n",
    "    print(\"Configuration Name: \", name)\n",
    "    print(df_metrics.to_string(index=False))\n",
    "\n",
    "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_stc_proba, multi_class='ovr', average='macro')\n",
    "    storeResults(\n",
    "          'Stacking Classifier',\n",
    "          name,\n",
    "          metrics.accuracy_score(y_test, y_test_stc),\n",
    "          metrics.f1_score(y_test, y_test_stc, average='macro'),\n",
    "          metrics.recall_score(y_test, y_test_stc, average='macro'),\n",
    "          metrics.precision_score(y_test, y_test_stc, average='macro'),\n",
    "          auc_score\n",
    "      )\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f31f98b",
   "metadata": {
    "id": "DZjM6oTkrnlq",
    "papermill": {
     "duration": 0.019963,
     "end_time": "2025-10-16T10:06:01.054861",
     "exception": false,
     "start_time": "2025-10-16T10:06:01.034898",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cd1ee0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T10:06:01.096673Z",
     "iopub.status.busy": "2025-10-16T10:06:01.096300Z",
     "iopub.status.idle": "2025-10-16T10:06:01.134577Z",
     "shell.execute_reply": "2025-10-16T10:06:01.133910Z"
    },
    "id": "Jgo6Ddxwr281",
    "outputId": "0153a636-8676-4863-9cae-74d8626ef10c",
    "papermill": {
     "duration": 0.061194,
     "end_time": "2025-10-16T10:06:01.135832",
     "exception": false,
     "start_time": "2025-10-16T10:06:01.074638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "MODEL PERFORMANCE RESULTS\n",
      "====================================================================================================\n",
      "           ML Model                     Configuration Accuracy F1 Score  Recall Precision ROC_AUC\n",
      "Logistic Regression                     Original Data  97.333%  95.658% 95.556%   96.296% 99.818%\n",
      "Logistic Regression Normalized Data with RobustScaler  94.667%  92.142% 92.715%   91.634% 99.781%\n",
      "Logistic Regression         SMOTETomek + RobustScaler  96.000%  94.379% 94.798%   94.737% 99.263%\n",
      "Logistic Regression                                MI  94.667%  92.224% 92.853%   91.667% 97.354%\n",
      "Logistic Regression                   MI + SMOTETomek  82.667%  80.626% 84.571%   79.625% 95.948%\n",
      "Logistic Regression                               LDA  92.000%  89.715% 91.338%   89.006% 95.821%\n",
      "Logistic Regression                  LDA + SMOTETomek  81.333%  79.893% 83.813%   79.833% 95.357%\n",
      "Logistic Regression Normalized Data with MinMaxScaler  97.333%  95.658% 95.556%   96.296% 99.929%\n",
      "Logistic Regression          SMOTETomek + MiMaxScaler  94.667%  93.744% 95.505%   92.515% 98.152%\n",
      "Logistic Regression                            Boruta  93.333%  90.093% 90.492%   90.418% 97.255%\n",
      "Logistic Regression               Boruta + SMOTETomek  93.333%  91.015% 91.957%   90.602% 95.970%\n",
      "Logistic Regression                       Autoencoder  94.667%  93.598% 94.179%   93.081% 97.350%\n",
      "Logistic Regression          Autoencoder + SMOTETomek  89.333%  86.191% 88.220%   85.000% 96.564%\n",
      "K-Nearest Neighbors                     Original Data  92.000%  89.526% 91.200%   88.194% 95.163%\n",
      "K-Nearest Neighbors Normalized Data with RobustScaler  94.667%  92.142% 92.715%   91.634% 95.625%\n",
      "K-Nearest Neighbors         SMOTETomek + RobustScaler  89.333%  85.604% 87.033%   85.029% 95.843%\n",
      "K-Nearest Neighbors                                MI  96.000%  94.308% 94.937%   93.750% 96.203%\n",
      "K-Nearest Neighbors                   MI + SMOTETomek  98.667%  97.850% 97.917%   97.917% 98.178%\n",
      "K-Nearest Neighbors                               LDA  96.000%  93.521% 93.472%   93.698% 96.164%\n",
      "K-Nearest Neighbors                  LDA + SMOTETomek  96.000%  93.521% 93.472%   93.698% 96.381%\n",
      "K-Nearest Neighbors Normalized Data with MinMaxScaler  92.000%  91.276% 91.200%   91.477% 98.443%\n",
      "K-Nearest Neighbors          SMOTETomek + MiMaxScaler  88.000%  85.439% 87.740%   85.498% 94.755%\n",
      "K-Nearest Neighbors                            Boruta  93.333%  92.096% 92.096%   92.602% 98.719%\n",
      "K-Nearest Neighbors               Boruta + SMOTETomek  88.000%  84.320% 86.275%   82.898% 95.427%\n",
      "K-Nearest Neighbors                       Autoencoder  93.333%  92.324% 93.422%   91.799% 98.706%\n",
      "K-Nearest Neighbors          Autoencoder + SMOTETomek  89.333%  87.348% 88.498%   87.260% 97.073%\n",
      "      Random Forest                     Original Data  94.667%  91.998% 92.576%   92.173% 99.563%\n",
      "      Random Forest Normalized Data with RobustScaler  94.667%  91.317% 91.250%   91.880% 99.563%\n",
      "      Random Forest         SMOTETomek + RobustScaler  96.000%  93.521% 93.472%   93.698% 99.707%\n",
      "      Random Forest                                MI  93.333%  90.122% 90.631%   89.673% 99.495%\n",
      "      Random Forest                   MI + SMOTETomek  93.333%  90.074% 90.631%   89.583% 99.349%\n",
      "      Random Forest                               LDA  96.000%  93.521% 93.472%   93.698% 97.712%\n",
      "      Random Forest                  LDA + SMOTETomek  93.333%  89.948% 90.492%   89.683% 98.867%\n",
      "      Random Forest Normalized Data with MinMaxScaler  94.667%  91.998% 92.576%   92.173% 99.563%\n",
      "      Random Forest          SMOTETomek + MiMaxScaler  97.333%  95.658% 95.556%   96.296% 99.525%\n",
      "      Random Forest                            Boruta  94.667%  93.081% 94.179%   92.222% 99.712%\n",
      "      Random Forest               Boruta + SMOTETomek  96.000%  95.042% 96.263%   94.074% 99.487%\n",
      "      Random Forest                       Autoencoder  96.000%  95.218% 96.263%   94.737% 99.254%\n",
      "      Random Forest          Autoencoder + SMOTETomek  93.333%  91.627% 93.422%   90.196% 99.180%\n",
      "      XGBoost Model                     Original Data  97.333%  95.694% 95.694%   95.694% 99.347%\n",
      "      XGBoost Model Normalized Data with RobustScaler  97.333%  95.694% 95.694%   95.694% 99.783%\n",
      "      XGBoost Model         SMOTETomek + RobustScaler  96.000%  93.521% 93.472%   93.698% 99.312%\n",
      "      XGBoost Model                                MI  94.667%  91.389% 91.389%   91.389% 99.347%\n",
      "      XGBoost Model                   MI + SMOTETomek  93.333%  90.074% 90.631%   89.583% 98.874%\n",
      "      XGBoost Model                               LDA  94.667%  91.317% 91.250%   91.880% 99.338%\n",
      "      XGBoost Model                  LDA + SMOTETomek  94.667%  91.317% 91.250%   91.880% 99.196%\n",
      "      XGBoost Model Normalized Data with MinMaxScaler  97.333%  95.694% 95.694%   95.694% 99.714%\n",
      "      XGBoost Model          SMOTETomek + MiMaxScaler  97.333%  95.694% 95.694%   95.694% 99.307%\n",
      "      XGBoost Model                            Boruta  96.000%  93.521% 93.472%   93.698% 99.712%\n",
      "      XGBoost Model               Boruta + SMOTETomek  97.333%  95.694% 95.694%   95.694% 99.233%\n",
      "      XGBoost Model                       Autoencoder  93.333%  92.285% 92.096%   92.602% 99.291%\n",
      "      XGBoost Model          Autoencoder + SMOTETomek  94.667%  93.081% 94.179%   92.222% 98.618%\n",
      "     XGBoost2 Model                     Original Data  96.000%  93.521% 93.472%   93.698% 99.640%\n",
      "     XGBoost2 Model Normalized Data with RobustScaler  97.333%  95.694% 95.694%   95.694% 99.638%\n",
      "     XGBoost2 Model         SMOTETomek + RobustScaler  96.000%  93.521% 93.472%   93.698% 99.423%\n",
      "     XGBoost2 Model                                MI  93.333%  89.202% 89.167%   89.356% 99.421%\n",
      "     XGBoost2 Model                   MI + SMOTETomek  98.667%  97.850% 97.917%   97.917% 99.421%\n",
      "     XGBoost2 Model                               LDA  94.667%  91.317% 91.250%   91.880% 99.450%\n",
      "     XGBoost2 Model                  LDA + SMOTETomek  94.667%  92.142% 92.715%   91.634% 99.351%\n",
      "     XGBoost2 Model Normalized Data with MinMaxScaler  97.333%  95.694% 95.694%   95.694% 99.712%\n",
      "     XGBoost2 Model          SMOTETomek + MiMaxScaler  96.000%  93.521% 93.472%   93.698% 99.270%\n",
      "     XGBoost2 Model                            Boruta  93.333%  89.247% 89.306%   89.306% 99.176%\n",
      "     XGBoost2 Model               Boruta + SMOTETomek  94.667%  92.593% 91.528%   93.948% 99.196%\n",
      "     XGBoost2 Model                       Autoencoder  93.333%  92.285% 92.096%   92.602% 99.184%\n",
      "     XGBoost2 Model          Autoencoder + SMOTETomek  96.000%  94.364% 94.937%   93.856% 98.687%\n",
      "  Gradient Boosting                     Original Data  97.333%  96.328% 95.833%   97.176% 99.859%\n",
      "  Gradient Boosting Normalized Data with RobustScaler  96.000%  94.177% 93.611%   94.815% 99.603%\n",
      "  Gradient Boosting         SMOTETomek + RobustScaler  96.000%  94.379% 94.798%   94.737% 99.271%\n",
      "  Gradient Boosting                                MI  94.667%  91.389% 91.389%   91.389% 99.603%\n",
      "  Gradient Boosting                   MI + SMOTETomek  93.333%  90.074% 90.631%   89.583% 99.312%\n",
      "  Gradient Boosting                               LDA  96.000%  93.548% 93.611%   93.611% 98.033%\n",
      "  Gradient Boosting                  LDA + SMOTETomek  93.333%  89.948% 90.492%   89.683% 98.576%\n",
      "  Gradient Boosting Normalized Data with MinMaxScaler  94.667%  92.000% 91.250%   93.704% 99.315%\n",
      "  Gradient Boosting          SMOTETomek + MiMaxScaler  96.000%  93.521% 93.472%   93.698% 99.453%\n",
      "  Gradient Boosting                            Boruta  96.000%  93.521% 93.472%   93.698% 99.458%\n",
      "  Gradient Boosting               Boruta + SMOTETomek  93.333%  90.122% 90.770%   90.212% 98.918%\n",
      "  Gradient Boosting                       Autoencoder  92.000%  90.728% 90.013%   91.528% 98.126%\n",
      "  Gradient Boosting          Autoencoder + SMOTETomek  92.000%  90.529% 90.013%   91.111% 98.948%\n",
      "        Extra Trees                     Original Data  96.000%  93.521% 93.472%   93.698% 99.712%\n",
      "        Extra Trees Normalized Data with RobustScaler  97.333%  95.658% 95.556%   96.296% 99.857%\n",
      "        Extra Trees         SMOTETomek + RobustScaler  96.000%  93.521% 93.472%   93.698% 99.603%\n",
      "        Extra Trees                                MI  96.000%  93.521% 93.472%   93.698% 99.640%\n",
      "        Extra Trees                   MI + SMOTETomek  94.667%  91.389% 91.389%   91.389% 99.342%\n",
      "        Extra Trees                               LDA  96.000%  93.521% 93.472%   93.698% 98.645%\n",
      "        Extra Trees                  LDA + SMOTETomek  96.000%  93.548% 93.611%   93.611% 99.426%\n",
      "        Extra Trees Normalized Data with MinMaxScaler  96.000%  93.439% 93.333%   94.737% 99.857%\n",
      "        Extra Trees          SMOTETomek + MiMaxScaler  94.667%  92.210% 92.576%   93.333% 99.414%\n",
      "        Extra Trees                            Boruta  96.000%  93.521% 93.472%   93.698% 99.855%\n",
      "        Extra Trees               Boruta + SMOTETomek  98.667%  97.840% 97.778%   98.039% 99.672%\n",
      "        Extra Trees                       Autoencoder  93.333%  92.285% 92.096%   92.602% 98.954%\n",
      "        Extra Trees          Autoencoder + SMOTETomek  92.000%  91.053% 91.338%   91.042% 99.269%\n",
      "           AdaBoost                     Original Data  93.333%  89.247% 89.306%   89.306% 97.737%\n",
      "           AdaBoost Normalized Data with RobustScaler  94.667%  91.389% 91.528%   91.737% 98.668%\n",
      "           AdaBoost         SMOTETomek + RobustScaler  94.667%  91.389% 91.528%   91.737% 97.389%\n",
      "           AdaBoost                                MI  97.333%  95.694% 95.694%   95.694% 99.785%\n",
      "           AdaBoost                   MI + SMOTETomek  97.333%  95.694% 95.694%   95.694% 97.667%\n",
      "           AdaBoost                               LDA  93.333%  89.247% 89.306%   89.306% 94.731%\n",
      "           AdaBoost                  LDA + SMOTETomek  93.333%  89.247% 89.306%   89.306% 90.165%\n",
      "           AdaBoost Normalized Data with MinMaxScaler  92.000%  87.991% 88.548%   87.712% 97.879%\n",
      "           AdaBoost          SMOTETomek + MiMaxScaler  90.667%  87.712% 89.255%   87.619% 96.501%\n",
      "           AdaBoost                            Boruta  93.333%  89.247% 89.306%   89.306% 98.267%\n",
      "           AdaBoost               Boruta + SMOTETomek  92.000%  89.056% 88.548%   89.630% 94.555%\n",
      "           AdaBoost                       Autoencoder  94.667%  93.780% 94.179%   93.669% 98.641%\n",
      "           AdaBoost          Autoencoder + SMOTETomek  92.000%  89.883% 90.013%   89.874% 94.148%\n",
      "     MLP Classifier                     Original Data  61.333%  32.479% 37.500%   53.425% 53.714%\n",
      "     MLP Classifier Normalized Data with RobustScaler  96.000%  93.439% 93.333%   94.737% 99.529%\n",
      "     MLP Classifier         SMOTETomek + RobustScaler  94.667%  92.210% 92.576%   93.333% 98.749%\n",
      "     MLP Classifier                                MI  96.000%  93.548% 93.611%   93.611% 99.234%\n",
      "     MLP Classifier                   MI + SMOTETomek  93.333%  90.948% 92.096%   90.370% 98.827%\n",
      "     MLP Classifier                               LDA  93.333%  90.122% 90.631%   89.673% 96.060%\n",
      "     MLP Classifier                  LDA + SMOTETomek  93.333%  90.892% 92.096%   89.951% 94.643%\n",
      "     MLP Classifier Normalized Data with MinMaxScaler  92.000%  89.617% 91.200%   88.538% 99.254%\n",
      "     MLP Classifier          SMOTETomek + MiMaxScaler  92.000%  89.557% 91.061%   89.499% 96.749%\n",
      "     MLP Classifier                            Boruta  90.667%  87.523% 89.255%   86.343% 98.531%\n",
      "     MLP Classifier               Boruta + SMOTETomek  89.333%  85.503% 87.033%   84.314% 97.064%\n",
      "     MLP Classifier                       Autoencoder  92.000%  89.883% 90.013%   89.874% 98.423%\n",
      "     MLP Classifier          Autoencoder + SMOTETomek  93.333%  91.676% 93.422%   90.278% 98.048%\n",
      "           LightGBM                     Original Data  97.333%  95.694% 95.694%   95.694% 99.783%\n",
      "           LightGBM Normalized Data with RobustScaler  96.000%  93.521% 93.472%   93.698% 99.638%\n",
      "           LightGBM         SMOTETomek + RobustScaler  96.000%  94.308% 94.937%   93.750% 99.372%\n",
      "           LightGBM                                MI  97.333%  95.694% 95.694%   95.694% 99.677%\n",
      "           LightGBM                   MI + SMOTETomek  98.667%  97.850% 97.917%   97.917% 98.923%\n",
      "           LightGBM                               LDA  96.000%  93.521% 93.472%   93.698% 98.101%\n",
      "           LightGBM                  LDA + SMOTETomek  90.667%  86.778% 87.790%   86.296% 96.621%\n",
      "           LightGBM Normalized Data with MinMaxScaler  97.333%  95.694% 95.694%   95.694% 99.675%\n",
      "           LightGBM          SMOTETomek + MiMaxScaler  94.667%  92.142% 92.715%   91.634% 98.041%\n",
      "           LightGBM                            Boruta  96.000%  93.521% 93.472%   93.698% 99.130%\n",
      "           LightGBM               Boruta + SMOTETomek  93.333%  90.122% 90.770%   90.212% 98.879%\n",
      "           LightGBM                       Autoencoder  92.000%  90.013% 90.013%   90.013% 97.674%\n",
      "           LightGBM          Autoencoder + SMOTETomek  93.333%  91.676% 93.422%   90.278% 98.732%\n",
      "\n",
      "====================================================================================================\n",
      "SORTED MODEL PERFORMANCE RESULTS (by Accuracy and F1 Score)\n",
      "====================================================================================================\n",
      "           ML Model                     Configuration Accuracy F1 Score  Recall Precision ROC_AUC\n",
      "K-Nearest Neighbors                   MI + SMOTETomek  98.667%  97.850% 97.917%   97.917% 98.178%\n",
      "     XGBoost2 Model                   MI + SMOTETomek  98.667%  97.850% 97.917%   97.917% 99.421%\n",
      "           LightGBM                   MI + SMOTETomek  98.667%  97.850% 97.917%   97.917% 98.923%\n",
      "        Extra Trees               Boruta + SMOTETomek  98.667%  97.840% 97.778%   98.039% 99.672%\n",
      "  Gradient Boosting                     Original Data  97.333%  96.328% 95.833%   97.176% 99.859%\n",
      "      XGBoost Model                     Original Data  97.333%  95.694% 95.694%   95.694% 99.347%\n",
      "      XGBoost Model Normalized Data with RobustScaler  97.333%  95.694% 95.694%   95.694% 99.783%\n",
      "      XGBoost Model Normalized Data with MinMaxScaler  97.333%  95.694% 95.694%   95.694% 99.714%\n",
      "      XGBoost Model          SMOTETomek + MiMaxScaler  97.333%  95.694% 95.694%   95.694% 99.307%\n",
      "      XGBoost Model               Boruta + SMOTETomek  97.333%  95.694% 95.694%   95.694% 99.233%\n",
      "     XGBoost2 Model Normalized Data with RobustScaler  97.333%  95.694% 95.694%   95.694% 99.638%\n",
      "     XGBoost2 Model Normalized Data with MinMaxScaler  97.333%  95.694% 95.694%   95.694% 99.712%\n",
      "           AdaBoost                                MI  97.333%  95.694% 95.694%   95.694% 99.785%\n",
      "           AdaBoost                   MI + SMOTETomek  97.333%  95.694% 95.694%   95.694% 97.667%\n",
      "           LightGBM                     Original Data  97.333%  95.694% 95.694%   95.694% 99.783%\n",
      "           LightGBM                                MI  97.333%  95.694% 95.694%   95.694% 99.677%\n",
      "           LightGBM Normalized Data with MinMaxScaler  97.333%  95.694% 95.694%   95.694% 99.675%\n",
      "Logistic Regression                     Original Data  97.333%  95.658% 95.556%   96.296% 99.818%\n",
      "Logistic Regression Normalized Data with MinMaxScaler  97.333%  95.658% 95.556%   96.296% 99.929%\n",
      "      Random Forest          SMOTETomek + MiMaxScaler  97.333%  95.658% 95.556%   96.296% 99.525%\n",
      "        Extra Trees Normalized Data with RobustScaler  97.333%  95.658% 95.556%   96.296% 99.857%\n",
      "      Random Forest                       Autoencoder  96.000%  95.218% 96.263%   94.737% 99.254%\n",
      "      Random Forest               Boruta + SMOTETomek  96.000%  95.042% 96.263%   94.074% 99.487%\n",
      "Logistic Regression         SMOTETomek + RobustScaler  96.000%  94.379% 94.798%   94.737% 99.263%\n",
      "  Gradient Boosting         SMOTETomek + RobustScaler  96.000%  94.379% 94.798%   94.737% 99.271%\n",
      "     XGBoost2 Model          Autoencoder + SMOTETomek  96.000%  94.364% 94.937%   93.856% 98.687%\n",
      "K-Nearest Neighbors                                MI  96.000%  94.308% 94.937%   93.750% 96.203%\n",
      "           LightGBM         SMOTETomek + RobustScaler  96.000%  94.308% 94.937%   93.750% 99.372%\n",
      "  Gradient Boosting Normalized Data with RobustScaler  96.000%  94.177% 93.611%   94.815% 99.603%\n",
      "  Gradient Boosting                               LDA  96.000%  93.548% 93.611%   93.611% 98.033%\n",
      "        Extra Trees                  LDA + SMOTETomek  96.000%  93.548% 93.611%   93.611% 99.426%\n",
      "     MLP Classifier                                MI  96.000%  93.548% 93.611%   93.611% 99.234%\n",
      "K-Nearest Neighbors                               LDA  96.000%  93.521% 93.472%   93.698% 96.164%\n",
      "K-Nearest Neighbors                  LDA + SMOTETomek  96.000%  93.521% 93.472%   93.698% 96.381%\n",
      "      Random Forest         SMOTETomek + RobustScaler  96.000%  93.521% 93.472%   93.698% 99.707%\n",
      "      Random Forest                               LDA  96.000%  93.521% 93.472%   93.698% 97.712%\n",
      "      XGBoost Model         SMOTETomek + RobustScaler  96.000%  93.521% 93.472%   93.698% 99.312%\n",
      "      XGBoost Model                            Boruta  96.000%  93.521% 93.472%   93.698% 99.712%\n",
      "     XGBoost2 Model                     Original Data  96.000%  93.521% 93.472%   93.698% 99.640%\n",
      "     XGBoost2 Model         SMOTETomek + RobustScaler  96.000%  93.521% 93.472%   93.698% 99.423%\n",
      "     XGBoost2 Model          SMOTETomek + MiMaxScaler  96.000%  93.521% 93.472%   93.698% 99.270%\n",
      "  Gradient Boosting          SMOTETomek + MiMaxScaler  96.000%  93.521% 93.472%   93.698% 99.453%\n",
      "  Gradient Boosting                            Boruta  96.000%  93.521% 93.472%   93.698% 99.458%\n",
      "        Extra Trees                     Original Data  96.000%  93.521% 93.472%   93.698% 99.712%\n",
      "        Extra Trees         SMOTETomek + RobustScaler  96.000%  93.521% 93.472%   93.698% 99.603%\n",
      "        Extra Trees                                MI  96.000%  93.521% 93.472%   93.698% 99.640%\n",
      "        Extra Trees                               LDA  96.000%  93.521% 93.472%   93.698% 98.645%\n",
      "        Extra Trees                            Boruta  96.000%  93.521% 93.472%   93.698% 99.855%\n",
      "           LightGBM Normalized Data with RobustScaler  96.000%  93.521% 93.472%   93.698% 99.638%\n",
      "           LightGBM                               LDA  96.000%  93.521% 93.472%   93.698% 98.101%\n",
      "           LightGBM                            Boruta  96.000%  93.521% 93.472%   93.698% 99.130%\n",
      "        Extra Trees Normalized Data with MinMaxScaler  96.000%  93.439% 93.333%   94.737% 99.857%\n",
      "     MLP Classifier Normalized Data with RobustScaler  96.000%  93.439% 93.333%   94.737% 99.529%\n",
      "           AdaBoost                       Autoencoder  94.667%  93.780% 94.179%   93.669% 98.641%\n",
      "Logistic Regression          SMOTETomek + MiMaxScaler  94.667%  93.744% 95.505%   92.515% 98.152%\n",
      "Logistic Regression                       Autoencoder  94.667%  93.598% 94.179%   93.081% 97.350%\n",
      "      Random Forest                            Boruta  94.667%  93.081% 94.179%   92.222% 99.712%\n",
      "      XGBoost Model          Autoencoder + SMOTETomek  94.667%  93.081% 94.179%   92.222% 98.618%\n",
      "     XGBoost2 Model               Boruta + SMOTETomek  94.667%  92.593% 91.528%   93.948% 99.196%\n",
      "Logistic Regression                                MI  94.667%  92.224% 92.853%   91.667% 97.354%\n",
      "        Extra Trees          SMOTETomek + MiMaxScaler  94.667%  92.210% 92.576%   93.333% 99.414%\n",
      "     MLP Classifier         SMOTETomek + RobustScaler  94.667%  92.210% 92.576%   93.333% 98.749%\n",
      "Logistic Regression Normalized Data with RobustScaler  94.667%  92.142% 92.715%   91.634% 99.781%\n",
      "K-Nearest Neighbors Normalized Data with RobustScaler  94.667%  92.142% 92.715%   91.634% 95.625%\n",
      "     XGBoost2 Model                  LDA + SMOTETomek  94.667%  92.142% 92.715%   91.634% 99.351%\n",
      "           LightGBM          SMOTETomek + MiMaxScaler  94.667%  92.142% 92.715%   91.634% 98.041%\n",
      "  Gradient Boosting Normalized Data with MinMaxScaler  94.667%  92.000% 91.250%   93.704% 99.315%\n",
      "      Random Forest                     Original Data  94.667%  91.998% 92.576%   92.173% 99.563%\n",
      "      Random Forest Normalized Data with MinMaxScaler  94.667%  91.998% 92.576%   92.173% 99.563%\n",
      "      XGBoost Model                                MI  94.667%  91.389% 91.389%   91.389% 99.347%\n",
      "  Gradient Boosting                                MI  94.667%  91.389% 91.389%   91.389% 99.603%\n",
      "        Extra Trees                   MI + SMOTETomek  94.667%  91.389% 91.389%   91.389% 99.342%\n",
      "           AdaBoost Normalized Data with RobustScaler  94.667%  91.389% 91.528%   91.737% 98.668%\n",
      "           AdaBoost         SMOTETomek + RobustScaler  94.667%  91.389% 91.528%   91.737% 97.389%\n",
      "      Random Forest Normalized Data with RobustScaler  94.667%  91.317% 91.250%   91.880% 99.563%\n",
      "      XGBoost Model                               LDA  94.667%  91.317% 91.250%   91.880% 99.338%\n",
      "      XGBoost Model                  LDA + SMOTETomek  94.667%  91.317% 91.250%   91.880% 99.196%\n",
      "     XGBoost2 Model                               LDA  94.667%  91.317% 91.250%   91.880% 99.450%\n",
      "K-Nearest Neighbors                       Autoencoder  93.333%  92.324% 93.422%   91.799% 98.706%\n",
      "      XGBoost Model                       Autoencoder  93.333%  92.285% 92.096%   92.602% 99.291%\n",
      "     XGBoost2 Model                       Autoencoder  93.333%  92.285% 92.096%   92.602% 99.184%\n",
      "        Extra Trees                       Autoencoder  93.333%  92.285% 92.096%   92.602% 98.954%\n",
      "K-Nearest Neighbors                            Boruta  93.333%  92.096% 92.096%   92.602% 98.719%\n",
      "     MLP Classifier          Autoencoder + SMOTETomek  93.333%  91.676% 93.422%   90.278% 98.048%\n",
      "           LightGBM          Autoencoder + SMOTETomek  93.333%  91.676% 93.422%   90.278% 98.732%\n",
      "      Random Forest          Autoencoder + SMOTETomek  93.333%  91.627% 93.422%   90.196% 99.180%\n",
      "Logistic Regression               Boruta + SMOTETomek  93.333%  91.015% 91.957%   90.602% 95.970%\n",
      "     MLP Classifier                   MI + SMOTETomek  93.333%  90.948% 92.096%   90.370% 98.827%\n",
      "     MLP Classifier                  LDA + SMOTETomek  93.333%  90.892% 92.096%   89.951% 94.643%\n",
      "      Random Forest                                MI  93.333%  90.122% 90.631%   89.673% 99.495%\n",
      "  Gradient Boosting               Boruta + SMOTETomek  93.333%  90.122% 90.770%   90.212% 98.918%\n",
      "     MLP Classifier                               LDA  93.333%  90.122% 90.631%   89.673% 96.060%\n",
      "           LightGBM               Boruta + SMOTETomek  93.333%  90.122% 90.770%   90.212% 98.879%\n",
      "Logistic Regression                            Boruta  93.333%  90.093% 90.492%   90.418% 97.255%\n",
      "      Random Forest                   MI + SMOTETomek  93.333%  90.074% 90.631%   89.583% 99.349%\n",
      "      XGBoost Model                   MI + SMOTETomek  93.333%  90.074% 90.631%   89.583% 98.874%\n",
      "  Gradient Boosting                   MI + SMOTETomek  93.333%  90.074% 90.631%   89.583% 99.312%\n",
      "      Random Forest                  LDA + SMOTETomek  93.333%  89.948% 90.492%   89.683% 98.867%\n",
      "  Gradient Boosting                  LDA + SMOTETomek  93.333%  89.948% 90.492%   89.683% 98.576%\n",
      "     XGBoost2 Model                            Boruta  93.333%  89.247% 89.306%   89.306% 99.176%\n",
      "           AdaBoost                     Original Data  93.333%  89.247% 89.306%   89.306% 97.737%\n",
      "           AdaBoost                               LDA  93.333%  89.247% 89.306%   89.306% 94.731%\n",
      "           AdaBoost                  LDA + SMOTETomek  93.333%  89.247% 89.306%   89.306% 90.165%\n",
      "           AdaBoost                            Boruta  93.333%  89.247% 89.306%   89.306% 98.267%\n",
      "     XGBoost2 Model                                MI  93.333%  89.202% 89.167%   89.356% 99.421%\n",
      "K-Nearest Neighbors Normalized Data with MinMaxScaler  92.000%  91.276% 91.200%   91.477% 98.443%\n",
      "        Extra Trees          Autoencoder + SMOTETomek  92.000%  91.053% 91.338%   91.042% 99.269%\n",
      "  Gradient Boosting                       Autoencoder  92.000%  90.728% 90.013%   91.528% 98.126%\n",
      "  Gradient Boosting          Autoencoder + SMOTETomek  92.000%  90.529% 90.013%   91.111% 98.948%\n",
      "           LightGBM                       Autoencoder  92.000%  90.013% 90.013%   90.013% 97.674%\n",
      "           AdaBoost          Autoencoder + SMOTETomek  92.000%  89.883% 90.013%   89.874% 94.148%\n",
      "     MLP Classifier                       Autoencoder  92.000%  89.883% 90.013%   89.874% 98.423%\n",
      "Logistic Regression                               LDA  92.000%  89.715% 91.338%   89.006% 95.821%\n",
      "     MLP Classifier Normalized Data with MinMaxScaler  92.000%  89.617% 91.200%   88.538% 99.254%\n",
      "     MLP Classifier          SMOTETomek + MiMaxScaler  92.000%  89.557% 91.061%   89.499% 96.749%\n",
      "K-Nearest Neighbors                     Original Data  92.000%  89.526% 91.200%   88.194% 95.163%\n",
      "           AdaBoost               Boruta + SMOTETomek  92.000%  89.056% 88.548%   89.630% 94.555%\n",
      "           AdaBoost Normalized Data with MinMaxScaler  92.000%  87.991% 88.548%   87.712% 97.879%\n",
      "           AdaBoost          SMOTETomek + MiMaxScaler  90.667%  87.712% 89.255%   87.619% 96.501%\n",
      "     MLP Classifier                            Boruta  90.667%  87.523% 89.255%   86.343% 98.531%\n",
      "           LightGBM                  LDA + SMOTETomek  90.667%  86.778% 87.790%   86.296% 96.621%\n",
      "K-Nearest Neighbors          Autoencoder + SMOTETomek  89.333%  87.348% 88.498%   87.260% 97.073%\n",
      "Logistic Regression          Autoencoder + SMOTETomek  89.333%  86.191% 88.220%   85.000% 96.564%\n",
      "K-Nearest Neighbors         SMOTETomek + RobustScaler  89.333%  85.604% 87.033%   85.029% 95.843%\n",
      "     MLP Classifier               Boruta + SMOTETomek  89.333%  85.503% 87.033%   84.314% 97.064%\n",
      "K-Nearest Neighbors          SMOTETomek + MiMaxScaler  88.000%  85.439% 87.740%   85.498% 94.755%\n",
      "K-Nearest Neighbors               Boruta + SMOTETomek  88.000%  84.320% 86.275%   82.898% 95.427%\n",
      "Logistic Regression                   MI + SMOTETomek  82.667%  80.626% 84.571%   79.625% 95.948%\n",
      "Logistic Regression                  LDA + SMOTETomek  81.333%  79.893% 83.813%   79.833% 95.357%\n",
      "     MLP Classifier                     Original Data  61.333%  32.479% 37.500%   53.425% 53.714%\n",
      "\n",
      "====================================================================================================\n",
      "TOP CONFIGURATION PER MODEL\n",
      "====================================================================================================\n",
      "           ML Model            Configuration Accuracy F1 Score  Recall Precision ROC_AUC\n",
      "           AdaBoost                       MI  97.333%  95.694% 95.694%   95.694% 99.785%\n",
      "        Extra Trees      Boruta + SMOTETomek  98.667%  97.840% 97.778%   98.039% 99.672%\n",
      "  Gradient Boosting            Original Data  97.333%  96.328% 95.833%   97.176% 99.859%\n",
      "K-Nearest Neighbors          MI + SMOTETomek  98.667%  97.850% 97.917%   97.917% 98.178%\n",
      "           LightGBM          MI + SMOTETomek  98.667%  97.850% 97.917%   97.917% 98.923%\n",
      "Logistic Regression            Original Data  97.333%  95.658% 95.556%   96.296% 99.818%\n",
      "     MLP Classifier                       MI  96.000%  93.548% 93.611%   93.611% 99.234%\n",
      "      Random Forest SMOTETomek + MiMaxScaler  97.333%  95.658% 95.556%   96.296% 99.525%\n",
      "      XGBoost Model            Original Data  97.333%  95.694% 95.694%   95.694% 99.347%\n",
      "     XGBoost2 Model          MI + SMOTETomek  98.667%  97.850% 97.917%   97.917% 99.421%\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataframe\n",
    "result = pd.DataFrame({\n",
    "    'ML Model': ML_Model,\n",
    "    'Configuration': ML_Config,\n",
    "    'Accuracy': [f\"{acc * 100:.3f}%\" for acc in accuracy],\n",
    "    'F1 Score': [f\"{f1 * 100:.3f}%\" for f1 in f1_score],\n",
    "    'Recall': [f\"{rec * 100:.3f}%\" for rec in recall],\n",
    "    'Precision': [f\"{prec * 100:.3f}%\" for prec in precision],\n",
    "    'ROC_AUC': [f\"{roc * 100:.3f}%\" for roc in auc_roc],\n",
    "})\n",
    "\n",
    "# Remove duplicates based on model and configuration\n",
    "result.drop_duplicates(subset=[\"ML Model\", \"Configuration\"], inplace=True)\n",
    "\n",
    "# Display the result\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"MODEL PERFORMANCE RESULTS\")\n",
    "print(\"=\" * 100)\n",
    "print(result.to_string(index=False))\n",
    "\n",
    "# Save the result to a CSV file\n",
    "# result.to_csv('final_results/model_results.csv', index=False)\n",
    "# print(\"\\nResults saved to model_results.csv\")\n",
    "\n",
    "# Sort by Accuracy and F1 Score\n",
    "sorted_result = result.sort_values(by=['Accuracy', 'F1 Score'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display the sorted result\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"SORTED MODEL PERFORMANCE RESULTS (by Accuracy and F1 Score)\")\n",
    "print(\"=\" * 100)\n",
    "print(sorted_result.to_string(index=False))\n",
    "\n",
    "# Save the sorted result\n",
    "# sorted_result.to_csv('final_results/sorted_model_results.csv', index=False)\n",
    "# print(\"\\nSorted results saved to sorted_model_results.csv\")\n",
    "\n",
    "# Extract top configuration per ML model\n",
    "top_per_model = sorted_result.groupby('ML Model', as_index=False).first()\n",
    "\n",
    "# Display and save the top configuration table\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"TOP CONFIGURATION PER MODEL\")\n",
    "print(\"=\" * 100)\n",
    "print(top_per_model.to_string(index=False))\n",
    "\n",
    "# top_per_model.to_csv('final_results/top_configurations.csv', index=False)\n",
    "# print(\"\\nTop configuration per model saved to top_configurations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23844063",
   "metadata": {
    "papermill": {
     "duration": 0.02073,
     "end_time": "2025-10-16T10:06:01.690816",
     "exception": false,
     "start_time": "2025-10-16T10:06:01.670086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 3321433,
     "sourceId": 6491929,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18606.508318,
   "end_time": "2025-10-16T10:06:04.429490",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-16T04:55:57.921172",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
