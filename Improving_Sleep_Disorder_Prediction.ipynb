{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2FdmQ/7H4U4VA9dhRabBj"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Importing Libraries\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Yb1fHaSMPI85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boruta category_encoders xgboost catboost"
      ],
      "metadata": {
        "id": "_TgpA4zWnk0L",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a76b9387-6381-4bbe-cd57-ad70060c15ad"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting boruta\n",
            "  Downloading Boruta-0.4.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.8.1-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.0.4)\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.12/dist-packages (from boruta) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.12/dist-packages (from boruta) (1.6.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.12/dist-packages (from boruta) (1.16.1)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.0.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (0.14.5)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.17.1->boruta) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.17.1->boruta) (3.6.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels>=0.9.0->category_encoders) (25.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading Boruta-0.4.3-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading category_encoders-2.8.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost, boruta, category_encoders\n",
            "Successfully installed boruta-0.4.3 catboost-1.2.8 category_encoders-2.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "diEVyiMoJ4Cm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, RobustScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler, SMOTENC\n",
        "from imblearn.combine import SMOTETomek, SMOTEENN\n",
        "from imblearn.under_sampling import CondensedNearestNeighbour, TomekLinks, RandomUnderSampler\n",
        "from boruta import BorutaPy\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Load & Preprocessing"
      ],
      "metadata": {
        "id": "unb2qil8SoTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/Sleep_health_and_lifestyle_dataset.csv\")\n",
        "df.fillna(\"None\", inplace=True)\n",
        "\n",
        "# Dividing Blood Pressure into Systolic and Diastolic BP\n",
        "df[['Systolic BP', 'Diastolic BP']] = df['Blood Pressure'].str.split('/', expand=True).astype(int)\n",
        "df.drop(['Person ID', 'Blood Pressure'], axis=1, inplace=True)\n",
        "\n",
        "# Labeling less number of careers as other\n",
        "df['Occupation'] = df['Occupation'].replace(['Manager', 'Sales Representative', 'Scientist', 'Software Engineer'], 'Other')\n",
        "\n",
        "# Adding the average BMI for the range\n",
        "df['BMI Category'] = df['BMI Category'].replace({'Normal':22, 'Normal Weight':22, 'Overweight':27, 'Obese':30})\n",
        "\n",
        "# Creating Interaction features\n",
        "df['Stress_sleep_interaction'] = df['Stress Level'] / df['Quality of Sleep']\n",
        "df['BMI_Activity'] = df['BMI Category'] * df['Physical Activity Level']\n",
        "df['Sleep_Heart_ratio'] = df['Sleep Duration'] / df['Heart Rate']\n",
        "df['Sleep_Steps_ratio'] = df['Sleep Duration'] / df['Daily Steps']\n",
        "df['Sleep_Stress_ratio'] = df['Sleep Duration'] / df['Stress Level']\n",
        "\n",
        "df = pd.get_dummies(df, columns=['Occupation'], drop_first=False)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "columns = ['Gender', 'Sleep Disorder']\n",
        "for col in columns:\n",
        "  df[col] = label_encoder.fit_transform(df[col])\n",
        "\n",
        "num_col = ['Age', 'Sleep Duration', 'Quality of Sleep', 'Physical Activity Level', 'Stress Level', 'Stress_sleep_interaction',\n",
        "          'Sleep_Heart_ratio', 'Sleep_Steps_ratio', 'Sleep_Stress_ratio', 'Heart Rate', 'Daily Steps',\n",
        "           'Systolic BP', 'Diastolic BP']\n",
        "\n",
        "Q1 = df[num_col].quantile(0.25)\n",
        "Q3 = df[num_col].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# getting rid of the outliers\n",
        "df = df[~((df[num_col] < (Q1 - 1.4 * IQR)) | (df[num_col] > (Q3 + 1.4 * IQR))).any(axis=1)]\n",
        "\n",
        "X = df.drop('Sleep Disorder', axis=1)\n",
        "y = df['Sleep Disorder']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n"
      ],
      "metadata": {
        "id": "R6yAAGUeSsn3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apply RobustSclaer, MI, LDA, Boruta, Autoencoder, and SMOTETomek"
      ],
      "metadata": {
        "id": "auoUKdHC731t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Normalize the data\n",
        "scaler = RobustScaler()\n",
        "X_train_normalized = scaler.fit_transform(X_train)\n",
        "X_test_normalized = scaler.transform(X_test)\n",
        "\n",
        "# Applying Mutual information\n",
        "mi = SelectKBest(score_func=mutual_info_classif, k=5)\n",
        "X_train_mi = mi.fit_transform(X_train_normalized, y_train)\n",
        "X_test_mi = mi.transform(X_test_normalized)\n",
        "\n",
        "# Applying LDA\n",
        "lda = LinearDiscriminantAnalysis(n_components=2)\n",
        "X_train_lda = lda.fit_transform(X_train_mi, y_train)\n",
        "X_test_lda = lda.transform(X_test_mi)\n",
        "\n",
        "# RandomForest classifier\n",
        "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Applying Boruta Feature Selection\n",
        "boruta_selector = BorutaPy(rfc, n_estimators='auto', verbose=0, random_state=42)\n",
        "\n",
        "X_train_boruta = boruta_selector.fit_transform(X_train_normalized, y_train)\n",
        "X_test_boruta = boruta_selector.transform(X_test_normalized)\n",
        "\n",
        "# applying Autoencoder\n",
        "n_features = X_train_boruta.shape[1]\n",
        "input_layer = Input(shape=(n_features,))\n",
        "encoded = Dense(32, activation='relu')(input_layer)\n",
        "bottleneck = Dense(16, activation='relu')(encoded)\n",
        "decoded = Dense(32, activation='relu')(encoded)\n",
        "decoded = Dense(n_features, activation='sigmoid')(encoded)\n",
        "\n",
        "autoencoder = Model(input_layer, decoded)\n",
        "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "autoencoder.fit(X_train_boruta, X_train_boruta, epochs=10, batch_size=32, verbose=0)\n",
        "encoder = Model(input_layer, bottleneck)\n",
        "X_train_encoded = encoder.predict(X_train_boruta)\n",
        "X_test_encoded = encoder.predict(X_test_boruta)\n",
        "\n",
        "smotetomek = SMOTETomek(sampling_strategy='auto',\n",
        "                   smote=SMOTE(k_neighbors=3, random_state=42),\n",
        "                   tomek=TomekLinks(sampling_strategy='auto', n_jobs=-1),\n",
        "                   n_jobs=-1,\n",
        "                   random_state=42)\n",
        "\n",
        "X_train_resample, y_train_resample = smotetomek.fit_resample(X_train_normalized, y_train)"
      ],
      "metadata": {
        "id": "b5AYEVMw8BMY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54b27317-4096-43e1-f178-9d50e40c53e5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trial with LazyClassifier"
      ],
      "metadata": {
        "id": "qEu8cN4IWbGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
        "# models, preds = clf.fit(X_train, X_test, y_train, y_test)\n",
        "# models, preds = clf.fit(X_train_normalized, X_test_normalized, y_train, y_test)\n",
        "# models, preds = clf.fit(X_train_mi, X_test_mi, y_train, y_test)\n",
        "# models, preds = clf.fit(X_train_lda, X_test_lda, y_train, y_test)\n",
        "# models, preds = clf.fit(X_train_boruta, X_test_boruta, y_train, y_test)\n",
        "# models, preds = clf.fit(X_train_encoded, X_test_encoded, y_train, y_test)\n",
        "# models, preds = clf.fit(X_train_resample, X_test_normalized, y_train_resample, y_test)\n",
        "# print(models)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "u7YIHQlAWpmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Model Result Storage"
      ],
      "metadata": {
        "id": "TpEDoK5Ihy4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ML_Model = []\n",
        "ML_Config = []\n",
        "accuracy = []\n",
        "f1_score = []\n",
        "recall = []\n",
        "precision = []\n",
        "auc_roc = []  # Adding a holder for AUC-ROC\n",
        "\n",
        "# Function to call for storing the results\n",
        "def storeResults(model, config, a, b, c, d, e):\n",
        "    \"\"\"\n",
        "    Store model performance results\n",
        "\n",
        "    Parameters:\n",
        "    model: Name of the ML model\n",
        "    config: Configuration name (preprocessing steps applied)\n",
        "    a: Accuracy score\n",
        "    b: F1 score\n",
        "    c: Recall score\n",
        "    d: Precision score\n",
        "    e: AUC-ROC score\n",
        "    \"\"\"\n",
        "    ML_Model.append(model)\n",
        "    ML_Config.append(config)\n",
        "    accuracy.append(round(a, 6))\n",
        "    f1_score.append(round(b, 6))\n",
        "    recall.append(round(c, 6))\n",
        "    precision.append(round(d, 6))\n",
        "    auc_roc.append(round(e, 6))"
      ],
      "metadata": {
        "id": "tlV5BgjKhyYX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "Pg8p5Fg07Hsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import uniform\n",
        "configurations = []\n",
        "\n",
        "configurations.append(('Original Data', X_train, X_test, y_train))\n",
        "configurations.append(('SMOTETomek', X_train_resample, X_test_normalized, y_train_resample))\n",
        "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
        "configurations.append(('MI', X_train_mi, X_test_mi, y_train))\n",
        "configurations.append(('LDA', X_train_lda, X_test_lda, y_train))\n",
        "configurations.append(('Boruta', X_train_boruta, X_test_boruta, y_train))\n",
        "configurations.append(('Autoencoder', X_train_encoded, X_test_encoded, y_train))\n",
        "\n",
        "params = {\n",
        "    'penalty': ['l2'],\n",
        "    'C': [0.01],\n",
        "    'solver': ['liblinear'],\n",
        "    'max_iter': [455]\n",
        "}\n",
        "\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning Logistic Regression with {name} configuration...\")\n",
        "    # logr = RandomizedSearchCV(LogisticRegression(), params, cv=5, n_iter=50,\n",
        "    #                          n_jobs=-1, scoring=['accuracy', 'f1_macro'], refit='f1_macro', verbose=2)\n",
        "    logr= GridSearchCV(LogisticRegression(), params, cv=5, n_jobs=-1, scoring='accuracy', verbose=2)\n",
        "    logr.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_lr = logr.predict(X_train_cfg)\n",
        "    y_test_lr = logr.predict(X_test_cfg)\n",
        "    y_train_lr_proba = logr.predict_proba(X_train_cfg)\n",
        "    y_test_lr_proba = logr.predict_proba(X_test_cfg)\n",
        "\n",
        "    metrics_dict = {\n",
        "          \"Dataset\": [\"Training\", \"Test\"],\n",
        "          \"Accuracy\": [\n",
        "              metrics.accuracy_score(y_train_cfg, y_train_lr),\n",
        "              metrics.accuracy_score(y_test, y_test_lr),\n",
        "          ],\n",
        "          \"F1 Score\": [\n",
        "              metrics.f1_score(y_train_cfg, y_train_lr, average='macro'),\n",
        "              metrics.f1_score(y_test, y_test_lr, average='macro'),\n",
        "          ],\n",
        "          \"Recall\": [\n",
        "              metrics.recall_score(y_train_cfg, y_train_lr, average='macro'),\n",
        "              metrics.recall_score(y_test, y_test_lr, average='macro'),\n",
        "          ],\n",
        "          \"Precision\": [\n",
        "              metrics.precision_score(y_train_cfg, y_train_lr, average='macro'),\n",
        "              metrics.precision_score(y_test, y_test_lr, average='macro'),\n",
        "          ],\n",
        "          \"AUC-ROC\": [\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_lr_proba, multi_class='ovr', average='macro'),\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_test), y_test_lr_proba, multi_class='ovr', average='macro'),\n",
        "          ]\n",
        "      }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nLogistic Regression Model Performance Metrics\")\n",
        "    print(\"Configuration Name: \", name)\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_lr_proba, multi_class='ovr', average='macro')\n",
        "    storeResults(\n",
        "          'Logistic Regression',\n",
        "          name,\n",
        "          metrics.accuracy_score(y_test, y_test_lr),\n",
        "          metrics.f1_score(y_test, y_test_lr, average='macro'),\n",
        "          metrics.recall_score(y_test, y_test_lr, average='macro'),\n",
        "          metrics.precision_score(y_test, y_test_lr, average='macro'),\n",
        "          auc_score\n",
        "      )\n",
        "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
        "    print(logr.best_params_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axcfe3_47JuW",
        "outputId": "73cc908f-ed84-46db-d974-e4394cb4f746"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running Logistic Regression with Original Data configuration...\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "\n",
            "Logistic Regression Model Performance Metrics\n",
            "Configuration Name:  Original Data\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.912281  0.874860 0.863776   0.887701 0.893520\n",
            "    Test  0.965517  0.972039 0.966667   0.979798 0.981503\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'C': 0.01, 'max_iter': 455, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "\n",
            "Running Logistic Regression with SMOTETomek configuration...\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "\n",
            "Logistic Regression Model Performance Metrics\n",
            "Configuration Name:  SMOTETomek\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.818777  0.820379 0.818713   0.834236 0.903782\n",
            "    Test  0.879310  0.907212 0.918817   0.907179 0.987961\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'C': 0.01, 'max_iter': 455, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "\n",
            "Running Logistic Regression with Normalized Data configuration...\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "\n",
            "Logistic Regression Model Performance Metrics\n",
            "Configuration Name:  Normalized Data\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.916667  0.878962 0.865955   0.895299 0.897343\n",
            "    Test  0.982759  0.986162 0.983333   0.989583 0.988400\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'C': 0.01, 'max_iter': 455, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "\n",
            "Running Logistic Regression with MI configuration...\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "\n",
            "Logistic Regression Model Performance Metrics\n",
            "Configuration Name:  MI\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.671053  0.267717 0.333333   0.223684 0.768238\n",
            "    Test  0.534483  0.232210 0.333333   0.178161 0.812397\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'C': 0.01, 'max_iter': 455, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "\n",
            "Running Logistic Regression with LDA configuration...\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "\n",
            "Logistic Regression Model Performance Metrics\n",
            "Configuration Name:  LDA\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.855263   0.81163 0.820713   0.804054 0.894114\n",
            "    Test  0.879310   0.90104 0.895161   0.911765 0.929962\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'C': 0.01, 'max_iter': 455, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "\n",
            "Running Logistic Regression with Boruta configuration...\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "\n",
            "Logistic Regression Model Performance Metrics\n",
            "Configuration Name:  Boruta\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.916667  0.881082 0.865955   0.899518 0.901454\n",
            "    Test  0.965517  0.972039 0.966667   0.979798 0.986207\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'C': 0.01, 'max_iter': 455, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "\n",
            "Running Logistic Regression with Autoencoder configuration...\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "\n",
            "Logistic Regression Model Performance Metrics\n",
            "Configuration Name:  Autoencoder\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.671053  0.267717 0.333333   0.223684 0.779093\n",
            "    Test  0.534483  0.232210 0.333333   0.178161 0.867950\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'C': 0.01, 'max_iter': 455, 'penalty': 'l2', 'solver': 'liblinear'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN"
      ],
      "metadata": {
        "id": "BmgqNGgGKtdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "configurations = []\n",
        "\n",
        "configurations.append(('Original Data', X_train, X_test, y_train))\n",
        "configurations.append(('SMOTETomek', X_train_resample, X_test_normalized, y_train_resample))\n",
        "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
        "configurations.append(('MI', X_train_mi, X_test_mi, y_train))\n",
        "configurations.append(('LDA', X_train_lda, X_test_lda, y_train))\n",
        "configurations.append(('Boruta', X_train_boruta, X_test_boruta, y_train))\n",
        "configurations.append(('Autoencoder', X_train_encoded, X_test_encoded, y_train))\n",
        "\n",
        "params = {\n",
        "    'n_neighbors': [4, 5, 6, 7],\n",
        "    'weights': ['distance'],\n",
        "    'metric': ['manhattan'],\n",
        "    'p': [1, 2]\n",
        "}\n",
        "\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning KNN with {name} configuration...\")\n",
        "    knn = GridSearchCV(KNeighborsClassifier(), params, cv=5,\n",
        "                             n_jobs=-1, scoring=['accuracy', 'f1_macro'], refit='accuracy', verbose=2)\n",
        "    knn.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_knn = knn.predict(X_train_cfg)\n",
        "    y_test_knn = knn.predict(X_test_cfg)\n",
        "    y_train_knn_proba = knn.predict_proba(X_train_cfg)\n",
        "    y_test_knn_proba = knn.predict_proba(X_test_cfg)\n",
        "\n",
        "    metrics_dict = {\n",
        "          \"Dataset\": [\"Training\", \"Test\"],\n",
        "          \"Accuracy\": [\n",
        "              metrics.accuracy_score(y_train_cfg, y_train_knn),\n",
        "              metrics.accuracy_score(y_test, y_test_knn),\n",
        "          ],\n",
        "          \"F1 Score\": [\n",
        "              metrics.f1_score(y_train_cfg, y_train_knn, average='macro'),\n",
        "              metrics.f1_score(y_test, y_test_knn, average='macro'),\n",
        "          ],\n",
        "          \"Recall\": [\n",
        "              metrics.recall_score(y_train_cfg, y_train_knn, average='macro'),\n",
        "              metrics.recall_score(y_test, y_test_knn, average='macro'),\n",
        "          ],\n",
        "          \"Precision\": [\n",
        "              metrics.precision_score(y_train_cfg, y_train_knn, average='macro'),\n",
        "              metrics.precision_score(y_test, y_test_knn, average='macro'),\n",
        "          ],\n",
        "          \"AUC-ROC\": [\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_knn_proba, multi_class='ovr', average='macro'),\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_test), y_test_knn_proba, multi_class='ovr', average='macro'),\n",
        "          ]\n",
        "      }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nKNearestNeighbors Model Performance Metrics\")\n",
        "    print(\"Configuration Name: \", name)\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_knn_proba, multi_class='ovr', average='macro')\n",
        "    storeResults(\n",
        "          'K-Nearest Neighbors',\n",
        "          name,\n",
        "          metrics.accuracy_score(y_test, y_test_knn),\n",
        "          metrics.f1_score(y_test, y_test_knn, average='macro'),\n",
        "          metrics.recall_score(y_test, y_test_knn, average='macro'),\n",
        "          metrics.precision_score(y_test, y_test_knn, average='macro'),\n",
        "          auc_score\n",
        "      )\n",
        "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
        "    print(knn.best_params_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZEKc2YMKRB8",
        "outputId": "2772737f-2aa7-461b-97c5-85099c28e707"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running KNN with Original Data configuration...\n",
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
            "\n",
            "KNearestNeighbors Model Performance Metrics\n",
            "Configuration Name:  Original Data\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.925439  0.894501 0.884952   0.905923 0.969501\n",
            "    Test  0.965517  0.954943 0.966667   0.947917 0.982891\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'metric': 'manhattan', 'n_neighbors': 4, 'p': 1, 'weights': 'distance'}\n",
            "\n",
            "Running KNN with SMOTETomek configuration...\n",
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
            "\n",
            "KNearestNeighbors Model Performance Metrics\n",
            "Configuration Name:  SMOTETomek\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.962882  0.962945 0.962877   0.963111 0.990684\n",
            "    Test  0.965517  0.954943 0.966667   0.947917 0.982891\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'metric': 'manhattan', 'n_neighbors': 4, 'p': 1, 'weights': 'distance'}\n",
            "\n",
            "Running KNN with Normalized Data configuration...\n",
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
            "\n",
            "KNearestNeighbors Model Performance Metrics\n",
            "Configuration Name:  Normalized Data\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.925439  0.894501 0.884952   0.905923 0.969501\n",
            "    Test  0.965517  0.954943 0.966667   0.947917 0.982891\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'metric': 'manhattan', 'n_neighbors': 4, 'p': 1, 'weights': 'distance'}\n",
            "\n",
            "Running KNN with MI configuration...\n",
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
            "\n",
            "KNearestNeighbors Model Performance Metrics\n",
            "Configuration Name:  MI\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.921053  0.889597 0.877860   0.903171 0.950664\n",
            "    Test  0.965517  0.954943 0.966667   0.947917 0.983816\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'metric': 'manhattan', 'n_neighbors': 5, 'p': 1, 'weights': 'distance'}\n",
            "\n",
            "Running KNN with LDA configuration...\n",
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
            "\n",
            "KNearestNeighbors Model Performance Metrics\n",
            "Configuration Name:  LDA\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.921053  0.889597 0.877860   0.903171 0.944243\n",
            "    Test  0.965517  0.954943 0.966667   0.947917 0.981245\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'metric': 'manhattan', 'n_neighbors': 5, 'p': 1, 'weights': 'distance'}\n",
            "\n",
            "Running KNN with Boruta configuration...\n",
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
            "\n",
            "KNearestNeighbors Model Performance Metrics\n",
            "Configuration Name:  Boruta\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.925439  0.893934 0.880039   0.909783 0.964616\n",
            "    Test  0.965517  0.954943 0.966667   0.947917 0.984015\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'metric': 'manhattan', 'n_neighbors': 4, 'p': 1, 'weights': 'distance'}\n",
            "\n",
            "Running KNN with Autoencoder configuration...\n",
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
            "\n",
            "KNearestNeighbors Model Performance Metrics\n",
            "Configuration Name:  Autoencoder\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.934211  0.907097 0.899036   0.915965 0.970734\n",
            "    Test  0.965517  0.954943 0.966667   0.947917 0.971114\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'metric': 'manhattan', 'n_neighbors': 4, 'p': 1, 'weights': 'distance'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "SHcHbkJxdLvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "configurations = []\n",
        "\n",
        "configurations.append(('Original Data', X_train, X_test, y_train))\n",
        "configurations.append(('SMOTETomek', X_train_resample, X_test_normalized, y_train_resample))\n",
        "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
        "configurations.append(('MI', X_train_mi, X_test_mi, y_train))\n",
        "configurations.append(('LDA', X_train_lda, X_test_lda, y_train))\n",
        "\n",
        "selected_features = boruta_selector.support_\n",
        "optimal_features = sum(selected_features)\n",
        "print(f\"Optimal number of features to select using Boruta: {optimal_features}\")\n",
        "\n",
        "configurations.append(('Boruta', X_train_boruta, X_test_boruta, y_train))\n",
        "configurations.append(('Autoencoder', X_train_encoded, X_test_encoded, y_train))\n",
        "\n",
        "# Step 4: Random Forest + GridSearchCV\n",
        "print(\"\\n=== Random Forest Model Performance with Hyperparameter Tuning ===\")\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': range(100, 160, 3),\n",
        "    'max_depth': range(2, 20, 3),\n",
        "    'min_samples_split': range(3, 7, 1),\n",
        "    'min_samples_leaf': range(3, 7, 1),\n",
        "    'max_features': ['sqrt'],\n",
        "    'bootstrap': [False],\n",
        "    'class_weight': ['balanced'],\n",
        "    'max_leaf_nodes': range(20, 40, 5),\n",
        "    'min_impurity_decrease': np.linspace(0.001, 0.05, 3),\n",
        "    'ccp_alpha': np.linspace(0.001, 0.07, 3),\n",
        "    'random_state': range(2, 15, 3),\n",
        "    'criterion': ['gini', 'entropy', 'log_loss']\n",
        "}\n",
        "\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning Random Forest with {name} configuration...\")\n",
        "    rf = RandomizedSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=10, n_jobs=-1, n_iter=100, scoring=['accuracy', 'f1_macro'], refit='accuracy', verbose=2)\n",
        "    rf.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_rf = rf.predict(X_train_cfg)\n",
        "    y_test_rf = rf.predict(X_test_cfg)\n",
        "    y_train_rf_proba = rf.predict_proba(X_train_cfg)\n",
        "    y_test_rf_proba = rf.predict_proba(X_test_cfg)\n",
        "\n",
        "    metrics_dict = {\n",
        "          \"Dataset\": [\"Training\", \"Test\"],\n",
        "          \"Accuracy\": [\n",
        "              metrics.accuracy_score(y_train_cfg, y_train_rf),\n",
        "              metrics.accuracy_score(y_test, y_test_rf),\n",
        "          ],\n",
        "          \"F1 Score\": [\n",
        "              metrics.f1_score(y_train_cfg, y_train_rf, average='macro'),\n",
        "              metrics.f1_score(y_test, y_test_rf, average='macro'),\n",
        "          ],\n",
        "          \"Recall\": [\n",
        "              metrics.recall_score(y_train_cfg, y_train_rf, average='macro'),\n",
        "              metrics.recall_score(y_test, y_test_rf, average='macro'),\n",
        "          ],\n",
        "          \"Precision\": [\n",
        "              metrics.precision_score(y_train_cfg, y_train_rf, average='macro'),\n",
        "              metrics.precision_score(y_test, y_test_rf, average='macro'),\n",
        "          ],\n",
        "          \"AUC-ROC\": [\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_rf_proba, multi_class='ovr', average='macro'),\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_test), y_test_rf_proba, multi_class='ovr', average='macro'),\n",
        "          ]\n",
        "      }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nRandom Forest Model Performance Metrics\")\n",
        "    print(\"Configuration Name: \", name)\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_rf_proba, multi_class='ovr', average='macro')\n",
        "    storeResults(\n",
        "          'Random Forest',\n",
        "          name,\n",
        "          metrics.accuracy_score(y_test, y_test_rf),\n",
        "          metrics.f1_score(y_test, y_test_rf, average='macro'),\n",
        "          metrics.recall_score(y_test, y_test_rf, average='macro'),\n",
        "          metrics.precision_score(y_test, y_test_rf, average='macro'),\n",
        "          auc_score\n",
        "      )\n",
        "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
        "    print(rf.best_params_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7NTek4OUXv5",
        "outputId": "5ee893e3-46da-4d81-b732-dad67403e7a4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal number of features to select using Boruta: 10\n",
            "\n",
            "=== Random Forest Model Performance with Hyperparameter Tuning ===\n",
            "\n",
            "Running Random Forest with Original Data configuration...\n",
            "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
            "\n",
            "Random Forest Model Performance Metrics\n",
            "Configuration Name:  Original Data\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.921053  0.889597 0.877860   0.903171 0.967244\n",
            "    Test  0.965517  0.954943 0.966667   0.947917 0.991868\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'random_state': 14, 'n_estimators': 157, 'min_samples_split': 3, 'min_samples_leaf': 5, 'min_impurity_decrease': np.float64(0.001), 'max_leaf_nodes': 20, 'max_features': 'sqrt', 'max_depth': 5, 'criterion': 'entropy', 'class_weight': 'balanced', 'ccp_alpha': np.float64(0.001), 'bootstrap': False}\n",
            "\n",
            "Running Random Forest with SMOTETomek configuration...\n",
            "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
            "\n",
            "Random Forest Model Performance Metrics\n",
            "Configuration Name:  SMOTETomek\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.954148  0.954363 0.954148   0.954952 0.987248\n",
            "    Test  0.965517  0.954943 0.966667   0.947917 0.986328\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'random_state': 11, 'n_estimators': 148, 'min_samples_split': 5, 'min_samples_leaf': 4, 'min_impurity_decrease': np.float64(0.001), 'max_leaf_nodes': 20, 'max_features': 'sqrt', 'max_depth': 17, 'criterion': 'entropy', 'class_weight': 'balanced', 'ccp_alpha': np.float64(0.001), 'bootstrap': False}\n",
            "\n",
            "Running Random Forest with Normalized Data configuration...\n",
            "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
            "\n",
            "Random Forest Model Performance Metrics\n",
            "Configuration Name:  Normalized Data\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.916667  0.881082 0.865955   0.899518 0.922134\n",
            "    Test  0.965517  0.972039 0.966667   0.979798 0.997550\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'random_state': 14, 'n_estimators': 103, 'min_samples_split': 5, 'min_samples_leaf': 3, 'min_impurity_decrease': np.float64(0.05), 'max_leaf_nodes': 30, 'max_features': 'sqrt', 'max_depth': 5, 'criterion': 'entropy', 'class_weight': 'balanced', 'ccp_alpha': np.float64(0.035500000000000004), 'bootstrap': False}\n",
            "\n",
            "Running Random Forest with MI configuration...\n",
            "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
            "\n",
            "Random Forest Model Performance Metrics\n",
            "Configuration Name:  MI\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.912281  0.881490 0.892955   0.871492 0.962273\n",
            "    Test  0.931034  0.907239 0.945161   0.888889 0.995796\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'random_state': 11, 'n_estimators': 115, 'min_samples_split': 5, 'min_samples_leaf': 4, 'min_impurity_decrease': np.float64(0.001), 'max_leaf_nodes': 35, 'max_features': 'sqrt', 'max_depth': 17, 'criterion': 'gini', 'class_weight': 'balanced', 'ccp_alpha': np.float64(0.001), 'bootstrap': False}\n",
            "\n",
            "Running Random Forest with LDA configuration...\n",
            "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
            "\n",
            "Random Forest Model Performance Metrics\n",
            "Configuration Name:  LDA\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.855263  0.814867 0.840267   0.794903 0.909667\n",
            "    Test  0.879310  0.875000 0.906989   0.852937 0.950560\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'random_state': 2, 'n_estimators': 136, 'min_samples_split': 4, 'min_samples_leaf': 3, 'min_impurity_decrease': np.float64(0.05), 'max_leaf_nodes': 35, 'max_features': 'sqrt', 'max_depth': 14, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': np.float64(0.001), 'bootstrap': False}\n",
            "\n",
            "Running Random Forest with Boruta configuration...\n",
            "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
            "\n",
            "Random Forest Model Performance Metrics\n",
            "Configuration Name:  Boruta\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.916667  0.881082 0.865955   0.899518 0.919477\n",
            "    Test  0.965517  0.972039 0.966667   0.979798 0.995957\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'random_state': 8, 'n_estimators': 106, 'min_samples_split': 4, 'min_samples_leaf': 5, 'min_impurity_decrease': np.float64(0.05), 'max_leaf_nodes': 35, 'max_features': 'sqrt', 'max_depth': 5, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': np.float64(0.07), 'bootstrap': False}\n",
            "\n",
            "Running Random Forest with Autoencoder configuration...\n",
            "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
            "\n",
            "Random Forest Model Performance Metrics\n",
            "Configuration Name:  Autoencoder\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.912281  0.874860 0.863776   0.887701 0.925152\n",
            "    Test  0.948276  0.944361 0.955914   0.937500 0.986610\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'random_state': 11, 'n_estimators': 145, 'min_samples_split': 3, 'min_samples_leaf': 5, 'min_impurity_decrease': np.float64(0.025500000000000002), 'max_leaf_nodes': 35, 'max_features': 'sqrt', 'max_depth': 5, 'criterion': 'gini', 'class_weight': 'balanced', 'ccp_alpha': np.float64(0.001), 'bootstrap': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X_F-w3vDsLHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "configurations = []\n",
        "configurations.append(('Original Data', X_train, X_test, y_train))\n",
        "configurations.append(('SMOTETomek', X_train_resample, X_test_normalized, y_train_resample))\n",
        "\n",
        "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
        "configurations.append(('MI', X_train_mi, X_test_mi, y_train))\n",
        "configurations.append(('LDA', X_train_lda, X_test_lda, y_train))\n",
        "\n",
        "configurations.append(('Boruta', X_train_boruta, X_test_boruta, y_train))\n",
        "configurations.append(('Autoencoder', X_train_encoded, X_test_encoded, y_train))\n",
        "\n",
        "selected_features = boruta_selector.support_\n",
        "optimal_features = sum(selected_features)\n",
        "print(f\"Optimal number of features to select using Boruta: {optimal_features}\")\n",
        "\n",
        "# Step 4: XGBoost + GridSearchCV\n",
        "print(\"\\n=== XGBoost Model Performance with Hyperparameter Tuning ===\")\n",
        "\n",
        "param_grid = {\n",
        "    'booster': ['gbtree',],\n",
        "    'learning_rate': np.linspace(0.0001, 0.1, 10),\n",
        "    'n_estimators': range(50, 500, 10),\n",
        "    'estimator__n_estimators': range(50, 500, 10),\n",
        "    'max_depth': range(2, 50, 10),\n",
        "    'min_child_weight': range(1, 10, 1),\n",
        "    'gamma': np.linspace(0, 0.1, 3),\n",
        "    'subsample': np.linspace(0.1, 1, 1),\n",
        "    'colsample_bytree': [0.3, 0.8],\n",
        "    'colsample_bylevel': [1.0],\n",
        "    'colsample_bynode': [0.6, 0.8],\n",
        "    'max_delta_step': [0, 5],\n",
        "\n",
        "    'reg_alpha': np.linspace(0.1, 1, 1),\n",
        "    'reg_lambda': np.linspace(0.1, 1, 1),\n",
        "    'scale_pos_weight': [1, 2, 5],\n",
        "\n",
        "    'sample_type': [\"weighted\"],\n",
        "    'normalize_type': [\"tree\", \"forest\"],\n",
        "    'rate_drop': [0, 0.1],\n",
        "    'skip_drop': [0, 0.1],\n",
        "\n",
        "    'random_state': [2, 42, 49, 51]\n",
        "}\n",
        "\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning XGBoost with {name} configuration...\")\n",
        "    xgb = RandomizedSearchCV(XGBClassifier(), param_grid, n_iter=50, cv=10,\n",
        "                             n_jobs=-1, scoring=['accuracy', 'f1_macro'], refit='accuracy', verbose=2)\n",
        "    xgb.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_xg = xgb.predict(X_train_cfg)\n",
        "    y_test_xg = xgb.predict(X_test_cfg)\n",
        "    y_train_xg_proba = xgb.predict_proba(X_train_cfg)\n",
        "    y_test_xg_proba = xgb.predict_proba(X_test_cfg)\n",
        "\n",
        "    metrics_dict = {\n",
        "          \"Dataset\": [\"Training\", \"Test\"],\n",
        "          \"Accuracy\": [\n",
        "              metrics.accuracy_score(y_train_cfg, y_train_xg),\n",
        "              metrics.accuracy_score(y_test, y_test_xg),\n",
        "          ],\n",
        "          \"F1 Score\": [\n",
        "              metrics.f1_score(y_train_cfg, y_train_xg, average='macro'),\n",
        "              metrics.f1_score(y_test, y_test_xg, average='macro'),\n",
        "          ],\n",
        "          \"Recall\": [\n",
        "              metrics.recall_score(y_train_cfg, y_train_xg, average='macro'),\n",
        "              metrics.recall_score(y_test, y_test_xg, average='macro'),\n",
        "          ],\n",
        "          \"Precision\": [\n",
        "              metrics.precision_score(y_train_cfg, y_train_xg, average='macro'),\n",
        "              metrics.precision_score(y_test, y_test_xg, average='macro'),\n",
        "          ],\n",
        "          \"AUC-ROC\": [\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_xg_proba, multi_class='ovr', average='macro'),\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_test), y_test_xg_proba, multi_class='ovr', average='macro'),\n",
        "          ]\n",
        "      }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nXGBoost Model Performance Metrics\")\n",
        "    print(\"Configuration Name: \", name)\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_xg_proba, multi_class='ovr', average='macro')\n",
        "    storeResults(\n",
        "        'XGBoost Model',\n",
        "        name,\n",
        "        metrics.accuracy_score(y_test, y_test_xg),\n",
        "        metrics.f1_score(y_test, y_test_xg, average='macro'),\n",
        "        metrics.recall_score(y_test, y_test_xg, average='macro'),\n",
        "        metrics.precision_score(y_test, y_test_xg, average='macro'),\n",
        "        auc_score\n",
        "    )\n",
        "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
        "    print(xgb.best_params_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gFupPmNsedu",
        "outputId": "6d9a97af-a512-4d97-a925-e80c66720193"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal number of features to select using Boruta: 10\n",
            "\n",
            "=== XGBoost Model Performance with Hyperparameter Tuning ===\n",
            "\n",
            "Running XGBoost with Original Data configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "XGBoost Model Performance Metrics\n",
            "Configuration Name:  Original Data\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.916667  0.881082 0.865955   0.899518 0.905616\n",
            "    Test  0.965517  0.972039 0.966667   0.979798 0.994818\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'subsample': np.float64(0.1), 'skip_drop': 0.1, 'scale_pos_weight': 5, 'sample_type': 'weighted', 'reg_lambda': np.float64(0.1), 'reg_alpha': np.float64(0.1), 'rate_drop': 0, 'random_state': 42, 'normalize_type': 'tree', 'n_estimators': 350, 'min_child_weight': 3, 'max_depth': 32, 'max_delta_step': 5, 'learning_rate': np.float64(0.0889), 'gamma': np.float64(0.05), 'estimator__n_estimators': 300, 'colsample_bytree': 0.8, 'colsample_bynode': 0.8, 'colsample_bylevel': 1.0, 'booster': 'gbtree'}\n",
            "\n",
            "Running XGBoost with SMOTETomek configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "XGBoost Model Performance Metrics\n",
            "Configuration Name:  SMOTETomek\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.945415  0.945760 0.945376   0.947253 0.977084\n",
            "    Test  0.965517  0.954943 0.966667   0.947917 0.993703\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'subsample': np.float64(0.1), 'skip_drop': 0.1, 'scale_pos_weight': 1, 'sample_type': 'weighted', 'reg_lambda': np.float64(0.1), 'reg_alpha': np.float64(0.1), 'rate_drop': 0.1, 'random_state': 49, 'normalize_type': 'forest', 'n_estimators': 370, 'min_child_weight': 2, 'max_depth': 22, 'max_delta_step': 0, 'learning_rate': np.float64(0.07780000000000001), 'gamma': np.float64(0.1), 'estimator__n_estimators': 330, 'colsample_bytree': 0.8, 'colsample_bynode': 0.6, 'colsample_bylevel': 1.0, 'booster': 'gbtree'}\n",
            "\n",
            "Running XGBoost with Normalized Data configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "XGBoost Model Performance Metrics\n",
            "Configuration Name:  Normalized Data\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.916667  0.881082 0.865955   0.899518 0.902488\n",
            "    Test  0.965517  0.972039 0.966667   0.979798 0.992625\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'subsample': np.float64(0.1), 'skip_drop': 0, 'scale_pos_weight': 5, 'sample_type': 'weighted', 'reg_lambda': np.float64(0.1), 'reg_alpha': np.float64(0.1), 'rate_drop': 0.1, 'random_state': 2, 'normalize_type': 'tree', 'n_estimators': 280, 'min_child_weight': 2, 'max_depth': 2, 'max_delta_step': 0, 'learning_rate': np.float64(0.0112), 'gamma': np.float64(0.0), 'estimator__n_estimators': 90, 'colsample_bytree': 0.3, 'colsample_bynode': 0.8, 'colsample_bylevel': 1.0, 'booster': 'gbtree'}\n",
            "\n",
            "Running XGBoost with MI configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "XGBoost Model Performance Metrics\n",
            "Configuration Name:  MI\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.912281  0.876106 0.858863   0.896785 0.935032\n",
            "    Test  0.948276  0.957588 0.950000   0.970588 0.983283\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'subsample': np.float64(0.1), 'skip_drop': 0.1, 'scale_pos_weight': 1, 'sample_type': 'weighted', 'reg_lambda': np.float64(0.1), 'reg_alpha': np.float64(0.1), 'rate_drop': 0, 'random_state': 2, 'normalize_type': 'tree', 'n_estimators': 380, 'min_child_weight': 1, 'max_depth': 2, 'max_delta_step': 0, 'learning_rate': np.float64(0.0889), 'gamma': np.float64(0.0), 'estimator__n_estimators': 460, 'colsample_bytree': 0.8, 'colsample_bynode': 0.8, 'colsample_bylevel': 1.0, 'booster': 'gbtree'}\n",
            "\n",
            "Running XGBoost with LDA configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "XGBoost Model Performance Metrics\n",
            "Configuration Name:  LDA\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.877193  0.836961 0.831607   0.843094 0.929053\n",
            "    Test  0.896552  0.914141 0.905914   0.931548 0.967217\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'subsample': np.float64(0.1), 'skip_drop': 0, 'scale_pos_weight': 5, 'sample_type': 'weighted', 'reg_lambda': np.float64(0.1), 'reg_alpha': np.float64(0.1), 'rate_drop': 0.1, 'random_state': 2, 'normalize_type': 'tree', 'n_estimators': 350, 'min_child_weight': 1, 'max_depth': 42, 'max_delta_step': 5, 'learning_rate': np.float64(0.033400000000000006), 'gamma': np.float64(0.0), 'estimator__n_estimators': 80, 'colsample_bytree': 0.3, 'colsample_bynode': 0.8, 'colsample_bylevel': 1.0, 'booster': 'gbtree'}\n",
            "\n",
            "Running XGBoost with Boruta configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "XGBoost Model Performance Metrics\n",
            "Configuration Name:  Boruta\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.916667  0.881082 0.865955   0.899518 0.906865\n",
            "    Test  0.965517  0.972039 0.966667   0.979798 0.993940\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'subsample': np.float64(0.1), 'skip_drop': 0.1, 'scale_pos_weight': 5, 'sample_type': 'weighted', 'reg_lambda': np.float64(0.1), 'reg_alpha': np.float64(0.1), 'rate_drop': 0, 'random_state': 49, 'normalize_type': 'tree', 'n_estimators': 130, 'min_child_weight': 2, 'max_depth': 2, 'max_delta_step': 0, 'learning_rate': np.float64(0.0889), 'gamma': np.float64(0.0), 'estimator__n_estimators': 410, 'colsample_bytree': 0.3, 'colsample_bynode': 0.6, 'colsample_bylevel': 1.0, 'booster': 'gbtree'}\n",
            "\n",
            "Running XGBoost with Autoencoder configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "XGBoost Model Performance Metrics\n",
            "Configuration Name:  Autoencoder\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.916667  0.881082 0.865955   0.899518 0.930648\n",
            "    Test  0.965517  0.972039 0.966667   0.979798 0.989196\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'subsample': np.float64(0.1), 'skip_drop': 0, 'scale_pos_weight': 2, 'sample_type': 'weighted', 'reg_lambda': np.float64(0.1), 'reg_alpha': np.float64(0.1), 'rate_drop': 0.1, 'random_state': 49, 'normalize_type': 'tree', 'n_estimators': 480, 'min_child_weight': 2, 'max_depth': 32, 'max_delta_step': 0, 'learning_rate': np.float64(0.07780000000000001), 'gamma': np.float64(0.05), 'estimator__n_estimators': 130, 'colsample_bytree': 0.8, 'colsample_bynode': 0.6, 'colsample_bylevel': 1.0, 'booster': 'gbtree'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Boosting"
      ],
      "metadata": {
        "id": "RQWUpTh_uU8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "configurations = []\n",
        "configurations.append(('Original Data', X_train, X_test, y_train))\n",
        "\n",
        "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
        "configurations.append(('MI', X_train_mi, X_test_mi, y_train))\n",
        "configurations.append(('LDA', X_train_lda, X_test_lda, y_train))\n",
        "\n",
        "configurations.append(('Boruta', X_train_boruta, X_test_boruta, y_train))\n",
        "configurations.append(('Autoencoder', X_train_encoded, X_test_encoded, y_train))\n",
        "\n",
        "selected_features = boruta_selector.support_\n",
        "optimal_features = sum(selected_features)\n",
        "print(f\"Optimal number of features to select using Boruta: {optimal_features}\")\n",
        "\n",
        "configurations.append(('SMOTETomek', X_train_resample, X_test_normalized, y_train_resample))\n",
        "\n",
        "# Step 4: Gradient Boosting + GridSearchCV\n",
        "print(\"\\n=== Gradient Boosting Model Performance with Hyperparameter Tuning ===\")\n",
        "\n",
        "param_grid = {\n",
        "    'loss': ['log_loss'],\n",
        "    'learning_rate': np.linspace(0.0001, 0.1, 10),\n",
        "    'n_estimators': range(40, 400, 10),\n",
        "    'subsample': np.linspace(0.1, 0.9, 3),\n",
        "    'max_depth': range(20, 60, 5),\n",
        "    'init': [None],\n",
        "    'max_leaf_nodes': [None],\n",
        "    'min_samples_split': range(2, 20, 3),\n",
        "    'min_samples_leaf': range(2, 10, 3),\n",
        "    'min_weight_fraction_leaf': [0.0],\n",
        "    'min_impurity_decrease': [0.0],\n",
        "    'validation_fraction': [0.1],\n",
        "    'n_iter_no_change': [None],\n",
        "    'tol': np.linspace(0.001, 0.05, 5),\n",
        "    'ccp_alpha': np.linspace(0.005, 0.05, 5),\n",
        "    'max_features': ['sqrt'],\n",
        "    'verbose': [0],\n",
        "    'warm_start': [False],\n",
        "    'criterion': ['friedman_mse'],\n",
        "    'random_state': [0]\n",
        "}\n",
        "\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning Gradient Boosting with {name} configuration...\")\n",
        "    gbc = RandomizedSearchCV(GradientBoostingClassifier(), param_grid, cv=10, n_iter=50, n_jobs=-1, scoring=['accuracy'], refit='accuracy', verbose=2)\n",
        "    gbc.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_gb = gbc.predict(X_train_cfg)\n",
        "    y_test_gb = gbc.predict(X_test_cfg)\n",
        "    y_train_gb_proba = gbc.predict_proba(X_train_cfg)\n",
        "    y_test_gb_proba = gbc.predict_proba(X_test_cfg)\n",
        "\n",
        "    metrics_dict = {\n",
        "          \"Dataset\": [\"Training\", \"Test\"],\n",
        "          \"Accuracy\": [\n",
        "              metrics.accuracy_score(y_train_cfg, y_train_gb),\n",
        "              metrics.accuracy_score(y_test, y_test_gb),\n",
        "          ],\n",
        "          \"F1 Score\": [\n",
        "              metrics.f1_score(y_train_cfg, y_train_gb, average='macro'),\n",
        "              metrics.f1_score(y_test, y_test_gb, average='macro'),\n",
        "          ],\n",
        "          \"Recall\": [\n",
        "              metrics.recall_score(y_train_cfg, y_train_gb, average='macro'),\n",
        "              metrics.recall_score(y_test, y_test_gb, average='macro'),\n",
        "          ],\n",
        "          \"Precision\": [\n",
        "              metrics.precision_score(y_train_cfg, y_train_gb, average='macro'),\n",
        "              metrics.precision_score(y_test, y_test_gb, average='macro'),\n",
        "          ],\n",
        "          \"AUC-ROC\": [\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_gb_proba, multi_class='ovr', average='macro'),\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_test), y_test_gb_proba, multi_class='ovr', average='macro'),\n",
        "          ]\n",
        "      }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nGradien Boosting Model Performance Metrics\")\n",
        "    print(\"Configuration Name: \", name)\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_gb_proba, multi_class='ovr', average='macro')\n",
        "    storeResults(\n",
        "          'Gradient Boosting',\n",
        "          name,\n",
        "          metrics.accuracy_score(y_test, y_test_gb),\n",
        "          metrics.f1_score(y_test, y_test_gb, average='macro'),\n",
        "          metrics.recall_score(y_test, y_test_gb, average='macro'),\n",
        "          metrics.precision_score(y_test, y_test_gb, average='macro'),\n",
        "          auc_score\n",
        "      )\n",
        "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
        "    print(gbc.best_params_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XoccNFtuYDy",
        "outputId": "f30ea84b-87f6-478c-d687-8ec94ae1d18e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal number of features to select using Boruta: 10\n",
            "\n",
            "=== Gradient Boosting Model Performance with Hyperparameter Tuning ===\n",
            "\n",
            "Running Gradient Boosting with Original Data configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "Gradien Boosting Model Performance Metrics\n",
            "Configuration Name:  Original Data\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.916667  0.881082 0.865955   0.899518 0.905339\n",
            "    Test  0.965517  0.972039 0.966667   0.979798 0.997908\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'warm_start': False, 'verbose': 0, 'validation_fraction': 0.1, 'tol': np.float64(0.025500000000000002), 'subsample': np.float64(0.5), 'random_state': 0, 'n_iter_no_change': None, 'n_estimators': 110, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 17, 'min_samples_leaf': 8, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': None, 'max_features': 'sqrt', 'max_depth': 50, 'loss': 'log_loss', 'learning_rate': np.float64(0.0889), 'init': None, 'criterion': 'friedman_mse', 'ccp_alpha': np.float64(0.01625)}\n",
            "\n",
            "Running Gradient Boosting with Normalized Data configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "Gradien Boosting Model Performance Metrics\n",
            "Configuration Name:  Normalized Data\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.916667  0.881082 0.865955   0.899518 0.904861\n",
            "    Test  0.965517  0.972039 0.966667   0.979798 0.994838\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'warm_start': False, 'verbose': 0, 'validation_fraction': 0.1, 'tol': np.float64(0.001), 'subsample': np.float64(0.1), 'random_state': 0, 'n_iter_no_change': None, 'n_estimators': 150, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 2, 'min_samples_leaf': 5, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': None, 'max_features': 'sqrt', 'max_depth': 50, 'loss': 'log_loss', 'learning_rate': np.float64(0.06670000000000001), 'init': None, 'criterion': 'friedman_mse', 'ccp_alpha': np.float64(0.05)}\n",
            "\n",
            "Running Gradient Boosting with MI configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "Gradien Boosting Model Performance Metrics\n",
            "Configuration Name:  MI\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.912281  0.876106 0.858863   0.896785 0.890037\n",
            "    Test  0.948276  0.957588 0.950000   0.970588 0.978101\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'warm_start': False, 'verbose': 0, 'validation_fraction': 0.1, 'tol': np.float64(0.037750000000000006), 'subsample': np.float64(0.9), 'random_state': 0, 'n_iter_no_change': None, 'n_estimators': 380, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 11, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': None, 'max_features': 'sqrt', 'max_depth': 30, 'loss': 'log_loss', 'learning_rate': np.float64(0.044500000000000005), 'init': None, 'criterion': 'friedman_mse', 'ccp_alpha': np.float64(0.01625)}\n",
            "\n",
            "Running Gradient Boosting with LDA configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "Gradien Boosting Model Performance Metrics\n",
            "Configuration Name:  LDA\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.890351  0.851171 0.847970   0.855332 0.942551\n",
            "    Test  0.879310  0.904838 0.906989   0.903175 0.964429\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'warm_start': False, 'verbose': 0, 'validation_fraction': 0.1, 'tol': np.float64(0.037750000000000006), 'subsample': np.float64(0.1), 'random_state': 0, 'n_iter_no_change': None, 'n_estimators': 120, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 14, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': None, 'max_features': 'sqrt', 'max_depth': 55, 'loss': 'log_loss', 'learning_rate': np.float64(0.055600000000000004), 'init': None, 'criterion': 'friedman_mse', 'ccp_alpha': np.float64(0.005)}\n",
            "\n",
            "Running Gradient Boosting with Boruta configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "Gradien Boosting Model Performance Metrics\n",
            "Configuration Name:  Boruta\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.916667  0.881082 0.865955   0.899518 0.917235\n",
            "    Test  0.965517  0.972039 0.966667   0.979798 0.997908\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'warm_start': False, 'verbose': 0, 'validation_fraction': 0.1, 'tol': np.float64(0.013250000000000001), 'subsample': np.float64(0.5), 'random_state': 0, 'n_iter_no_change': None, 'n_estimators': 90, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 2, 'min_samples_leaf': 8, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': None, 'max_features': 'sqrt', 'max_depth': 30, 'loss': 'log_loss', 'learning_rate': np.float64(0.07780000000000001), 'init': None, 'criterion': 'friedman_mse', 'ccp_alpha': np.float64(0.005)}\n",
            "\n",
            "Running Gradient Boosting with Autoencoder configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "Gradien Boosting Model Performance Metrics\n",
            "Configuration Name:  Autoencoder\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.916667  0.881082 0.865955   0.899518 0.955671\n",
            "    Test  0.965517  0.972039 0.966667   0.979798 0.994540\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'warm_start': False, 'verbose': 0, 'validation_fraction': 0.1, 'tol': np.float64(0.013250000000000001), 'subsample': np.float64(0.5), 'random_state': 0, 'n_iter_no_change': None, 'n_estimators': 280, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 2, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': None, 'max_features': 'sqrt', 'max_depth': 50, 'loss': 'log_loss', 'learning_rate': np.float64(0.0112), 'init': None, 'criterion': 'friedman_mse', 'ccp_alpha': np.float64(0.005)}\n",
            "\n",
            "Running Gradient Boosting with SMOTETomek configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "Gradien Boosting Model Performance Metrics\n",
            "Configuration Name:  SMOTETomek\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.941048  0.941528 0.941019   0.943486 0.981164\n",
            "    Test  0.965517  0.972039 0.966667   0.979798 0.992030\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'warm_start': False, 'verbose': 0, 'validation_fraction': 0.1, 'tol': np.float64(0.05), 'subsample': np.float64(0.1), 'random_state': 0, 'n_iter_no_change': None, 'n_estimators': 360, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 5, 'min_samples_leaf': 5, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': None, 'max_features': 'sqrt', 'max_depth': 45, 'loss': 'log_loss', 'learning_rate': np.float64(0.033400000000000006), 'init': None, 'criterion': 'friedman_mse', 'ccp_alpha': np.float64(0.005)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra Trees"
      ],
      "metadata": {
        "id": "EtjUT-NQw64A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "configurations = []\n",
        "configurations.append(('Original Data', X_train, X_test, y_train))\n",
        "\n",
        "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
        "configurations.append(('MI', X_train_mi, X_test_mi, y_train))\n",
        "configurations.append(('LDA', X_train_lda, X_test_lda, y_train))\n",
        "\n",
        "configurations.append(('Boruta', X_train_boruta, X_test_boruta, y_train))\n",
        "configurations.append(('Autoencoder', X_train_encoded, X_test_encoded, y_train))\n",
        "\n",
        "selected_features = boruta_selector.support_\n",
        "optimal_features = sum(selected_features)\n",
        "print(f\"Optimal number of features to select using Boruta: {optimal_features}\")\n",
        "\n",
        "configurations.append(('SMOTETomek', X_train_resample, X_test_normalized, y_train_resample))\n",
        "\n",
        "# Step 4: Extra Trees + GridSearchCV\n",
        "print(\"\\n=== Extra Trees Model Performance with Hyperparameter Tuning ===\")\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': range(300, 500, 13),\n",
        "    'max_depth': range(30, 50, 3),\n",
        "    'max_leaf_nodes': range(30, 60, 8),\n",
        "    'min_samples_split': [2, 3, 4],\n",
        "    'min_samples_leaf': [2, 3, 4],\n",
        "    'min_weight_fraction_leaf': [0.0],\n",
        "    'min_impurity_decrease': [0.0],\n",
        "    'ccp_alpha': np.linspace(0.001, 0.05, 11),\n",
        "    'max_features': ['sqrt', 'log2'],\n",
        "    'class_weight': [None],\n",
        "    'bootstrap': [True, False],\n",
        "    'oob_score': [True, False],\n",
        "    'criterion': ['gini', 'log_loss'],\n",
        "    'random_state': range(2, 10, 1),\n",
        "}\n",
        "\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning Extra Trees with {name} configuration...\")\n",
        "    etc = RandomizedSearchCV(ExtraTreesClassifier(), param_grid, cv=10, n_iter=50, n_jobs=-1, scoring=[\"accuracy\", \"f1_macro\"], refit='accuracy', verbose=2)\n",
        "    etc.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_et = etc.predict(X_train_cfg)\n",
        "    y_test_et = etc.predict(X_test_cfg)\n",
        "    y_train_et_proba = etc.predict_proba(X_train_cfg)\n",
        "    y_test_et_proba = etc.predict_proba(X_test_cfg)\n",
        "\n",
        "    metrics_dict = {\n",
        "          \"Dataset\": [\"Training\", \"Test\"],\n",
        "          \"Accuracy\": [\n",
        "              metrics.accuracy_score(y_train_cfg, y_train_et),\n",
        "              metrics.accuracy_score(y_test, y_test_et),\n",
        "          ],\n",
        "          \"F1 Score\": [\n",
        "              metrics.f1_score(y_train_cfg, y_train_et, average='macro'),\n",
        "              metrics.f1_score(y_test, y_test_et, average='macro'),\n",
        "          ],\n",
        "          \"Recall\": [\n",
        "              metrics.recall_score(y_train_cfg, y_train_et, average='macro'),\n",
        "              metrics.recall_score(y_test, y_test_et, average='macro'),\n",
        "          ],\n",
        "          \"Precision\": [\n",
        "              metrics.precision_score(y_train_cfg, y_train_et, average='macro'),\n",
        "              metrics.precision_score(y_test, y_test_et, average='macro'),\n",
        "          ],\n",
        "          \"AUC-ROC\": [\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_et_proba, multi_class='ovr', average='macro'),\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_test), y_test_et_proba, multi_class='ovr', average='macro'),\n",
        "          ]\n",
        "      }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nExtraTrees Model Performance Metrics\")\n",
        "    print(\"Configuration Name: \", name)\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_et_proba, multi_class='ovr', average='macro')\n",
        "    storeResults(\n",
        "          'Extra Trees',\n",
        "          name,\n",
        "          metrics.accuracy_score(y_test, y_test_et),\n",
        "          metrics.f1_score(y_test, y_test_et, average='macro'),\n",
        "          metrics.recall_score(y_test, y_test_et, average='macro'),\n",
        "          metrics.precision_score(y_test, y_test_et, average='macro'),\n",
        "          auc_score\n",
        "      )\n",
        "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
        "    print(etc.best_params_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01gypP_5xEKE",
        "outputId": "83a82d39-64ac-4774-9034-bd061acac093"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal number of features to select using Boruta: 10\n",
            "\n",
            "=== Extra Trees Model Performance with Hyperparameter Tuning ===\n",
            "\n",
            "Running Extra Trees with Original Data configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "ExtraTrees Model Performance Metrics\n",
            "Configuration Name:  Original Data\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.916667  0.881082 0.865955   0.899518 0.968569\n",
            "    Test  0.965517  0.972039 0.966667   0.979798 0.996254\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'random_state': 2, 'oob_score': False, 'n_estimators': 339, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 4, 'min_samples_leaf': 4, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': 38, 'max_features': 'log2', 'max_depth': 30, 'criterion': 'log_loss', 'class_weight': None, 'ccp_alpha': np.float64(0.001), 'bootstrap': False}\n",
            "\n",
            "Running Extra Trees with Normalized Data configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "ExtraTrees Model Performance Metrics\n",
            "Configuration Name:  Normalized Data\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.916667  0.881082 0.865955   0.899518 0.939071\n",
            "    Test  0.965517  0.972039 0.966667   0.979798 0.991274\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'random_state': 5, 'oob_score': False, 'n_estimators': 391, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 2, 'min_samples_leaf': 4, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': 46, 'max_features': 'log2', 'max_depth': 36, 'criterion': 'log_loss', 'class_weight': None, 'ccp_alpha': np.float64(0.0353), 'bootstrap': False}\n",
            "\n",
            "Running Extra Trees with MI configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "ExtraTrees Model Performance Metrics\n",
            "Configuration Name:  MI\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.899123  0.860691 0.837586   0.888518 0.908771\n",
            "    Test  0.948276  0.957588 0.950000   0.970588 0.976508\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'random_state': 6, 'oob_score': True, 'n_estimators': 300, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 4, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': 46, 'max_features': 'sqrt', 'max_depth': 30, 'criterion': 'gini', 'class_weight': None, 'ccp_alpha': np.float64(0.0451), 'bootstrap': True}\n",
            "\n",
            "Running Extra Trees with LDA configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "ExtraTrees Model Performance Metrics\n",
            "Configuration Name:  LDA\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.907895  0.871839 0.856684   0.890302 0.958715\n",
            "    Test  0.931034  0.944079 0.939247   0.951178 0.975787\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'random_state': 8, 'oob_score': False, 'n_estimators': 495, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 4, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': 54, 'max_features': 'sqrt', 'max_depth': 48, 'criterion': 'gini', 'class_weight': None, 'ccp_alpha': np.float64(0.001), 'bootstrap': False}\n",
            "\n",
            "Running Extra Trees with Boruta configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "ExtraTrees Model Performance Metrics\n",
            "Configuration Name:  Boruta\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.916667  0.881082 0.865955   0.899518 0.934037\n",
            "    Test  0.965517  0.972039 0.966667   0.979798 0.994182\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'random_state': 2, 'oob_score': False, 'n_estimators': 417, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 3, 'min_samples_leaf': 3, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': 38, 'max_features': 'sqrt', 'max_depth': 30, 'criterion': 'log_loss', 'class_weight': None, 'ccp_alpha': np.float64(0.0402), 'bootstrap': False}\n",
            "\n",
            "Running Extra Trees with Autoencoder configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "ExtraTrees Model Performance Metrics\n",
            "Configuration Name:  Autoencoder\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.916667  0.881082 0.865955   0.899518 0.921691\n",
            "    Test  0.965517  0.972039 0.966667   0.979798 0.987407\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'random_state': 3, 'oob_score': True, 'n_estimators': 339, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 2, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': 46, 'max_features': 'sqrt', 'max_depth': 42, 'criterion': 'gini', 'class_weight': None, 'ccp_alpha': np.float64(0.05), 'bootstrap': True}\n",
            "\n",
            "Running Extra Trees with SMOTETomek configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "ExtraTrees Model Performance Metrics\n",
            "Configuration Name:  SMOTETomek\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.938865  0.939182 0.938869   0.939975 0.981644\n",
            "    Test  0.931034  0.907239 0.945161   0.888889 0.983737\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'random_state': 7, 'oob_score': False, 'n_estimators': 352, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 3, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': 38, 'max_features': 'sqrt', 'max_depth': 39, 'criterion': 'log_loss', 'class_weight': None, 'ccp_alpha': np.float64(0.0108), 'bootstrap': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ADABoost"
      ],
      "metadata": {
        "id": "gSVf2NIHes4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "configurations = []\n",
        "configurations.append(('Original Data', X_train, X_test, y_train))\n",
        "\n",
        "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
        "configurations.append(('MI', X_train_mi, X_test_mi, y_train))\n",
        "configurations.append(('LDA', X_train_lda, X_test_lda, y_train))\n",
        "\n",
        "configurations.append(('Boruta', X_train_boruta, X_test_boruta, y_train))\n",
        "configurations.append(('Autoencoder', X_train_encoded, X_test_encoded, y_train))\n",
        "\n",
        "selected_features = boruta_selector.support_\n",
        "optimal_features = sum(selected_features)\n",
        "print(f\"Optimal number of features to select using Boruta: {optimal_features}\")\n",
        "\n",
        "configurations.append(('SMOTETomek', X_train_resample, X_test_normalized, y_train_resample))\n",
        "\n",
        "# Step 4: Extra Trees + GridSearchCV\n",
        "print(\"\\n=== AdaBoost Model Performance with Hyperparameter Tuning ===\")\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': range(300, 450, 13), #[50, 150],\n",
        "    'learning_rate': np.linspace(0.01, 0.05, 3), #[0.005, 0.5, 0.03, 0.003],\n",
        "    'estimator__max_depth': range(2, 10, 3), #[5, 20],\n",
        "    'estimator__min_samples_split': range(1, 5, 1), #[8],\n",
        "    'random_state': range(20, 60) #[42, 1234]\n",
        "}\n",
        "\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning AdaBoost with {name} configuration...\")\n",
        "    # adb = RandomizedSearchCV(AdaBoostClassifier(estimator=DecisionTreeClassifier()), param_grid, cv=10, n_iter=50, n_jobs=-1,\n",
        "    #                          scoring=['accuracy', 'f1_macro'], refit='accuracy', verbose=2)\n",
        "    adb = AdaBoostClassifier()\n",
        "    adb.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_ad = adb.predict(X_train_cfg)\n",
        "    y_test_ad = adb.predict(X_test_cfg)\n",
        "    y_train_ad_proba = adb.predict_proba(X_train_cfg)\n",
        "    y_test_ad_proba = adb.predict_proba(X_test_cfg)\n",
        "\n",
        "    metrics_dict = {\n",
        "          \"Dataset\": [\"Training\", \"Test\"],\n",
        "          \"Accuracy\": [\n",
        "              metrics.accuracy_score(y_train_cfg, y_train_ad),\n",
        "              metrics.accuracy_score(y_test, y_test_ad),\n",
        "          ],\n",
        "          \"F1 Score\": [\n",
        "              metrics.f1_score(y_train_cfg, y_train_ad, average='macro'),\n",
        "              metrics.f1_score(y_test, y_test_ad, average='macro'),\n",
        "          ],\n",
        "          \"Recall\": [\n",
        "              metrics.recall_score(y_train_cfg, y_train_ad, average='macro'),\n",
        "              metrics.recall_score(y_test, y_test_ad, average='macro'),\n",
        "          ],\n",
        "          \"Precision\": [\n",
        "              metrics.precision_score(y_train_cfg, y_train_ad, average='macro'),\n",
        "              metrics.precision_score(y_test, y_test_ad, average='macro'),\n",
        "          ],\n",
        "          \"AUC-ROC\": [\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_ad_proba, multi_class='ovr', average='macro'),\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_test), y_test_ad_proba, multi_class='ovr', average='macro'),\n",
        "          ]\n",
        "      }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nAdaBoost Model Performance Metrics\")\n",
        "    print(\"Configuration Name: \", name)\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_ad_proba, multi_class='ovr', average='macro')\n",
        "    storeResults(\n",
        "          'AdaBoost',\n",
        "          name,\n",
        "          metrics.accuracy_score(y_test, y_test_ad),\n",
        "          metrics.f1_score(y_test, y_test_ad, average='macro'),\n",
        "          metrics.recall_score(y_test, y_test_ad, average='macro'),\n",
        "          metrics.precision_score(y_test, y_test_ad, average='macro'),\n",
        "          auc_score\n",
        "      )\n",
        "    # print(\"Best hyperparameters found by GridSearchCV:\")\n",
        "    # print(adb.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZClculzemGl",
        "outputId": "c04bb90d-ace6-4097-f6cc-88d74e42d9f2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal number of features to select using Boruta: 10\n",
            "\n",
            "=== AdaBoost Model Performance with Hyperparameter Tuning ===\n",
            "\n",
            "Running AdaBoost with Original Data configuration...\n",
            "\n",
            "AdaBoost Model Performance Metrics\n",
            "Configuration Name:  Original Data\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.907895  0.870594 0.861598   0.883355 0.954151\n",
            "    Test  0.965517  0.972581 0.972581   0.972581 0.991450\n",
            "\n",
            "Running AdaBoost with Normalized Data configuration...\n",
            "\n",
            "AdaBoost Model Performance Metrics\n",
            "Configuration Name:  Normalized Data\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.907895  0.870594 0.861598   0.883355 0.954151\n",
            "    Test  0.965517  0.972581 0.972581   0.972581 0.991450\n",
            "\n",
            "Running AdaBoost with MI configuration...\n",
            "\n",
            "AdaBoost Model Performance Metrics\n",
            "Configuration Name:  MI\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.907895  0.866700 0.861598   0.872586 0.945570\n",
            "    Test  0.982759  0.986162 0.983333   0.989583 0.982966\n",
            "\n",
            "Running AdaBoost with LDA configuration...\n",
            "\n",
            "AdaBoost Model Performance Metrics\n",
            "Configuration Name:  LDA\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.850877  0.781843 0.760178   0.826181 0.903943\n",
            "    Test  0.844828  0.819610 0.799923   0.848086 0.932632\n",
            "\n",
            "Running AdaBoost with Boruta configuration...\n",
            "\n",
            "AdaBoost Model Performance Metrics\n",
            "Configuration Name:  Boruta\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.916667  0.878962 0.865955   0.895299 0.945541\n",
            "    Test  0.965517  0.972581 0.972581   0.972581 0.984282\n",
            "\n",
            "Running AdaBoost with Autoencoder configuration...\n",
            "\n",
            "AdaBoost Model Performance Metrics\n",
            "Configuration Name:  Autoencoder\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.916667  0.881082 0.865955   0.899518 0.959115\n",
            "    Test  0.965517  0.972039 0.966667   0.979798 0.981781\n",
            "\n",
            "Running AdaBoost with SMOTETomek configuration...\n",
            "\n",
            "AdaBoost Model Performance Metrics\n",
            "Configuration Name:  SMOTETomek\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.925764  0.926323 0.925697   0.929354 0.972499\n",
            "    Test  0.931034  0.928051 0.945161   0.913889 0.983460\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP"
      ],
      "metadata": {
        "id": "_TA7vnOBtT3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "configurations = []\n",
        "configurations.append(('Original Data', X_train, X_test, y_train))\n",
        "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
        "configurations.append(('MI', X_train_mi, X_test_mi, y_train))\n",
        "configurations.append(('LDA', X_train_lda, X_test_lda, y_train))\n",
        "configurations.append(('Boruta', X_train_boruta, X_test_boruta, y_train))\n",
        "configurations.append(('Autoencoder', X_train_encoded, X_test_encoded, y_train))\n",
        "configurations.append(('SMOTETomek', X_train_resample, X_test_normalized, y_train_resample))\n",
        "\n",
        "mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(100, 50),\n",
        "    activation='relu',\n",
        "    solver='sgd',\n",
        "    alpha=0.01,\n",
        "    batch_size='auto',\n",
        "    learning_rate='constant',\n",
        "    max_iter=1000,\n",
        "    random_state=42)\n",
        "\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning MLP Classifier with {name} configuration...\")\n",
        "    mlp.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_mlp = mlp.predict(X_train_cfg)\n",
        "    y_test_mlp = mlp.predict(X_test_cfg)\n",
        "    y_train_mlp_proba = mlp.predict_proba(X_train_cfg)\n",
        "    y_test_mlp_proba = mlp.predict_proba(X_test_cfg)\n",
        "\n",
        "    metrics_dict = {\n",
        "          \"Dataset\": [\"Training\", \"Test\"],\n",
        "          \"Accuracy\": [\n",
        "              metrics.accuracy_score(y_train_cfg, y_train_mlp),\n",
        "              metrics.accuracy_score(y_test, y_test_mlp),\n",
        "          ],\n",
        "          \"F1 Score\": [\n",
        "              metrics.f1_score(y_train_cfg, y_train_mlp, average='macro'),\n",
        "              metrics.f1_score(y_test, y_test_mlp, average='macro'),\n",
        "          ],\n",
        "          \"Recall\": [\n",
        "              metrics.recall_score(y_train_cfg, y_train_mlp, average='macro'),\n",
        "              metrics.recall_score(y_test, y_test_mlp, average='macro'),\n",
        "          ],\n",
        "          \"Precision\": [\n",
        "              metrics.precision_score(y_train_cfg, y_train_mlp, average='macro'),\n",
        "              metrics.precision_score(y_test, y_test_mlp, average='macro'),\n",
        "          ],\n",
        "          \"AUC-ROC\": [\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_mlp_proba, multi_class='ovr', average='macro'),\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_test), y_test_mlp_proba, multi_class='ovr', average='macro'),\n",
        "          ]\n",
        "      }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\MLP Model Performance Metrics\")\n",
        "    print(\"Configuration Name: \", name)\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_mlp_proba, multi_class='ovr', average='macro')\n",
        "    storeResults(\n",
        "          'MLP Classifier',\n",
        "          name,\n",
        "          metrics.accuracy_score(y_test, y_test_mlp),\n",
        "          metrics.f1_score(y_test, y_test_mlp, average='macro'),\n",
        "          metrics.recall_score(y_test, y_test_mlp, average='macro'),\n",
        "          metrics.precision_score(y_test, y_test_mlp, average='macro'),\n",
        "          auc_score\n",
        "      )\n",
        "    # print(\"Best hyperparameters found by GridSearchCV:\")\n",
        "    # print(mlp.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScDMjCP9tTZH",
        "outputId": "66dfcbc3-8691-4912-e0f9-2cd6677ca9ca"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running MLP Classifier with Original Data configuration...\n",
            "\\MLP Model Performance Metrics\n",
            "Configuration Name:  Original Data\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.671053  0.267717 0.333333   0.223684      0.5\n",
            "    Test  0.534483  0.232210 0.333333   0.178161      0.5\n",
            "\n",
            "Running MLP Classifier with Normalized Data configuration...\n",
            "\\MLP Model Performance Metrics\n",
            "Configuration Name:  Normalized Data\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.916667  0.881082 0.865955   0.899518 0.925551\n",
            "    Test  0.965517  0.972039 0.966667   0.979798 0.992347\n",
            "\n",
            "Running MLP Classifier with MI configuration...\n",
            "\\MLP Model Performance Metrics\n",
            "Configuration Name:  MI\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.903509  0.867630 0.854506   0.884081 0.904625\n",
            "    Test  0.948276  0.957588 0.950000   0.970588 0.960688\n",
            "\n",
            "Running MLP Classifier with LDA configuration...\n",
            "\\MLP Model Performance Metrics\n",
            "Configuration Name:  LDA\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.864035  0.819471 0.825071   0.814444 0.897752\n",
            "    Test  0.879310  0.901040 0.895161   0.911765 0.935820\n",
            "\n",
            "Running MLP Classifier with Boruta configuration...\n",
            "\\MLP Model Performance Metrics\n",
            "Configuration Name:  Boruta\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.916667  0.881082 0.865955   0.899518 0.912721\n",
            "    Test  0.965517  0.972039 0.966667   0.979798 0.987603\n",
            "\n",
            "Running MLP Classifier with Autoencoder configuration...\n",
            "\\MLP Model Performance Metrics\n",
            "Configuration Name:  Autoencoder\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.912281  0.876805 0.863776   0.893162 0.915102\n",
            "    Test  0.965517  0.972039 0.966667   0.979798 0.993063\n",
            "\n",
            "Running MLP Classifier with SMOTETomek configuration...\n",
            "\\MLP Model Performance Metrics\n",
            "Configuration Name:  SMOTETomek\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.906114  0.907240 0.906060   0.912438 0.974124\n",
            "    Test  0.931034  0.907239 0.945161   0.888889 0.986449\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stacking classifier"
      ],
      "metadata": {
        "id": "cGw9YygPuPPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "configurations = []\n",
        "configurations.append(('Original Data', X_train, X_test, y_train))\n",
        "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
        "configurations.append(('MI', X_train_mi, X_test_mi, y_train))\n",
        "configurations.append(('LDA', X_train_lda, X_test_lda, y_train))\n",
        "configurations.append(('Boruta', X_train_boruta, X_test_boruta, y_train))\n",
        "configurations.append(('Autoencoder', X_train_encoded, X_test_encoded, y_train))\n",
        "configurations.append(('SMOTETomek', X_train_resample, X_test_normalized, y_train_resample))\n",
        "\n",
        "xgb_params = {'subsample': np.float64(0.1), 'skip_drop': 0, 'scale_pos_weight': 5,\n",
        "                    'sample_type': 'weighted', 'reg_lambda': np.float64(0.1), 'reg_alpha': np.float64(0.1),\n",
        "                    'rate_drop': 0, 'random_state': 42, 'normalize_type': 'tree', 'n_estimators': 260,\n",
        "                    'min_child_weight': 1, 'max_depth': 12, 'max_delta_step': 0, 'learning_rate': np.float64(0.0889),\n",
        "                    'gamma': np.float64(0.1), 'estimator__n_estimators': 320, 'colsample_bytree': 0.8,\n",
        "                    'colsample_bynode': 0.8, 'colsample_bylevel': 1.0, 'booster': 'gbtree'}\n",
        "rf_params = {'random_state': 22, 'n_estimators': 340, 'min_samples_split': 4, 'min_samples_leaf': 5,\n",
        "                            'min_impurity_decrease': np.float64(0.0112), 'max_leaf_nodes': 12, 'max_features': 'sqrt',\n",
        "                            'max_depth': 42, 'criterion': 'log_loss', 'class_weight': 'balanced',\n",
        "                            'ccp_alpha': np.float64(0.0112), 'bootstrap': False}\n",
        "gbc_params = {'warm_start': False, 'verbose': 0, 'validation_fraction': 0.1, 'tol': np.float64(0.07780000000000001),\n",
        "              'subsample': np.float64(0.9), 'random_state': 0, 'n_iter_no_change': None, 'n_estimators': 310,\n",
        "              'min_weight_fraction_leaf': 0.0, 'min_samples_split': 41, 'min_samples_leaf': 29,\n",
        "              'min_impurity_decrease': 0.0, 'max_leaf_nodes': None, 'max_features': 'sqrt', 'max_depth': 27,\n",
        "              'loss': 'log_loss', 'learning_rate': np.float64(0.033400000000000006), 'init': None,\n",
        "              'criterion': 'friedman_mse', 'ccp_alpha': np.float64(0.0001)}\n",
        "knn_params = {'weights': 'distance', 'p': np.int64(1), 'n_neighbors': np.int64(6), 'metric': 'manhattan'}\n",
        "logr_params = {'solver': 'liblinear', 'penalty': 'l2', 'max_iter': 456, 'C': np.float64(0.01)}\n",
        "\n",
        "estimators = [\n",
        "    ('xgb', XGBClassifier(**xgb_params)),\n",
        "    ('rf', RandomForestClassifier(**rf_params)),\n",
        "    ('gbc', GradientBoostingClassifier(**gbc_params)),\n",
        "    ('knn', KNeighborsClassifier(**knn_params)),\n",
        "    ('adb', AdaBoostClassifier()),\n",
        "    ('logr', LogisticRegression(**logr_params))\n",
        "]\n",
        "\n",
        "log_reg_params = {'solver': 'liblinear', 'penalty': 'l2', 'max_iter': 816, 'C': np.float64(0.01)}\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LogisticRegression(**log_reg_params),\n",
        "    cv = 5,\n",
        "    stack_method = \"predict_proba\",\n",
        "    n_jobs=-1\n",
        "    )\n",
        "\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning Stacking Classifier with {name} configuration...\")\n",
        "    stacking_clf.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_stc = stacking_clf.predict(X_train_cfg)\n",
        "    y_test_stc = stacking_clf.predict(X_test_cfg)\n",
        "    y_train_stc_proba = stacking_clf.predict_proba(X_train_cfg)\n",
        "    y_test_stc_proba = stacking_clf.predict_proba(X_test_cfg)\n",
        "\n",
        "    metrics_dict = {\n",
        "          \"Dataset\": [\"Training\", \"Test\"],\n",
        "          \"Accuracy\": [\n",
        "              metrics.accuracy_score(y_train_cfg, y_train_stc),\n",
        "              metrics.accuracy_score(y_test, y_test_stc),\n",
        "          ],\n",
        "          \"F1 Score\": [\n",
        "              metrics.f1_score(y_train_cfg, y_train_stc, average='macro'),\n",
        "              metrics.f1_score(y_test, y_test_stc, average='macro'),\n",
        "          ],\n",
        "          \"Recall\": [\n",
        "              metrics.recall_score(y_train_cfg, y_train_stc, average='macro'),\n",
        "              metrics.recall_score(y_test, y_test_stc, average='macro'),\n",
        "          ],\n",
        "          \"Precision\": [\n",
        "              metrics.precision_score(y_train_cfg, y_train_stc, average='macro'),\n",
        "              metrics.precision_score(y_test, y_test_stc, average='macro'),\n",
        "          ],\n",
        "          \"AUC-ROC\": [\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_stc_proba, multi_class='ovr', average='macro'),\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_test), y_test_stc_proba, multi_class='ovr', average='macro'),\n",
        "          ]\n",
        "      }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\Stacking Classifier Performance Metrics\")\n",
        "    print(\"Configuration Name: \", name)\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_stc_proba, multi_class='ovr', average='macro')\n",
        "    storeResults(\n",
        "          'Stacking Classifier',\n",
        "          name,\n",
        "          metrics.accuracy_score(y_test, y_test_stc),\n",
        "          metrics.f1_score(y_test, y_test_stc, average='macro'),\n",
        "          metrics.recall_score(y_test, y_test_stc, average='macro'),\n",
        "          metrics.precision_score(y_test, y_test_stc, average='macro'),\n",
        "          auc_score\n",
        "      )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPcypVO3uSfZ",
        "outputId": "63611969-f41b-441e-ec0c-dc8bebfeb6b6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running Stacking Classifier with Original Data configuration...\n",
            "\\Stacking Classifier Performance Metrics\n",
            "Configuration Name:  Original Data\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.842105  0.599583 0.624670   0.579073 0.959595\n",
            "    Test  0.844828  0.606869 0.633333   0.591667 0.991389\n",
            "\n",
            "Running Stacking Classifier with Normalized Data configuration...\n",
            "\\Stacking Classifier Performance Metrics\n",
            "Configuration Name:  Normalized Data\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.842105  0.599583 0.624670   0.579073 0.961763\n",
            "    Test  0.844828  0.606869 0.633333   0.591667 0.992226\n",
            "\n",
            "Running Stacking Classifier with MI configuration...\n",
            "\\Stacking Classifier Performance Metrics\n",
            "Configuration Name:  MI\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.833333  0.590424 0.615399   0.569995 0.944603\n",
            "    Test  0.827586  0.593343 0.616667   0.585366 0.989696\n",
            "\n",
            "Running Stacking Classifier with LDA configuration...\n",
            "\\Stacking Classifier Performance Metrics\n",
            "Configuration Name:  LDA\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.793860  0.539272 0.541742   0.561150 0.945839\n",
            "    Test  0.706897  0.483826 0.500000   0.548611 0.983757\n",
            "\n",
            "Running Stacking Classifier with Boruta configuration...\n",
            "\\Stacking Classifier Performance Metrics\n",
            "Configuration Name:  Boruta\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.842105  0.599583 0.624670   0.579073 0.958965\n",
            "    Test  0.844828  0.606869 0.633333   0.591667 0.992226\n",
            "\n",
            "Running Stacking Classifier with Autoencoder configuration...\n",
            "\\Stacking Classifier Performance Metrics\n",
            "Configuration Name:  Autoencoder\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.833333  0.588353 0.600658   0.586170 0.971776\n",
            "    Test  0.827586  0.593343 0.616667   0.585366 0.990991\n",
            "\n",
            "Running Stacking Classifier with SMOTETomek configuration...\n",
            "\\Stacking Classifier Performance Metrics\n",
            "Configuration Name:  SMOTETomek\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.960699  0.960801 0.960698   0.961028 0.990504\n",
            "    Test  0.965517  0.954943 0.966667   0.947917 0.989398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Result\n"
      ],
      "metadata": {
        "id": "DZjM6oTkrnlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the dataframe\n",
        "result = pd.DataFrame({\n",
        "    'ML Model': ML_Model,\n",
        "    'Configuration': ML_Config,\n",
        "    'Accuracy': [f\"{acc * 100:.3f}%\" for acc in accuracy],\n",
        "    'F1 Score': [f\"{f1 * 100:.3f}%\" for f1 in f1_score],\n",
        "    'Recall': [f\"{rec * 100:.3f}%\" for rec in recall],\n",
        "    'Precision': [f\"{prec * 100:.3f}%\" for prec in precision],\n",
        "    'ROC_AUC': [f\"{roc * 100:.3f}%\" for roc in auc_roc],\n",
        "})\n",
        "\n",
        "# Remove duplicates based on model and configuration\n",
        "result.drop_duplicates(subset=[\"ML Model\", \"Configuration\"], inplace=True)\n",
        "\n",
        "# Display the result\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"MODEL PERFORMANCE RESULTS\")\n",
        "print(\"=\" * 100)\n",
        "print(result.to_string(index=False))\n",
        "\n",
        "# Save the result to a CSV file\n",
        "# result.to_csv('final_results/model_results.csv', index=False)\n",
        "# print(\"\\nResults saved to model_results.csv\")\n",
        "\n",
        "# Sort by Accuracy and F1 Score\n",
        "sorted_result = result.sort_values(by=['Accuracy', 'F1 Score'], ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Display the sorted result\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"SORTED MODEL PERFORMANCE RESULTS (by Accuracy and F1 Score)\")\n",
        "print(\"=\" * 100)\n",
        "print(sorted_result.to_string(index=False))\n",
        "\n",
        "# Save the sorted result\n",
        "# sorted_result.to_csv('final_results/sorted_model_results.csv', index=False)\n",
        "# print(\"\\nSorted results saved to sorted_model_results.csv\")\n",
        "\n",
        "# Extract top configuration per ML model\n",
        "top_per_model = sorted_result.groupby('ML Model', as_index=False).first()\n",
        "\n",
        "# Display and save the top configuration table\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"TOP CONFIGURATION PER MODEL\")\n",
        "print(\"=\" * 100)\n",
        "print(top_per_model.to_string(index=False))\n",
        "\n",
        "# top_per_model.to_csv('final_results/top_configurations.csv', index=False)\n",
        "# print(\"\\nTop configuration per model saved to top_configurations.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jgo6Ddxwr281",
        "outputId": "597e1cd3-87d7-4fc3-d57d-008223773115"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================================================================================================\n",
            "MODEL PERFORMANCE RESULTS\n",
            "====================================================================================================\n",
            "           ML Model   Configuration Accuracy F1 Score  Recall Precision ROC_AUC\n",
            "Logistic Regression   Original Data  96.552%  97.204% 96.667%   97.980% 98.150%\n",
            "Logistic Regression      SMOTETomek  87.931%  90.721% 91.882%   90.718% 98.796%\n",
            "Logistic Regression Normalized Data  98.276%  98.616% 98.333%   98.958% 98.840%\n",
            "Logistic Regression              MI  53.448%  23.221% 33.333%   17.816% 81.240%\n",
            "Logistic Regression             LDA  87.931%  90.104% 89.516%   91.177% 92.996%\n",
            "Logistic Regression          Boruta  96.552%  97.204% 96.667%   97.980% 98.621%\n",
            "Logistic Regression     Autoencoder  53.448%  23.221% 33.333%   17.816% 86.795%\n",
            "K-Nearest Neighbors   Original Data  96.552%  95.494% 96.667%   94.792% 98.289%\n",
            "K-Nearest Neighbors      SMOTETomek  96.552%  95.494% 96.667%   94.792% 98.289%\n",
            "K-Nearest Neighbors Normalized Data  96.552%  95.494% 96.667%   94.792% 98.289%\n",
            "K-Nearest Neighbors              MI  96.552%  95.494% 96.667%   94.792% 98.382%\n",
            "K-Nearest Neighbors             LDA  96.552%  95.494% 96.667%   94.792% 98.124%\n",
            "K-Nearest Neighbors          Boruta  96.552%  95.494% 96.667%   94.792% 98.401%\n",
            "K-Nearest Neighbors     Autoencoder  96.552%  95.494% 96.667%   94.792% 97.111%\n",
            "      Random Forest   Original Data  96.552%  95.494% 96.667%   94.792% 99.187%\n",
            "      Random Forest      SMOTETomek  96.552%  95.494% 96.667%   94.792% 98.633%\n",
            "      Random Forest Normalized Data  96.552%  97.204% 96.667%   97.980% 99.755%\n",
            "      Random Forest              MI  93.103%  90.724% 94.516%   88.889% 99.580%\n",
            "      Random Forest             LDA  87.931%  87.500% 90.699%   85.294% 95.056%\n",
            "      Random Forest          Boruta  96.552%  97.204% 96.667%   97.980% 99.596%\n",
            "      Random Forest     Autoencoder  94.828%  94.436% 95.591%   93.750% 98.661%\n",
            "           AdaBoost   Original Data  96.552%  97.258% 97.258%   97.258% 99.145%\n",
            "           AdaBoost Normalized Data  96.552%  97.258% 97.258%   97.258% 99.145%\n",
            "           AdaBoost              MI  98.276%  98.616% 98.333%   98.958% 98.297%\n",
            "           AdaBoost             LDA  84.483%  81.961% 79.992%   84.809% 93.263%\n",
            "           AdaBoost          Boruta  96.552%  97.258% 97.258%   97.258% 98.428%\n",
            "           AdaBoost     Autoencoder  96.552%  97.204% 96.667%   97.980% 98.178%\n",
            "           AdaBoost      SMOTETomek  93.103%  92.805% 94.516%   91.389% 98.346%\n",
            "     MLP Classifier   Original Data  53.448%  23.221% 33.333%   17.816% 50.000%\n",
            "     MLP Classifier Normalized Data  96.552%  97.204% 96.667%   97.980% 99.235%\n",
            "     MLP Classifier              MI  94.828%  95.759% 95.000%   97.059% 96.069%\n",
            "     MLP Classifier             LDA  87.931%  90.104% 89.516%   91.177% 93.582%\n",
            "     MLP Classifier          Boruta  96.552%  97.204% 96.667%   97.980% 98.760%\n",
            "     MLP Classifier     Autoencoder  96.552%  97.204% 96.667%   97.980% 99.306%\n",
            "     MLP Classifier      SMOTETomek  93.103%  90.724% 94.516%   88.889% 98.645%\n",
            "Stacking Classifier   Original Data  84.483%  60.687% 63.333%   59.167% 99.139%\n",
            "Stacking Classifier Normalized Data  84.483%  60.687% 63.333%   59.167% 99.223%\n",
            "Stacking Classifier              MI  82.759%  59.334% 61.667%   58.537% 98.970%\n",
            "Stacking Classifier             LDA  70.690%  48.383% 50.000%   54.861% 98.376%\n",
            "Stacking Classifier          Boruta  84.483%  60.687% 63.333%   59.167% 99.223%\n",
            "Stacking Classifier     Autoencoder  82.759%  59.334% 61.667%   58.537% 99.099%\n",
            "Stacking Classifier      SMOTETomek  96.552%  95.494% 96.667%   94.792% 98.940%\n",
            "        Extra Trees   Original Data  96.552%  97.204% 96.667%   97.980% 99.625%\n",
            "        Extra Trees Normalized Data  96.552%  97.204% 96.667%   97.980% 99.127%\n",
            "        Extra Trees              MI  94.828%  95.759% 95.000%   97.059% 97.651%\n",
            "        Extra Trees             LDA  93.103%  94.408% 93.925%   95.118% 97.579%\n",
            "        Extra Trees          Boruta  96.552%  97.204% 96.667%   97.980% 99.418%\n",
            "        Extra Trees     Autoencoder  96.552%  97.204% 96.667%   97.980% 98.741%\n",
            "        Extra Trees      SMOTETomek  93.103%  90.724% 94.516%   88.889% 98.374%\n",
            "  Gradient Boosting   Original Data  96.552%  97.204% 96.667%   97.980% 99.791%\n",
            "  Gradient Boosting Normalized Data  96.552%  97.204% 96.667%   97.980% 99.484%\n",
            "  Gradient Boosting              MI  94.828%  95.759% 95.000%   97.059% 97.810%\n",
            "  Gradient Boosting             LDA  87.931%  90.484% 90.699%   90.317% 96.443%\n",
            "  Gradient Boosting          Boruta  96.552%  97.204% 96.667%   97.980% 99.791%\n",
            "  Gradient Boosting     Autoencoder  96.552%  97.204% 96.667%   97.980% 99.454%\n",
            "  Gradient Boosting      SMOTETomek  96.552%  97.204% 96.667%   97.980% 99.203%\n",
            "      XGBoost Model   Original Data  96.552%  97.204% 96.667%   97.980% 99.482%\n",
            "      XGBoost Model      SMOTETomek  96.552%  95.494% 96.667%   94.792% 99.370%\n",
            "      XGBoost Model Normalized Data  96.552%  97.204% 96.667%   97.980% 99.263%\n",
            "      XGBoost Model              MI  94.828%  95.759% 95.000%   97.059% 98.328%\n",
            "      XGBoost Model             LDA  89.655%  91.414% 90.591%   93.155% 96.722%\n",
            "      XGBoost Model          Boruta  96.552%  97.204% 96.667%   97.980% 99.394%\n",
            "      XGBoost Model     Autoencoder  96.552%  97.204% 96.667%   97.980% 98.920%\n",
            "\n",
            "====================================================================================================\n",
            "SORTED MODEL PERFORMANCE RESULTS (by Accuracy and F1 Score)\n",
            "====================================================================================================\n",
            "           ML Model   Configuration Accuracy F1 Score  Recall Precision ROC_AUC\n",
            "Logistic Regression Normalized Data  98.276%  98.616% 98.333%   98.958% 98.840%\n",
            "           AdaBoost              MI  98.276%  98.616% 98.333%   98.958% 98.297%\n",
            "           AdaBoost   Original Data  96.552%  97.258% 97.258%   97.258% 99.145%\n",
            "           AdaBoost Normalized Data  96.552%  97.258% 97.258%   97.258% 99.145%\n",
            "           AdaBoost          Boruta  96.552%  97.258% 97.258%   97.258% 98.428%\n",
            "Logistic Regression   Original Data  96.552%  97.204% 96.667%   97.980% 98.150%\n",
            "Logistic Regression          Boruta  96.552%  97.204% 96.667%   97.980% 98.621%\n",
            "      Random Forest Normalized Data  96.552%  97.204% 96.667%   97.980% 99.755%\n",
            "      Random Forest          Boruta  96.552%  97.204% 96.667%   97.980% 99.596%\n",
            "           AdaBoost     Autoencoder  96.552%  97.204% 96.667%   97.980% 98.178%\n",
            "     MLP Classifier Normalized Data  96.552%  97.204% 96.667%   97.980% 99.235%\n",
            "     MLP Classifier          Boruta  96.552%  97.204% 96.667%   97.980% 98.760%\n",
            "     MLP Classifier     Autoencoder  96.552%  97.204% 96.667%   97.980% 99.306%\n",
            "        Extra Trees   Original Data  96.552%  97.204% 96.667%   97.980% 99.625%\n",
            "        Extra Trees Normalized Data  96.552%  97.204% 96.667%   97.980% 99.127%\n",
            "        Extra Trees          Boruta  96.552%  97.204% 96.667%   97.980% 99.418%\n",
            "        Extra Trees     Autoencoder  96.552%  97.204% 96.667%   97.980% 98.741%\n",
            "  Gradient Boosting   Original Data  96.552%  97.204% 96.667%   97.980% 99.791%\n",
            "  Gradient Boosting Normalized Data  96.552%  97.204% 96.667%   97.980% 99.484%\n",
            "  Gradient Boosting          Boruta  96.552%  97.204% 96.667%   97.980% 99.791%\n",
            "  Gradient Boosting     Autoencoder  96.552%  97.204% 96.667%   97.980% 99.454%\n",
            "  Gradient Boosting      SMOTETomek  96.552%  97.204% 96.667%   97.980% 99.203%\n",
            "      XGBoost Model   Original Data  96.552%  97.204% 96.667%   97.980% 99.482%\n",
            "      XGBoost Model Normalized Data  96.552%  97.204% 96.667%   97.980% 99.263%\n",
            "      XGBoost Model          Boruta  96.552%  97.204% 96.667%   97.980% 99.394%\n",
            "      XGBoost Model     Autoencoder  96.552%  97.204% 96.667%   97.980% 98.920%\n",
            "K-Nearest Neighbors   Original Data  96.552%  95.494% 96.667%   94.792% 98.289%\n",
            "K-Nearest Neighbors      SMOTETomek  96.552%  95.494% 96.667%   94.792% 98.289%\n",
            "K-Nearest Neighbors Normalized Data  96.552%  95.494% 96.667%   94.792% 98.289%\n",
            "K-Nearest Neighbors              MI  96.552%  95.494% 96.667%   94.792% 98.382%\n",
            "K-Nearest Neighbors             LDA  96.552%  95.494% 96.667%   94.792% 98.124%\n",
            "K-Nearest Neighbors          Boruta  96.552%  95.494% 96.667%   94.792% 98.401%\n",
            "K-Nearest Neighbors     Autoencoder  96.552%  95.494% 96.667%   94.792% 97.111%\n",
            "      Random Forest   Original Data  96.552%  95.494% 96.667%   94.792% 99.187%\n",
            "      Random Forest      SMOTETomek  96.552%  95.494% 96.667%   94.792% 98.633%\n",
            "Stacking Classifier      SMOTETomek  96.552%  95.494% 96.667%   94.792% 98.940%\n",
            "      XGBoost Model      SMOTETomek  96.552%  95.494% 96.667%   94.792% 99.370%\n",
            "     MLP Classifier              MI  94.828%  95.759% 95.000%   97.059% 96.069%\n",
            "        Extra Trees              MI  94.828%  95.759% 95.000%   97.059% 97.651%\n",
            "  Gradient Boosting              MI  94.828%  95.759% 95.000%   97.059% 97.810%\n",
            "      XGBoost Model              MI  94.828%  95.759% 95.000%   97.059% 98.328%\n",
            "      Random Forest     Autoencoder  94.828%  94.436% 95.591%   93.750% 98.661%\n",
            "        Extra Trees             LDA  93.103%  94.408% 93.925%   95.118% 97.579%\n",
            "           AdaBoost      SMOTETomek  93.103%  92.805% 94.516%   91.389% 98.346%\n",
            "      Random Forest              MI  93.103%  90.724% 94.516%   88.889% 99.580%\n",
            "     MLP Classifier      SMOTETomek  93.103%  90.724% 94.516%   88.889% 98.645%\n",
            "        Extra Trees      SMOTETomek  93.103%  90.724% 94.516%   88.889% 98.374%\n",
            "      XGBoost Model             LDA  89.655%  91.414% 90.591%   93.155% 96.722%\n",
            "Logistic Regression      SMOTETomek  87.931%  90.721% 91.882%   90.718% 98.796%\n",
            "  Gradient Boosting             LDA  87.931%  90.484% 90.699%   90.317% 96.443%\n",
            "Logistic Regression             LDA  87.931%  90.104% 89.516%   91.177% 92.996%\n",
            "     MLP Classifier             LDA  87.931%  90.104% 89.516%   91.177% 93.582%\n",
            "      Random Forest             LDA  87.931%  87.500% 90.699%   85.294% 95.056%\n",
            "           AdaBoost             LDA  84.483%  81.961% 79.992%   84.809% 93.263%\n",
            "Stacking Classifier   Original Data  84.483%  60.687% 63.333%   59.167% 99.139%\n",
            "Stacking Classifier Normalized Data  84.483%  60.687% 63.333%   59.167% 99.223%\n",
            "Stacking Classifier          Boruta  84.483%  60.687% 63.333%   59.167% 99.223%\n",
            "Stacking Classifier              MI  82.759%  59.334% 61.667%   58.537% 98.970%\n",
            "Stacking Classifier     Autoencoder  82.759%  59.334% 61.667%   58.537% 99.099%\n",
            "Stacking Classifier             LDA  70.690%  48.383% 50.000%   54.861% 98.376%\n",
            "Logistic Regression              MI  53.448%  23.221% 33.333%   17.816% 81.240%\n",
            "Logistic Regression     Autoencoder  53.448%  23.221% 33.333%   17.816% 86.795%\n",
            "     MLP Classifier   Original Data  53.448%  23.221% 33.333%   17.816% 50.000%\n",
            "\n",
            "====================================================================================================\n",
            "TOP CONFIGURATION PER MODEL\n",
            "====================================================================================================\n",
            "           ML Model   Configuration Accuracy F1 Score  Recall Precision ROC_AUC\n",
            "           AdaBoost              MI  98.276%  98.616% 98.333%   98.958% 98.297%\n",
            "        Extra Trees   Original Data  96.552%  97.204% 96.667%   97.980% 99.625%\n",
            "  Gradient Boosting   Original Data  96.552%  97.204% 96.667%   97.980% 99.791%\n",
            "K-Nearest Neighbors   Original Data  96.552%  95.494% 96.667%   94.792% 98.289%\n",
            "Logistic Regression Normalized Data  98.276%  98.616% 98.333%   98.958% 98.840%\n",
            "     MLP Classifier Normalized Data  96.552%  97.204% 96.667%   97.980% 99.235%\n",
            "      Random Forest Normalized Data  96.552%  97.204% 96.667%   97.980% 99.755%\n",
            "Stacking Classifier      SMOTETomek  96.552%  95.494% 96.667%   94.792% 98.940%\n",
            "      XGBoost Model   Original Data  96.552%  97.204% 96.667%   97.980% 99.482%\n"
          ]
        }
      ]
    }
  ]
}