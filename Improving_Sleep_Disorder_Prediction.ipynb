{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9UTBuepKHdUm2Mky7Bigq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Miftahul-adib/sleep-disorder/blob/main/Improving_Sleep_Disorder_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Importing Libraries\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Yb1fHaSMPI85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boruta category_encoders xgboost catboost lazypredict"
      ],
      "metadata": {
        "id": "_TgpA4zWnk0L",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d65402b7-75ab-4800-c089-ae6a77f7fd02"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting boruta\n",
            "  Downloading Boruta-0.4.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.8.1-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.0.4)\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting lazypredict\n",
            "  Downloading lazypredict-0.2.16-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.12/dist-packages (from boruta) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.12/dist-packages (from boruta) (1.6.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.12/dist-packages (from boruta) (1.16.1)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.0.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (0.14.5)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from lazypredict) (8.2.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from lazypredict) (4.67.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from lazypredict) (1.5.2)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (from lazypredict) (4.6.0)\n",
            "Collecting pytest-runner (from lazypredict)\n",
            "  Downloading pytest_runner-6.0.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting mlflow>=2.0.0 (from lazypredict)\n",
            "  Downloading mlflow-3.3.2-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting mlflow-skinny==3.3.2 (from mlflow>=2.0.0->lazypredict)\n",
            "  Downloading mlflow_skinny-3.3.2-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting mlflow-tracing==3.3.2 (from mlflow>=2.0.0->lazypredict)\n",
            "  Downloading mlflow_tracing-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.12/dist-packages (from mlflow>=2.0.0->lazypredict) (3.1.2)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.12/dist-packages (from mlflow>=2.0.0->lazypredict) (1.16.5)\n",
            "Requirement already satisfied: cryptography<46,>=43.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow>=2.0.0->lazypredict) (43.0.3)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow>=2.0.0->lazypredict)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow>=2.0.0->lazypredict)\n",
            "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting gunicorn<24 (from mlflow>=2.0.0->lazypredict)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: pyarrow<22,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow>=2.0.0->lazypredict) (18.1.0)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow>=2.0.0->lazypredict) (2.0.43)\n",
            "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.3.2->mlflow>=2.0.0->lazypredict) (5.5.2)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.3.2->mlflow>=2.0.0->lazypredict) (3.1.1)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.3.2->mlflow>=2.0.0->lazypredict)\n",
            "  Downloading databricks_sdk-0.65.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: fastapi<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.3.2->mlflow>=2.0.0->lazypredict) (0.116.1)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.3.2->mlflow>=2.0.0->lazypredict) (3.1.45)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.3.2->mlflow>=2.0.0->lazypredict) (8.7.0)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.3.2->mlflow>=2.0.0->lazypredict) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.3.2->mlflow>=2.0.0->lazypredict) (1.36.0)\n",
            "Requirement already satisfied: packaging<26 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.3.2->mlflow>=2.0.0->lazypredict) (25.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.3.2->mlflow>=2.0.0->lazypredict) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.3.2->mlflow>=2.0.0->lazypredict) (2.11.7)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.3.2->mlflow>=2.0.0->lazypredict) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.3.2->mlflow>=2.0.0->lazypredict) (2.32.4)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.3.2->mlflow>=2.0.0->lazypredict) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.3.2->mlflow>=2.0.0->lazypredict) (4.15.0)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.3.2->mlflow>=2.0.0->lazypredict) (0.35.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.17.1->boruta) (3.6.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic!=1.10.0,<2->mlflow>=2.0.0->lazypredict) (1.3.10)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography<46,>=43.0.0->mlflow>=2.0.0->lazypredict) (1.17.1)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from docker<8,>=4.0.0->mlflow>=2.0.0->lazypredict) (2.5.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow>=2.0.0->lazypredict) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow>=2.0.0->lazypredict) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow>=2.0.0->lazypredict) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow>=2.0.0->lazypredict) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow>=2.0.0->lazypredict) (3.1.3)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow>=2.0.0->lazypredict)\n",
            "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow>=2.0.0->lazypredict)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow>=2.0.0->lazypredict) (3.2.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography<46,>=43.0.0->mlflow>=2.0.0->lazypredict) (2.22)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.12/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.3.2->mlflow>=2.0.0->lazypredict) (2.38.0)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi<1->mlflow-skinny==3.3.2->mlflow>=2.0.0->lazypredict) (0.47.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.3.2->mlflow>=2.0.0->lazypredict) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.3.2->mlflow>=2.0.0->lazypredict) (3.23.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.3.2->mlflow>=2.0.0->lazypredict) (0.57b0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.3.2->mlflow>=2.0.0->lazypredict) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.3.2->mlflow>=2.0.0->lazypredict) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.3.2->mlflow>=2.0.0->lazypredict) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.3.2->mlflow>=2.0.0->lazypredict) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.3.2->mlflow>=2.0.0->lazypredict) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.3.2->mlflow>=2.0.0->lazypredict) (2025.8.3)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn<1->mlflow-skinny==3.3.2->mlflow>=2.0.0->lazypredict) (0.16.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.3.2->mlflow>=2.0.0->lazypredict) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.3.2->mlflow>=2.0.0->lazypredict) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.3.2->mlflow>=2.0.0->lazypredict) (4.9.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny==3.3.2->mlflow>=2.0.0->lazypredict) (4.10.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny==3.3.2->mlflow>=2.0.0->lazypredict) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.3.2->mlflow>=2.0.0->lazypredict) (0.6.1)\n",
            "Downloading Boruta-0.4.3-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading category_encoders-2.8.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lazypredict-0.2.16-py2.py3-none-any.whl (14 kB)\n",
            "Downloading mlflow-3.3.2-py3-none-any.whl (26.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.4/26.4 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-3.3.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_tracing-3.3.2-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_runner-6.0.1-py3-none-any.whl (7.2 kB)\n",
            "Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.65.0-py3-none-any.whl (705 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.9/705.9 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: pytest-runner, gunicorn, graphql-core, graphql-relay, docker, graphene, databricks-sdk, catboost, boruta, category_encoders, mlflow-tracing, mlflow-skinny, mlflow, lazypredict\n",
            "Successfully installed boruta-0.4.3 catboost-1.2.8 category_encoders-2.8.1 databricks-sdk-0.65.0 docker-7.1.0 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 gunicorn-23.0.0 lazypredict-0.2.16 mlflow-3.3.2 mlflow-skinny-3.3.2 mlflow-tracing-3.3.2 pytest-runner-6.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "diEVyiMoJ4Cm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, RobustScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from lazypredict.Supervised import LazyClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler, SMOTENC\n",
        "from imblearn.combine import SMOTETomek, SMOTEENN\n",
        "from imblearn.under_sampling import CondensedNearestNeighbour, TomekLinks, RandomUnderSampler\n",
        "from boruta import BorutaPy\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Load & Preprocessing"
      ],
      "metadata": {
        "id": "unb2qil8SoTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/Sleep_health_and_lifestyle_dataset.csv\")\n",
        "df.fillna(\"None\", inplace=True)\n",
        "df[['Systolic BP', 'Diastolic BP']] = df['Blood Pressure'].str.split('/', expand=True).astype(int)\n",
        "df.drop(['Person ID', 'Blood Pressure'], axis=1, inplace=True)\n",
        "\n",
        "df['Occupation'] = df['Occupation'].replace(['Manager', 'Sales Representative', 'Scientist', 'Software Engineer'], 'Other')\n",
        "df['BMI Category'] = df['BMI Category'].replace({'Normal':22, 'Normal Weight':22, 'Overweight':27, 'Obese':30})\n",
        "\n",
        "df['Stress_sleep_interaction'] = df['Stress Level'] / df['Quality of Sleep']\n",
        "df['BMI_Activity'] = df['BMI Category'] * df['Physical Activity Level']\n",
        "df['Sleep_Heart_ratio'] = df['Sleep Duration'] / df['Heart Rate']\n",
        "df['Sleep_Steps_ratio'] = df['Sleep Duration'] / df['Daily Steps']\n",
        "df['Sleep_Stress_ratio'] = df['Sleep Duration'] / df['Stress Level']\n",
        "\n",
        "df = pd.get_dummies(df, columns=['Occupation'], drop_first=False)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "columns = ['Gender', 'Sleep Disorder']\n",
        "for col in columns:\n",
        "  df[col] = label_encoder.fit_transform(df[col])\n",
        "\n",
        "num_col = ['Age', 'Sleep Duration', 'Quality of Sleep', 'Physical Activity Level', 'Stress Level', 'Stress_sleep_interaction',\n",
        "          'Sleep_Heart_ratio', 'Sleep_Steps_ratio', 'Sleep_Stress_ratio', 'Heart Rate', 'Daily Steps',\n",
        "           'Systolic BP', 'Diastolic BP']\n",
        "\n",
        "Q1 = df[num_col].quantile(0.25)\n",
        "Q3 = df[num_col].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "df = df[~((df[num_col] < (Q1 - 1.5 * IQR)) | (df[num_col] > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "classes, count = np.unique(df['Sleep Disorder'], return_counts=True)\n",
        "\n",
        "X = df.drop('Sleep Disorder', axis=1)\n",
        "y = df['Sleep Disorder']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n"
      ],
      "metadata": {
        "id": "R6yAAGUeSsn3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apply RobustSclaer, MI, LDA, Boruta, Autoencoder, and SMOTETomek"
      ],
      "metadata": {
        "id": "auoUKdHC731t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Normalize the data\n",
        "scaler = RobustScaler()\n",
        "X_train_normalized = scaler.fit_transform(X_train)\n",
        "X_test_normalized = scaler.transform(X_test)\n",
        "\n",
        "# Applying Mutual information\n",
        "mi = SelectKBest(score_func=mutual_info_classif, k=5)\n",
        "X_train_mi = mi.fit_transform(X_train_normalized, y_train)\n",
        "X_test_mi = mi.transform(X_test_normalized)\n",
        "\n",
        "# Applying LDA\n",
        "lda = LinearDiscriminantAnalysis(n_components=2)\n",
        "X_train_lda = lda.fit_transform(X_train_mi, y_train)\n",
        "X_test_lda = lda.transform(X_test_mi)\n",
        "\n",
        "# RandomForest classifier\n",
        "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Applying Boruta Feature Selection\n",
        "boruta_selector = BorutaPy(rfc, n_estimators='auto', verbose=0, random_state=42)\n",
        "\n",
        "X_train_boruta = boruta_selector.fit_transform(X_train_normalized, y_train)\n",
        "X_test_boruta = boruta_selector.transform(X_test_normalized)\n",
        "\n",
        "# applying Autoencoder\n",
        "n_features = X_train_boruta.shape[1]\n",
        "input_layer = Input(shape=(n_features,))\n",
        "encoded = Dense(32, activation='relu')(input_layer)\n",
        "bottleneck = Dense(16, activation='relu')(encoded)\n",
        "decoded = Dense(32, activation='relu')(encoded)\n",
        "decoded = Dense(n_features, activation='sigmoid')(encoded)\n",
        "\n",
        "autoencoder = Model(input_layer, decoded)\n",
        "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "autoencoder.fit(X_train_boruta, X_train_boruta, epochs=10, batch_size=32, verbose=0)\n",
        "encoder = Model(input_layer, bottleneck)\n",
        "X_train_encoded = encoder.predict(X_train_boruta)\n",
        "X_test_encoded = encoder.predict(X_test_boruta)\n",
        "\n",
        "smotetomek = SMOTETomek(sampling_strategy='auto',\n",
        "                   smote=SMOTE(k_neighbors=3, random_state=42),\n",
        "                   tomek=TomekLinks(sampling_strategy='auto', n_jobs=-1),\n",
        "                   n_jobs=-1,\n",
        "                   random_state=42)\n",
        "\n",
        "X_train_resample, y_train_resample = smotetomek.fit_resample(X_train_normalized, y_train)"
      ],
      "metadata": {
        "id": "b5AYEVMw8BMY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b01e578-bb1f-415c-f545-eeec01258749"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trial with LazyClassifier"
      ],
      "metadata": {
        "id": "qEu8cN4IWbGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
        "# models, preds = clf.fit(X_train, X_test, y_train, y_test)\n",
        "# models, preds = clf.fit(X_train_normalized, X_test_normalized, y_train, y_test)\n",
        "# models, preds = clf.fit(X_train_mi, X_test_mi, y_train, y_test)\n",
        "# models, preds = clf.fit(X_train_lda, X_test_lda, y_train, y_test)\n",
        "# models, preds = clf.fit(X_train_boruta, X_test_boruta, y_train, y_test)\n",
        "# models, preds = clf.fit(X_train_encoded, X_test_encoded, y_train, y_test)\n",
        "# models, preds = clf.fit(X_train_resample, X_test_normalized, y_train_resample, y_test)\n",
        "# print(models)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "u7YIHQlAWpmA"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Model Result Storage"
      ],
      "metadata": {
        "id": "TpEDoK5Ihy4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ML_Model = []\n",
        "ML_Config = []\n",
        "accuracy = []\n",
        "f1_score = []\n",
        "recall = []\n",
        "precision = []\n",
        "auc_roc = []  # Adding a holder for AUC-ROC\n",
        "\n",
        "# Function to call for storing the results\n",
        "def storeResults(model, config, a, b, c, d, e):\n",
        "    \"\"\"\n",
        "    Store model performance results\n",
        "\n",
        "    Parameters:\n",
        "    model: Name of the ML model\n",
        "    config: Configuration name (preprocessing steps applied)\n",
        "    a: Accuracy score\n",
        "    b: F1 score\n",
        "    c: Recall score\n",
        "    d: Precision score\n",
        "    e: AUC-ROC score\n",
        "    \"\"\"\n",
        "    ML_Model.append(model)\n",
        "    ML_Config.append(config)\n",
        "    accuracy.append(round(a, 6))\n",
        "    f1_score.append(round(b, 6))\n",
        "    recall.append(round(c, 6))\n",
        "    precision.append(round(d, 6))\n",
        "    auc_roc.append(round(e, 6))"
      ],
      "metadata": {
        "id": "tlV5BgjKhyYX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "Pg8p5Fg07Hsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import uniform\n",
        "configurations = []\n",
        "\n",
        "configurations.append(('Original Data', X_train, X_test, y_train))\n",
        "configurations.append(('SMOTETomek', X_train_resample, X_test_normalized, y_train_resample))\n",
        "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
        "configurations.append(('MI', X_train_mi, X_test_mi, y_train))\n",
        "configurations.append(('LDA', X_train_lda, X_test_lda, y_train))\n",
        "configurations.append(('Boruta', X_train_boruta, X_test_boruta, y_train))\n",
        "configurations.append(('Autoencoder', X_train_encoded, X_test_encoded, y_train))\n",
        "\n",
        "params = {\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'C': [0.88, 0.89, 0.90, 0.91], #uniform(0.01, 10),\n",
        "    'solver': ['liblinear', 'saga', 'lbfgs', 'newton-cg'],  # Optimization solvers\n",
        "    'max_iter': [489, 490, 491] #range(100, 1000, 12) #[100, 200, 500, 1000]\n",
        "}\n",
        "\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning Logistic Regression with {name} configuration...\")\n",
        "    logr = GridSearchCV(LogisticRegression(), params, cv=5,\n",
        "                             n_jobs=-1, scoring=['accuracy', 'f1_macro'], refit='accuracy', verbose=2)\n",
        "    logr.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_lr = logr.predict(X_train_cfg)\n",
        "    y_test_lr = logr.predict(X_test_cfg)\n",
        "    y_train_lr_proba = logr.predict_proba(X_train_cfg)\n",
        "    y_test_lr_proba = logr.predict_proba(X_test_cfg)\n",
        "\n",
        "    metrics_dict = {\n",
        "          \"Dataset\": [\"Training\", \"Test\"],\n",
        "          \"Accuracy\": [\n",
        "              metrics.accuracy_score(y_train_cfg, y_train_lr),\n",
        "              metrics.accuracy_score(y_test, y_test_lr),\n",
        "          ],\n",
        "          \"F1 Score\": [\n",
        "              metrics.f1_score(y_train_cfg, y_train_lr, average='macro'),\n",
        "              metrics.f1_score(y_test, y_test_lr, average='macro'),\n",
        "          ],\n",
        "          \"Recall\": [\n",
        "              metrics.recall_score(y_train_cfg, y_train_lr, average='macro'),\n",
        "              metrics.recall_score(y_test, y_test_lr, average='macro'),\n",
        "          ],\n",
        "          \"Precision\": [\n",
        "              metrics.precision_score(y_train_cfg, y_train_lr, average='macro'),\n",
        "              metrics.precision_score(y_test, y_test_lr, average='macro'),\n",
        "          ],\n",
        "          \"AUC-ROC\": [\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_lr_proba, multi_class='ovr', average='macro'),\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_test), y_test_lr_proba, multi_class='ovr', average='macro'),\n",
        "          ]\n",
        "      }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nLogistic Regression Model Performance Metrics\")\n",
        "    print(\"Configuration Name: \", name)\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_lr_proba, multi_class='ovr', average='macro')\n",
        "    storeResults(\n",
        "          'Logistic Regression',\n",
        "          name,\n",
        "          metrics.accuracy_score(y_test, y_test_lr),\n",
        "          metrics.f1_score(y_test, y_test_lr, average='macro'),\n",
        "          metrics.recall_score(y_test, y_test_lr, average='macro'),\n",
        "          metrics.precision_score(y_test, y_test_lr, average='macro'),\n",
        "          auc_score\n",
        "      )\n",
        "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
        "    print(logr.best_params_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axcfe3_47JuW",
        "outputId": "bf8d8e57-c64f-41b2-b5cf-3bd20a1f6ade"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running Logistic Regression with Original Data configuration...\n",
            "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
            "\n",
            "Logistic Regression Model Performance Metrics\n",
            "Configuration Name:  Original Data\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.91      0.88    0.86       0.91     0.93\n",
            "    Test      0.95      0.93    0.95       0.92     0.97\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'C': 0.88, 'max_iter': 489, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "\n",
            "Running Logistic Regression with SMOTETomek configuration...\n",
            "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
            "\n",
            "Logistic Regression Model Performance Metrics\n",
            "Configuration Name:  SMOTETomek\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.90      0.90    0.90       0.90     0.96\n",
            "    Test      0.95      0.93    0.95       0.92     0.97\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'C': 0.88, 'max_iter': 489, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "\n",
            "Running Logistic Regression with Normalized Data configuration...\n",
            "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
            "\n",
            "Logistic Regression Model Performance Metrics\n",
            "Configuration Name:  Normalized Data\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.92      0.89    0.88       0.92     0.93\n",
            "    Test      0.95      0.93    0.95       0.92     0.98\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'C': 0.88, 'max_iter': 489, 'penalty': 'l1', 'solver': 'saga'}\n",
            "\n",
            "Running Logistic Regression with MI configuration...\n",
            "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
            "\n",
            "Logistic Regression Model Performance Metrics\n",
            "Configuration Name:  MI\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.83      0.79    0.76       0.84     0.88\n",
            "    Test      0.86      0.84    0.85       0.88     0.95\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'C': 0.88, 'max_iter': 489, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "\n",
            "Running Logistic Regression with LDA configuration...\n",
            "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
            "\n",
            "Logistic Regression Model Performance Metrics\n",
            "Configuration Name:  LDA\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.85      0.82    0.81       0.83     0.90\n",
            "    Test      0.91      0.90    0.92       0.88     0.95\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'C': 0.88, 'max_iter': 489, 'penalty': 'l1', 'solver': 'saga'}\n",
            "\n",
            "Running Logistic Regression with Boruta configuration...\n",
            "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
            "\n",
            "Logistic Regression Model Performance Metrics\n",
            "Configuration Name:  Boruta\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.90      0.87    0.85       0.90     0.92\n",
            "    Test      0.95      0.93    0.95       0.92     0.98\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'C': 0.88, 'max_iter': 489, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "\n",
            "Running Logistic Regression with Autoencoder configuration...\n",
            "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
            "\n",
            "Logistic Regression Model Performance Metrics\n",
            "Configuration Name:  Autoencoder\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.89      0.86    0.85       0.89     0.93\n",
            "    Test      0.95      0.93    0.95       0.92     0.99\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'C': 0.88, 'max_iter': 489, 'penalty': 'l1', 'solver': 'liblinear'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN"
      ],
      "metadata": {
        "id": "BmgqNGgGKtdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "configurations = []\n",
        "\n",
        "configurations.append(('Original Data', X_train, X_test, y_train))\n",
        "configurations.append(('SMOTETomek', X_train_resample, X_test_normalized, y_train_resample))\n",
        "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
        "configurations.append(('MI', X_train_mi, X_test_mi, y_train))\n",
        "configurations.append(('LDA', X_train_lda, X_test_lda, y_train))\n",
        "configurations.append(('Boruta', X_train_boruta, X_test_boruta, y_train))\n",
        "configurations.append(('Autoencoder', X_train_encoded, X_test_encoded, y_train))\n",
        "\n",
        "params = {\n",
        "    'n_neighbors': np.random.randint(1, 50, 3),\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['euclidean', 'manhattan', 'minkowski'],\n",
        "    'p': np.random.randint(1, 5, 1)\n",
        "}\n",
        "\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning KNN with {name} configuration...\")\n",
        "    knn = RandomizedSearchCV(KNeighborsClassifier(), params, n_iter=50, cv=10,\n",
        "                             n_jobs=-1, scoring=['accuracy', 'f1_macro'], refit='accuracy', verbose=2)\n",
        "    knn.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_knn = knn.predict(X_train_cfg)\n",
        "    y_test_knn = knn.predict(X_test_cfg)\n",
        "    y_train_knn_proba = knn.predict_proba(X_train_cfg)\n",
        "    y_test_knn_proba = knn.predict_proba(X_test_cfg)\n",
        "\n",
        "    metrics_dict = {\n",
        "          \"Dataset\": [\"Training\", \"Test\"],\n",
        "          \"Accuracy\": [\n",
        "              metrics.accuracy_score(y_train_cfg, y_train_knn),\n",
        "              metrics.accuracy_score(y_test, y_test_knn),\n",
        "          ],\n",
        "          \"F1 Score\": [\n",
        "              metrics.f1_score(y_train_cfg, y_train_knn, average='macro'),\n",
        "              metrics.f1_score(y_test, y_test_knn, average='macro'),\n",
        "          ],\n",
        "          \"Recall\": [\n",
        "              metrics.recall_score(y_train_cfg, y_train_knn, average='macro'),\n",
        "              metrics.recall_score(y_test, y_test_knn, average='macro'),\n",
        "          ],\n",
        "          \"Precision\": [\n",
        "              metrics.precision_score(y_train_cfg, y_train_knn, average='macro'),\n",
        "              metrics.precision_score(y_test, y_test_knn, average='macro'),\n",
        "          ],\n",
        "          \"AUC-ROC\": [\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_knn_proba, multi_class='ovr', average='macro'),\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_test), y_test_knn_proba, multi_class='ovr', average='macro'),\n",
        "          ]\n",
        "      }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nKNearestNeighbors Model Performance Metrics\")\n",
        "    print(\"Configuration Name: \", name)\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_knn_proba, multi_class='ovr', average='macro')\n",
        "    storeResults(\n",
        "          'K-Nearest Neighbors',\n",
        "          name,\n",
        "          metrics.accuracy_score(y_test, y_test_knn),\n",
        "          metrics.f1_score(y_test, y_test_knn, average='macro'),\n",
        "          metrics.recall_score(y_test, y_test_knn, average='macro'),\n",
        "          metrics.precision_score(y_test, y_test_knn, average='macro'),\n",
        "          auc_score\n",
        "      )\n",
        "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
        "    print(knn.best_params_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZEKc2YMKRB8",
        "outputId": "c5fa8412-c88f-40c6-bffd-cbbc4b7541ac"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running KNN with Original Data configuration...\n",
            "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n",
            "\n",
            "KNearestNeighbors Model Performance Metrics\n",
            "Configuration Name:  Original Data\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.93      0.90    0.89       0.92     0.99\n",
            "    Test      0.95      0.90    0.95       0.88     0.97\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'weights': 'distance', 'p': np.int64(1), 'n_neighbors': np.int64(23), 'metric': 'euclidean'}\n",
            "\n",
            "Running KNN with SMOTETomek configuration...\n",
            "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n",
            "\n",
            "KNearestNeighbors Model Performance Metrics\n",
            "Configuration Name:  SMOTETomek\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.96      0.96    0.96       0.96     1.00\n",
            "    Test      0.95      0.90    0.95       0.88     0.97\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'weights': 'distance', 'p': np.int64(1), 'n_neighbors': np.int64(23), 'metric': 'euclidean'}\n",
            "\n",
            "Running KNN with Normalized Data configuration...\n",
            "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n",
            "\n",
            "KNearestNeighbors Model Performance Metrics\n",
            "Configuration Name:  Normalized Data\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.93      0.90    0.89       0.92     0.99\n",
            "    Test      0.95      0.93    0.95       0.92     0.98\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'weights': 'distance', 'p': np.int64(1), 'n_neighbors': np.int64(23), 'metric': 'euclidean'}\n",
            "\n",
            "Running KNN with MI configuration...\n",
            "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n",
            "\n",
            "KNearestNeighbors Model Performance Metrics\n",
            "Configuration Name:  MI\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.92      0.90    0.88       0.92     0.98\n",
            "    Test      0.95      0.90    0.95       0.88     0.95\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'weights': 'distance', 'p': np.int64(1), 'n_neighbors': np.int64(23), 'metric': 'manhattan'}\n",
            "\n",
            "Running KNN with LDA configuration...\n",
            "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n",
            "\n",
            "KNearestNeighbors Model Performance Metrics\n",
            "Configuration Name:  LDA\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.92      0.90    0.88       0.92     0.98\n",
            "    Test      0.95      0.90    0.95       0.88     0.95\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'weights': 'distance', 'p': np.int64(1), 'n_neighbors': np.int64(29), 'metric': 'euclidean'}\n",
            "\n",
            "Running KNN with Boruta configuration...\n",
            "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n",
            "\n",
            "KNearestNeighbors Model Performance Metrics\n",
            "Configuration Name:  Boruta\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.93      0.90    0.89       0.92     0.99\n",
            "    Test      0.95      0.90    0.95       0.88     0.97\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'weights': 'distance', 'p': np.int64(1), 'n_neighbors': np.int64(29), 'metric': 'euclidean'}\n",
            "\n",
            "Running KNN with Autoencoder configuration...\n",
            "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n",
            "\n",
            "KNearestNeighbors Model Performance Metrics\n",
            "Configuration Name:  Autoencoder\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.93      0.90    0.89       0.92     0.99\n",
            "    Test      0.95      0.90    0.95       0.88     0.97\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'weights': 'distance', 'p': np.int64(1), 'n_neighbors': np.int64(29), 'metric': 'euclidean'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "SHcHbkJxdLvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "configurations = []\n",
        "\n",
        "configurations.append(('Original Data', X_train, X_test, y_train))\n",
        "configurations.append(('SMOTETomek', X_train_resample, X_test_normalized, y_train_resample))\n",
        "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
        "configurations.append(('MI', X_train_mi, X_test_mi, y_train))\n",
        "configurations.append(('LDA', X_train_lda, X_test_lda, y_train))\n",
        "\n",
        "selected_features = boruta_selector.support_\n",
        "optimal_features = sum(selected_features)\n",
        "print(f\"Optimal number of features to select using Boruta: {optimal_features}\")\n",
        "\n",
        "configurations.append(('Boruta', X_train_boruta, X_test_boruta, y_train))\n",
        "configurations.append(('Autoencoder', X_train_encoded, X_test_encoded, y_train))\n",
        "\n",
        "# Step 4: Random Forest + GridSearchCV\n",
        "print(\"\\n=== Random Forest Model Performance with Hyperparameter Tuning ===\")\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': range(150, 500, 10), #[300, 350, 400, 450, 500],\n",
        "    'max_depth': range(2, 50, 10), #[11, 17, 20],\n",
        "    'min_samples_split': range(1, 10, 1), #[1, 2],\n",
        "    'min_samples_leaf': range(1, 10, 1), #[2, 3],\n",
        "    'max_features': ['sqrt'],\n",
        "    'bootstrap': [False],\n",
        "    'class_weight': ['balanced'],\n",
        "    'max_leaf_nodes': range(2, 50, 10), #[30, 50],\n",
        "    'min_impurity_decrease': np.linspace(0.0001, 0.1, 10), #[0.01, 0.005],\n",
        "    'ccp_alpha': np.linspace(0.0001, 0.1, 10), #[0.001, 0.003, 0.008],\n",
        "    'random_state': range(2, 50, 10), #[49, 51],\n",
        "    'criterion': ['gini', 'entropy', 'log_loss']\n",
        "}\n",
        "\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning Random Forest with {name} configuration...\")\n",
        "    rf = RandomizedSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=10, n_jobs=-1, n_iter=50, scoring=['accuracy', 'f1_macro'], refit='accuracy', verbose=2)\n",
        "    rf.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_rf = rf.predict(X_train_cfg)\n",
        "    y_test_rf = rf.predict(X_test_cfg)\n",
        "    y_train_rf_proba = rf.predict_proba(X_train_cfg)\n",
        "    y_test_rf_proba = rf.predict_proba(X_test_cfg)\n",
        "\n",
        "    metrics_dict = {\n",
        "          \"Dataset\": [\"Training\", \"Test\"],\n",
        "          \"Accuracy\": [\n",
        "              metrics.accuracy_score(y_train_cfg, y_train_rf),\n",
        "              metrics.accuracy_score(y_test, y_test_rf),\n",
        "          ],\n",
        "          \"F1 Score\": [\n",
        "              metrics.f1_score(y_train_cfg, y_train_rf, average='macro'),\n",
        "              metrics.f1_score(y_test, y_test_rf, average='macro'),\n",
        "          ],\n",
        "          \"Recall\": [\n",
        "              metrics.recall_score(y_train_cfg, y_train_rf, average='macro'),\n",
        "              metrics.recall_score(y_test, y_test_rf, average='macro'),\n",
        "          ],\n",
        "          \"Precision\": [\n",
        "              metrics.precision_score(y_train_cfg, y_train_rf, average='macro'),\n",
        "              metrics.precision_score(y_test, y_test_rf, average='macro'),\n",
        "          ],\n",
        "          \"AUC-ROC\": [\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_rf_proba, multi_class='ovr', average='macro'),\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_test), y_test_rf_proba, multi_class='ovr', average='macro'),\n",
        "          ]\n",
        "      }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nRandom Forest Model Performance Metrics\")\n",
        "    print(\"Configuration Name: \", name)\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_rf_proba, multi_class='ovr', average='macro')\n",
        "    storeResults(\n",
        "          'Random Forest',\n",
        "          name,\n",
        "          metrics.accuracy_score(y_test, y_test_rf),\n",
        "          metrics.f1_score(y_test, y_test_rf, average='macro'),\n",
        "          metrics.recall_score(y_test, y_test_rf, average='macro'),\n",
        "          metrics.precision_score(y_test, y_test_rf, average='macro'),\n",
        "          auc_score\n",
        "      )\n",
        "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
        "    print(rf.best_params_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7NTek4OUXv5",
        "outputId": "64940270-42b7-4080-97cf-97df3df14114"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal number of features to select using Boruta: 10\n",
            "\n",
            "=== Random Forest Model Performance with Hyperparameter Tuning ===\n",
            "\n",
            "Running Random Forest with Original Data configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "Random Forest Model Performance Metrics\n",
            "Configuration Name:  Original Data\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.92      0.90    0.89       0.91     0.95\n",
            "    Test      0.95      0.90    0.95       0.88     0.98\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'random_state': 12, 'n_estimators': 170, 'min_samples_split': 4, 'min_samples_leaf': 1, 'min_impurity_decrease': np.float64(0.0112), 'max_leaf_nodes': 32, 'max_features': 'sqrt', 'max_depth': 12, 'criterion': 'gini', 'class_weight': 'balanced', 'ccp_alpha': np.float64(0.0112), 'bootstrap': False}\n",
            "\n",
            "Running Random Forest with SMOTETomek configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "Random Forest Model Performance Metrics\n",
            "Configuration Name:  SMOTETomek\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.92      0.92    0.92       0.92     0.98\n",
            "    Test      0.95      0.90    0.95       0.88     0.97\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'random_state': 2, 'n_estimators': 290, 'min_samples_split': 8, 'min_samples_leaf': 2, 'min_impurity_decrease': np.float64(0.0001), 'max_leaf_nodes': 32, 'max_features': 'sqrt', 'max_depth': 42, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': np.float64(0.0223), 'bootstrap': False}\n",
            "\n",
            "Running Random Forest with Normalized Data configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "Random Forest Model Performance Metrics\n",
            "Configuration Name:  Normalized Data\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.92      0.90    0.89       0.91     0.97\n",
            "    Test      0.97      0.94    0.97       0.92     0.97\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'random_state': 22, 'n_estimators': 340, 'min_samples_split': 4, 'min_samples_leaf': 5, 'min_impurity_decrease': np.float64(0.0112), 'max_leaf_nodes': 12, 'max_features': 'sqrt', 'max_depth': 42, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': np.float64(0.0112), 'bootstrap': False}\n",
            "\n",
            "Running Random Forest with MI configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "Random Forest Model Performance Metrics\n",
            "Configuration Name:  MI\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.90      0.87    0.87       0.87     0.96\n",
            "    Test      0.93      0.87    0.94       0.85     0.96\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'random_state': 42, 'n_estimators': 370, 'min_samples_split': 4, 'min_samples_leaf': 7, 'min_impurity_decrease': np.float64(0.0001), 'max_leaf_nodes': 32, 'max_features': 'sqrt', 'max_depth': 12, 'criterion': 'entropy', 'class_weight': 'balanced', 'ccp_alpha': np.float64(0.0112), 'bootstrap': False}\n",
            "\n",
            "Running Random Forest with LDA configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "Random Forest Model Performance Metrics\n",
            "Configuration Name:  LDA\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.91      0.88    0.89       0.88     0.96\n",
            "    Test      0.91      0.85    0.93       0.82     0.95\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'random_state': 12, 'n_estimators': 210, 'min_samples_split': 7, 'min_samples_leaf': 2, 'min_impurity_decrease': np.float64(0.0223), 'max_leaf_nodes': 42, 'max_features': 'sqrt', 'max_depth': 32, 'criterion': 'entropy', 'class_weight': 'balanced', 'ccp_alpha': np.float64(0.0223), 'bootstrap': False}\n",
            "\n",
            "Running Random Forest with Boruta configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "Random Forest Model Performance Metrics\n",
            "Configuration Name:  Boruta\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.92      0.90    0.89       0.91     0.93\n",
            "    Test      0.97      0.94    0.97       0.92     0.98\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'random_state': 32, 'n_estimators': 160, 'min_samples_split': 6, 'min_samples_leaf': 2, 'min_impurity_decrease': np.float64(0.044500000000000005), 'max_leaf_nodes': 42, 'max_features': 'sqrt', 'max_depth': 22, 'criterion': 'entropy', 'class_weight': 'balanced', 'ccp_alpha': np.float64(0.0001), 'bootstrap': False}\n",
            "\n",
            "Running Random Forest with Autoencoder configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "Random Forest Model Performance Metrics\n",
            "Configuration Name:  Autoencoder\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.92      0.90    0.89       0.91     0.96\n",
            "    Test      0.95      0.90    0.95       0.88     0.97\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'random_state': 2, 'n_estimators': 420, 'min_samples_split': 3, 'min_samples_leaf': 5, 'min_impurity_decrease': np.float64(0.033400000000000006), 'max_leaf_nodes': 42, 'max_features': 'sqrt', 'max_depth': 32, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': np.float64(0.0112), 'bootstrap': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X_F-w3vDsLHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "configurations = []\n",
        "configurations.append(('Original Data', X_train, X_test, y_train))\n",
        "configurations.append(('SMOTETomek', X_train_resample, X_test_normalized, y_train_resample))\n",
        "\n",
        "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
        "configurations.append(('MI', X_train_mi, X_test_mi, y_train))\n",
        "configurations.append(('LDA', X_train_lda, X_test_lda, y_train))\n",
        "\n",
        "configurations.append(('Boruta', X_train_boruta, X_test_boruta, y_train))\n",
        "configurations.append(('Autoencoder', X_train_encoded, X_test_encoded, y_train))\n",
        "\n",
        "selected_features = boruta_selector.support_\n",
        "optimal_features = sum(selected_features)\n",
        "print(f\"Optimal number of features to select using Boruta: {optimal_features}\")\n",
        "\n",
        "# Step 4: XGBoost + GridSearchCV\n",
        "print(\"\\n=== XGBoost Model Performance with Hyperparameter Tuning ===\")\n",
        "\n",
        "param_grid = {\n",
        "    'booster': ['gbtree',],\n",
        "    'learning_rate': np.linspace(0.0001, 0.1, 10), #[0.1, 0.2],\n",
        "    'n_estimators': range(50, 500, 10), #[200, 220],\n",
        "    'estimator__n_estimators': range(50, 500, 10),\n",
        "    'max_depth': range(2, 50, 10), #[5,12],\n",
        "    'min_child_weight': range(1, 10, 1), #[1, 3, 7],\n",
        "    'gamma': np.linspace(0, 0.1, 3), #[0, 0.1],\n",
        "    'subsample': np.linspace(0.1, 1, 1), #[0.8],\n",
        "    'colsample_bytree': [0.3, 0.8],\n",
        "    'colsample_bylevel': [1.0],\n",
        "    'colsample_bynode': [0.6, 0.8],\n",
        "    'max_delta_step': [0, 5],\n",
        "\n",
        "    'reg_alpha': np.linspace(0.1, 1, 1), #[0.1, 0.5, 0.7],\n",
        "    'reg_lambda': np.linspace(0.1, 1, 1), #[0.1, 0.5, 0.6],\n",
        "    'scale_pos_weight': [1, 2, 5],\n",
        "\n",
        "    'sample_type': [\"weighted\"],\n",
        "    'normalize_type': [\"tree\", \"forest\"],\n",
        "    'rate_drop': [0, 0.1],\n",
        "    'skip_drop': [0, 0.1],\n",
        "\n",
        "    'random_state': [2, 42, 49, 51]\n",
        "}\n",
        "\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning XGBoost with {name} configuration...\")\n",
        "    xgb = RandomizedSearchCV(XGBClassifier(), param_grid, n_iter=50, cv=10,\n",
        "                             n_jobs=-1, scoring=['accuracy', 'f1_macro'], refit='accuracy', verbose=2)\n",
        "    xgb.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_xg = xgb.predict(X_train_cfg)\n",
        "    y_test_xg = xgb.predict(X_test_cfg)\n",
        "    y_train_xg_proba = xgb.predict_proba(X_train_cfg)\n",
        "    y_test_xg_proba = xgb.predict_proba(X_test_cfg)\n",
        "\n",
        "    metrics_dict = {\n",
        "          \"Dataset\": [\"Training\", \"Test\"],\n",
        "          \"Accuracy\": [\n",
        "              metrics.accuracy_score(y_train_cfg, y_train_xg),\n",
        "              metrics.accuracy_score(y_test, y_test_xg),\n",
        "          ],\n",
        "          \"F1 Score\": [\n",
        "              metrics.f1_score(y_train_cfg, y_train_xg, average='macro'),\n",
        "              metrics.f1_score(y_test, y_test_xg, average='macro'),\n",
        "          ],\n",
        "          \"Recall\": [\n",
        "              metrics.recall_score(y_train_cfg, y_train_xg, average='macro'),\n",
        "              metrics.recall_score(y_test, y_test_xg, average='macro'),\n",
        "          ],\n",
        "          \"Precision\": [\n",
        "              metrics.precision_score(y_train_cfg, y_train_xg, average='macro'),\n",
        "              metrics.precision_score(y_test, y_test_xg, average='macro'),\n",
        "          ],\n",
        "          \"AUC-ROC\": [\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_xg_proba, multi_class='ovr', average='macro'),\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_test), y_test_xg_proba, multi_class='ovr', average='macro'),\n",
        "          ]\n",
        "      }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nXGBoost Model Performance Metrics\")\n",
        "    print(\"Configuration Name: \", name)\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_xg_proba, multi_class='ovr', average='macro')\n",
        "    storeResults(\n",
        "        'XGBoost Model',\n",
        "        name,\n",
        "        metrics.accuracy_score(y_test, y_test_xg),\n",
        "        metrics.f1_score(y_test, y_test_xg, average='macro'),\n",
        "        metrics.recall_score(y_test, y_test_xg, average='macro'),\n",
        "        metrics.precision_score(y_test, y_test_xg, average='macro'),\n",
        "        auc_score\n",
        "    )\n",
        "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
        "    print(xgb.best_params_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gFupPmNsedu",
        "outputId": "4d1e1d5d-77e7-4c05-e327-309cbe1f0179"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal number of features to select using Boruta: 10\n",
            "\n",
            "=== XGBoost Model Performance with Hyperparameter Tuning ===\n",
            "\n",
            "Running XGBoost with Original Data configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "XGBoost Model Performance Metrics\n",
            "Configuration Name:  Original Data\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.92      0.90    0.89       0.91     0.95\n",
            "    Test      0.95      0.93    0.95       0.92     0.98\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'subsample': np.float64(0.1), 'skip_drop': 0.1, 'scale_pos_weight': 5, 'sample_type': 'weighted', 'reg_lambda': np.float64(0.1), 'reg_alpha': np.float64(0.1), 'rate_drop': 0.1, 'random_state': 2, 'normalize_type': 'forest', 'n_estimators': 490, 'min_child_weight': 1, 'max_depth': 42, 'max_delta_step': 5, 'learning_rate': np.float64(0.06670000000000001), 'gamma': np.float64(0.1), 'estimator__n_estimators': 340, 'colsample_bytree': 0.8, 'colsample_bynode': 0.6, 'colsample_bylevel': 1.0, 'booster': 'gbtree'}\n",
            "\n",
            "Running XGBoost with SMOTETomek configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "XGBoost Model Performance Metrics\n",
            "Configuration Name:  SMOTETomek\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.95      0.95    0.95       0.95     0.99\n",
            "    Test      0.90      0.83    0.92       0.81     0.97\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'subsample': np.float64(0.1), 'skip_drop': 0, 'scale_pos_weight': 2, 'sample_type': 'weighted', 'reg_lambda': np.float64(0.1), 'reg_alpha': np.float64(0.1), 'rate_drop': 0, 'random_state': 49, 'normalize_type': 'tree', 'n_estimators': 400, 'min_child_weight': 1, 'max_depth': 12, 'max_delta_step': 0, 'learning_rate': np.float64(0.06670000000000001), 'gamma': np.float64(0.0), 'estimator__n_estimators': 390, 'colsample_bytree': 0.8, 'colsample_bynode': 0.8, 'colsample_bylevel': 1.0, 'booster': 'gbtree'}\n",
            "\n",
            "Running XGBoost with Normalized Data configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "XGBoost Model Performance Metrics\n",
            "Configuration Name:  Normalized Data\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.92      0.90    0.89       0.91     0.94\n",
            "    Test      0.97      0.94    0.97       0.92     0.97\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'subsample': np.float64(0.1), 'skip_drop': 0, 'scale_pos_weight': 5, 'sample_type': 'weighted', 'reg_lambda': np.float64(0.1), 'reg_alpha': np.float64(0.1), 'rate_drop': 0, 'random_state': 42, 'normalize_type': 'tree', 'n_estimators': 260, 'min_child_weight': 1, 'max_depth': 12, 'max_delta_step': 0, 'learning_rate': np.float64(0.0889), 'gamma': np.float64(0.1), 'estimator__n_estimators': 320, 'colsample_bytree': 0.8, 'colsample_bynode': 0.8, 'colsample_bylevel': 1.0, 'booster': 'gbtree'}\n",
            "\n",
            "Running XGBoost with MI configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "XGBoost Model Performance Metrics\n",
            "Configuration Name:  MI\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.92      0.89    0.88       0.92     0.93\n",
            "    Test      0.95      0.90    0.95       0.88     0.96\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'subsample': np.float64(0.1), 'skip_drop': 0.1, 'scale_pos_weight': 1, 'sample_type': 'weighted', 'reg_lambda': np.float64(0.1), 'reg_alpha': np.float64(0.1), 'rate_drop': 0, 'random_state': 2, 'normalize_type': 'tree', 'n_estimators': 440, 'min_child_weight': 1, 'max_depth': 42, 'max_delta_step': 0, 'learning_rate': np.float64(0.06670000000000001), 'gamma': np.float64(0.05), 'estimator__n_estimators': 340, 'colsample_bytree': 0.3, 'colsample_bynode': 0.6, 'colsample_bylevel': 1.0, 'booster': 'gbtree'}\n",
            "\n",
            "Running XGBoost with LDA configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "XGBoost Model Performance Metrics\n",
            "Configuration Name:  LDA\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.91      0.89    0.87       0.91     0.95\n",
            "    Test      0.95      0.90    0.95       0.88     0.95\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'subsample': np.float64(0.1), 'skip_drop': 0.1, 'scale_pos_weight': 1, 'sample_type': 'weighted', 'reg_lambda': np.float64(0.1), 'reg_alpha': np.float64(0.1), 'rate_drop': 0, 'random_state': 51, 'normalize_type': 'tree', 'n_estimators': 460, 'min_child_weight': 1, 'max_depth': 22, 'max_delta_step': 0, 'learning_rate': np.float64(0.06670000000000001), 'gamma': np.float64(0.1), 'estimator__n_estimators': 360, 'colsample_bytree': 0.3, 'colsample_bynode': 0.8, 'colsample_bylevel': 1.0, 'booster': 'gbtree'}\n",
            "\n",
            "Running XGBoost with Boruta configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "XGBoost Model Performance Metrics\n",
            "Configuration Name:  Boruta\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.91      0.88    0.86       0.90     0.92\n",
            "    Test      0.97      0.94    0.97       0.92     0.98\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'subsample': np.float64(0.1), 'skip_drop': 0.1, 'scale_pos_weight': 2, 'sample_type': 'weighted', 'reg_lambda': np.float64(0.1), 'reg_alpha': np.float64(0.1), 'rate_drop': 0, 'random_state': 51, 'normalize_type': 'forest', 'n_estimators': 340, 'min_child_weight': 3, 'max_depth': 12, 'max_delta_step': 0, 'learning_rate': np.float64(0.06670000000000001), 'gamma': np.float64(0.0), 'estimator__n_estimators': 50, 'colsample_bytree': 0.8, 'colsample_bynode': 0.6, 'colsample_bylevel': 1.0, 'booster': 'gbtree'}\n",
            "\n",
            "Running XGBoost with Autoencoder configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "XGBoost Model Performance Metrics\n",
            "Configuration Name:  Autoencoder\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.93      0.90    0.89       0.92     0.97\n",
            "    Test      0.95      0.93    0.95       0.92     0.96\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'subsample': np.float64(0.1), 'skip_drop': 0, 'scale_pos_weight': 5, 'sample_type': 'weighted', 'reg_lambda': np.float64(0.1), 'reg_alpha': np.float64(0.1), 'rate_drop': 0.1, 'random_state': 49, 'normalize_type': 'tree', 'n_estimators': 280, 'min_child_weight': 1, 'max_depth': 2, 'max_delta_step': 0, 'learning_rate': np.float64(0.0889), 'gamma': np.float64(0.1), 'estimator__n_estimators': 110, 'colsample_bytree': 0.8, 'colsample_bynode': 0.8, 'colsample_bylevel': 1.0, 'booster': 'gbtree'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Boosting"
      ],
      "metadata": {
        "id": "RQWUpTh_uU8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "configurations = []\n",
        "configurations.append(('Original Data', X_train, X_test, y_train))\n",
        "\n",
        "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
        "configurations.append(('MI', X_train_mi, X_test_mi, y_train))\n",
        "configurations.append(('LDA', X_train_lda, X_test_lda, y_train))\n",
        "\n",
        "configurations.append(('Boruta', X_train_boruta, X_test_boruta, y_train))\n",
        "configurations.append(('Autoencoder', X_train_encoded, X_test_encoded, y_train))\n",
        "\n",
        "selected_features = boruta_selector.support_\n",
        "optimal_features = sum(selected_features)\n",
        "print(f\"Optimal number of features to select using Boruta: {optimal_features}\")\n",
        "\n",
        "configurations.append(('SMOTETomek', X_train_resample, X_test_normalized, y_train_resample))\n",
        "\n",
        "# Step 4: Gradient Boosting + GridSearchCV\n",
        "print(\"\\n=== Gradient Boosting Model Performance with Hyperparameter Tuning ===\")\n",
        "\n",
        "param_grid = {\n",
        "    'loss': ['log_loss'],\n",
        "    'learning_rate': np.linspace(0.0001, 0.1, 10), #[0.001],\n",
        "    'n_estimators': range(40, 400, 10), #[47, 50, 400],\n",
        "    'subsample': np.linspace(0.1, 0.9, 3), #[0.8653],\n",
        "    'max_depth': range(2, 100, 5), #[28],\n",
        "    'init': [None],\n",
        "    'max_leaf_nodes': [None],\n",
        "    'min_samples_split': range(2, 50, 3), #[10],\n",
        "    'min_samples_leaf': range(2, 50, 3), #[15],\n",
        "    'min_weight_fraction_leaf': [0.0],\n",
        "    'min_impurity_decrease': [0.0],\n",
        "    'validation_fraction': [0.1],\n",
        "    'n_iter_no_change': [None],\n",
        "    'tol': np.linspace(0.0001, 0.1, 10), #[0.0001],\n",
        "    'ccp_alpha': np.linspace(0.0001, 0.1, 10), #[0.01, 0.0001],\n",
        "    'max_features': ['sqrt'],\n",
        "    'verbose': [0],\n",
        "    'warm_start': [False],\n",
        "    'criterion': ['friedman_mse'],\n",
        "    'random_state': [0]\n",
        "}\n",
        "\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning Gradient Boosting with {name} configuration...\")\n",
        "    gbc = RandomizedSearchCV(GradientBoostingClassifier(), param_grid, cv=10, n_iter=50, n_jobs=-1, scoring=['accuracy'], refit='accuracy', verbose=2)\n",
        "    gbc.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_gb = gbc.predict(X_train_cfg)\n",
        "    y_test_gb = gbc.predict(X_test_cfg)\n",
        "    y_train_gb_proba = gbc.predict_proba(X_train_cfg)\n",
        "    y_test_gb_proba = gbc.predict_proba(X_test_cfg)\n",
        "\n",
        "    metrics_dict = {\n",
        "          \"Dataset\": [\"Training\", \"Test\"],\n",
        "          \"Accuracy\": [\n",
        "              metrics.accuracy_score(y_train_cfg, y_train_gb),\n",
        "              metrics.accuracy_score(y_test, y_test_gb),\n",
        "          ],\n",
        "          \"F1 Score\": [\n",
        "              metrics.f1_score(y_train_cfg, y_train_gb, average='macro'),\n",
        "              metrics.f1_score(y_test, y_test_gb, average='macro'),\n",
        "          ],\n",
        "          \"Recall\": [\n",
        "              metrics.recall_score(y_train_cfg, y_train_gb, average='macro'),\n",
        "              metrics.recall_score(y_test, y_test_gb, average='macro'),\n",
        "          ],\n",
        "          \"Precision\": [\n",
        "              metrics.precision_score(y_train_cfg, y_train_gb, average='macro'),\n",
        "              metrics.precision_score(y_test, y_test_gb, average='macro'),\n",
        "          ],\n",
        "          \"AUC-ROC\": [\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_gb_proba, multi_class='ovr', average='macro'),\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_test), y_test_gb_proba, multi_class='ovr', average='macro'),\n",
        "          ]\n",
        "      }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nGradien Boosting Model Performance Metrics\")\n",
        "    print(\"Configuration Name: \", name)\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_gb_proba, multi_class='ovr', average='macro')\n",
        "    storeResults(\n",
        "          'Gradient Boosting',\n",
        "          name,\n",
        "          metrics.accuracy_score(y_test, y_test_gb),\n",
        "          metrics.f1_score(y_test, y_test_gb, average='macro'),\n",
        "          metrics.recall_score(y_test, y_test_gb, average='macro'),\n",
        "          metrics.precision_score(y_test, y_test_gb, average='macro'),\n",
        "          auc_score\n",
        "      )\n",
        "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
        "    print(gbc.best_params_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XoccNFtuYDy",
        "outputId": "8ce787f2-74f5-457d-b3ed-184e76445c7d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal number of features to select using Boruta: 10\n",
            "\n",
            "=== Gradient Boosting Model Performance with Hyperparameter Tuning ===\n",
            "\n",
            "Running Gradient Boosting with Original Data configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "Gradien Boosting Model Performance Metrics\n",
            "Configuration Name:  Original Data\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.92      0.90    0.89       0.91     0.98\n",
            "    Test      0.97      0.94    0.97       0.92     0.97\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'warm_start': False, 'verbose': 0, 'validation_fraction': 0.1, 'tol': np.float64(0.07780000000000001), 'subsample': np.float64(0.9), 'random_state': 0, 'n_iter_no_change': None, 'n_estimators': 310, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 41, 'min_samples_leaf': 29, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': None, 'max_features': 'sqrt', 'max_depth': 27, 'loss': 'log_loss', 'learning_rate': np.float64(0.033400000000000006), 'init': None, 'criterion': 'friedman_mse', 'ccp_alpha': np.float64(0.0001)}\n",
            "\n",
            "Running Gradient Boosting with Normalized Data configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "Gradien Boosting Model Performance Metrics\n",
            "Configuration Name:  Normalized Data\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.92      0.90    0.89       0.91     0.98\n",
            "    Test      0.97      0.94    0.97       0.92     0.97\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'warm_start': False, 'verbose': 0, 'validation_fraction': 0.1, 'tol': np.float64(0.0889), 'subsample': np.float64(0.9), 'random_state': 0, 'n_iter_no_change': None, 'n_estimators': 170, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 17, 'min_samples_leaf': 32, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': None, 'max_features': 'sqrt', 'max_depth': 52, 'loss': 'log_loss', 'learning_rate': np.float64(0.1), 'init': None, 'criterion': 'friedman_mse', 'ccp_alpha': np.float64(0.0001)}\n",
            "\n",
            "Running Gradient Boosting with MI configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "Gradien Boosting Model Performance Metrics\n",
            "Configuration Name:  MI\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.92      0.90    0.89       0.91     0.97\n",
            "    Test      0.95      0.90    0.95       0.88     0.96\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'warm_start': False, 'verbose': 0, 'validation_fraction': 0.1, 'tol': np.float64(0.033400000000000006), 'subsample': np.float64(0.9), 'random_state': 0, 'n_iter_no_change': None, 'n_estimators': 180, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 17, 'min_samples_leaf': 41, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': None, 'max_features': 'sqrt', 'max_depth': 17, 'loss': 'log_loss', 'learning_rate': np.float64(0.1), 'init': None, 'criterion': 'friedman_mse', 'ccp_alpha': np.float64(0.0001)}\n",
            "\n",
            "Running Gradient Boosting with LDA configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "Gradien Boosting Model Performance Metrics\n",
            "Configuration Name:  LDA\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.92      0.90    0.89       0.91     0.97\n",
            "    Test      0.93      0.87    0.94       0.85     0.96\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'warm_start': False, 'verbose': 0, 'validation_fraction': 0.1, 'tol': np.float64(0.044500000000000005), 'subsample': np.float64(0.9), 'random_state': 0, 'n_iter_no_change': None, 'n_estimators': 190, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 44, 'min_samples_leaf': 17, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': None, 'max_features': 'sqrt', 'max_depth': 47, 'loss': 'log_loss', 'learning_rate': np.float64(0.0889), 'init': None, 'criterion': 'friedman_mse', 'ccp_alpha': np.float64(0.0001)}\n",
            "\n",
            "Running Gradient Boosting with Boruta configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "Gradien Boosting Model Performance Metrics\n",
            "Configuration Name:  Boruta\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.92      0.90    0.88       0.92     0.98\n",
            "    Test      0.97      0.94    0.97       0.92     0.97\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'warm_start': False, 'verbose': 0, 'validation_fraction': 0.1, 'tol': np.float64(0.0112), 'subsample': np.float64(0.9), 'random_state': 0, 'n_iter_no_change': None, 'n_estimators': 310, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 32, 'min_samples_leaf': 23, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': None, 'max_features': 'sqrt', 'max_depth': 57, 'loss': 'log_loss', 'learning_rate': np.float64(0.033400000000000006), 'init': None, 'criterion': 'friedman_mse', 'ccp_alpha': np.float64(0.0001)}\n",
            "\n",
            "Running Gradient Boosting with Autoencoder configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "Gradien Boosting Model Performance Metrics\n",
            "Configuration Name:  Autoencoder\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.92      0.90    0.89       0.91     0.98\n",
            "    Test      0.95      0.90    0.95       0.88     0.96\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'warm_start': False, 'verbose': 0, 'validation_fraction': 0.1, 'tol': np.float64(0.07780000000000001), 'subsample': np.float64(0.9), 'random_state': 0, 'n_iter_no_change': None, 'n_estimators': 360, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 23, 'min_samples_leaf': 47, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': None, 'max_features': 'sqrt', 'max_depth': 47, 'loss': 'log_loss', 'learning_rate': np.float64(0.0223), 'init': None, 'criterion': 'friedman_mse', 'ccp_alpha': np.float64(0.0001)}\n",
            "\n",
            "Running Gradient Boosting with SMOTETomek configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "Gradien Boosting Model Performance Metrics\n",
            "Configuration Name:  SMOTETomek\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.96      0.96    0.96       0.96     0.99\n",
            "    Test      0.97      0.94    0.97       0.92     0.97\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'warm_start': False, 'verbose': 0, 'validation_fraction': 0.1, 'tol': np.float64(0.07780000000000001), 'subsample': np.float64(0.9), 'random_state': 0, 'n_iter_no_change': None, 'n_estimators': 270, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 2, 'min_samples_leaf': 38, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': None, 'max_features': 'sqrt', 'max_depth': 32, 'loss': 'log_loss', 'learning_rate': np.float64(0.07780000000000001), 'init': None, 'criterion': 'friedman_mse', 'ccp_alpha': np.float64(0.0001)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra Trees"
      ],
      "metadata": {
        "id": "EtjUT-NQw64A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "configurations = []\n",
        "configurations.append(('Original Data', X_train, X_test, y_train))\n",
        "\n",
        "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
        "configurations.append(('MI', X_train_mi, X_test_mi, y_train))\n",
        "configurations.append(('LDA', X_train_lda, X_test_lda, y_train))\n",
        "\n",
        "configurations.append(('Boruta', X_train_boruta, X_test_boruta, y_train))\n",
        "configurations.append(('Autoencoder', X_train_encoded, X_test_encoded, y_train))\n",
        "\n",
        "selected_features = boruta_selector.support_\n",
        "optimal_features = sum(selected_features)\n",
        "print(f\"Optimal number of features to select using Boruta: {optimal_features}\")\n",
        "\n",
        "configurations.append(('SMOTETomek', X_train_resample, X_test_normalized, y_train_resample))\n",
        "\n",
        "# Step 4: Extra Trees + GridSearchCV\n",
        "print(\"\\n=== Extra Trees Model Performance with Hyperparameter Tuning ===\")\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': range(100, 500, 13), #[50, 180],\n",
        "    'max_depth': range(5, 40, 3), #[10, 20],\n",
        "    'max_leaf_nodes': range(10, 100, 8), #[50, 100],\n",
        "    'min_samples_split': np.arange(1, 10, 1), #[2, 4],\n",
        "    'min_samples_leaf': np.arange(1, 10, 1), #[1, 2],\n",
        "    'min_weight_fraction_leaf': [0.0],\n",
        "    'min_impurity_decrease': [0.0],\n",
        "    'ccp_alpha': np.linspace(0.0001, 0.1, 11), #[0.001, 0.01],\n",
        "    'max_features': ['sqrt', 'log2'],\n",
        "    'class_weight': [None],\n",
        "    'bootstrap': [True, False],\n",
        "    'oob_score': [True, False],\n",
        "    'criterion': ['gini', 'log_loss'],\n",
        "    'random_state': range(2, 123, 5), #[51, 123]\n",
        "}\n",
        "\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning Extra Trees with {name} configuration...\")\n",
        "    etc = RandomizedSearchCV(ExtraTreesClassifier(), param_grid, cv=10, n_iter=50, n_jobs=-1, scoring=[\"accuracy\", \"f1_macro\"], refit='accuracy', verbose=2)\n",
        "    etc.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_et = etc.predict(X_train_cfg)\n",
        "    y_test_et = etc.predict(X_test_cfg)\n",
        "    y_train_et_proba = etc.predict_proba(X_train_cfg)\n",
        "    y_test_et_proba = etc.predict_proba(X_test_cfg)\n",
        "\n",
        "    metrics_dict = {\n",
        "          \"Dataset\": [\"Training\", \"Test\"],\n",
        "          \"Accuracy\": [\n",
        "              metrics.accuracy_score(y_train_cfg, y_train_et),\n",
        "              metrics.accuracy_score(y_test, y_test_et),\n",
        "          ],\n",
        "          \"F1 Score\": [\n",
        "              metrics.f1_score(y_train_cfg, y_train_et, average='macro'),\n",
        "              metrics.f1_score(y_test, y_test_et, average='macro'),\n",
        "          ],\n",
        "          \"Recall\": [\n",
        "              metrics.recall_score(y_train_cfg, y_train_et, average='macro'),\n",
        "              metrics.recall_score(y_test, y_test_et, average='macro'),\n",
        "          ],\n",
        "          \"Precision\": [\n",
        "              metrics.precision_score(y_train_cfg, y_train_et, average='macro'),\n",
        "              metrics.precision_score(y_test, y_test_et, average='macro'),\n",
        "          ],\n",
        "          \"AUC-ROC\": [\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_et_proba, multi_class='ovr', average='macro'),\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_test), y_test_et_proba, multi_class='ovr', average='macro'),\n",
        "          ]\n",
        "      }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nExtraTrees Model Performance Metrics\")\n",
        "    print(\"Configuration Name: \", name)\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_et_proba, multi_class='ovr', average='macro')\n",
        "    storeResults(\n",
        "          'Extra Trees',\n",
        "          name,\n",
        "          metrics.accuracy_score(y_test, y_test_et),\n",
        "          metrics.f1_score(y_test, y_test_et, average='macro'),\n",
        "          metrics.recall_score(y_test, y_test_et, average='macro'),\n",
        "          metrics.precision_score(y_test, y_test_et, average='macro'),\n",
        "          auc_score\n",
        "      )\n",
        "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
        "    print(etc.best_params_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01gypP_5xEKE",
        "outputId": "e60d1f33-6574-4ea7-c7c1-44003da6a654"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal number of features to select using Boruta: 10\n",
            "\n",
            "=== Extra Trees Model Performance with Hyperparameter Tuning ===\n",
            "\n",
            "Running Extra Trees with Original Data configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "ExtraTrees Model Performance Metrics\n",
            "Configuration Name:  Original Data\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.93      0.90    0.89       0.92     0.99\n",
            "    Test      0.95      0.90    0.95       0.88     0.97\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'random_state': 17, 'oob_score': False, 'n_estimators': 191, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': np.int64(2), 'min_samples_leaf': np.int64(1), 'min_impurity_decrease': 0.0, 'max_leaf_nodes': 98, 'max_features': 'log2', 'max_depth': 32, 'criterion': 'gini', 'class_weight': None, 'ccp_alpha': np.float64(0.0001), 'bootstrap': False}\n",
            "\n",
            "Running Extra Trees with Normalized Data configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "ExtraTrees Model Performance Metrics\n",
            "Configuration Name:  Normalized Data\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.92      0.90    0.89       0.91     0.98\n",
            "    Test      0.95      0.90    0.95       0.88     0.97\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'random_state': 42, 'oob_score': False, 'n_estimators': 230, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': np.int64(2), 'min_samples_leaf': np.int64(1), 'min_impurity_decrease': 0.0, 'max_leaf_nodes': 58, 'max_features': 'log2', 'max_depth': 14, 'criterion': 'log_loss', 'class_weight': None, 'ccp_alpha': np.float64(0.01009), 'bootstrap': True}\n",
            "\n",
            "Running Extra Trees with MI configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "ExtraTrees Model Performance Metrics\n",
            "Configuration Name:  MI\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.89      0.85    0.84       0.87     0.93\n",
            "    Test      0.93      0.89    0.94       0.87     0.96\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'random_state': 22, 'oob_score': False, 'n_estimators': 243, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': np.int64(7), 'min_samples_leaf': np.int64(7), 'min_impurity_decrease': 0.0, 'max_leaf_nodes': 50, 'max_features': 'log2', 'max_depth': 32, 'criterion': 'log_loss', 'class_weight': None, 'ccp_alpha': np.float64(0.01009), 'bootstrap': False}\n",
            "\n",
            "Running Extra Trees with LDA configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "ExtraTrees Model Performance Metrics\n",
            "Configuration Name:  LDA\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.89      0.86    0.82       0.90     0.95\n",
            "    Test      0.93      0.91    0.93       0.91     0.95\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'random_state': 57, 'oob_score': True, 'n_estimators': 451, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': np.int64(9), 'min_samples_leaf': np.int64(3), 'min_impurity_decrease': 0.0, 'max_leaf_nodes': 98, 'max_features': 'sqrt', 'max_depth': 20, 'criterion': 'gini', 'class_weight': None, 'ccp_alpha': np.float64(0.0001), 'bootstrap': True}\n",
            "\n",
            "Running Extra Trees with Boruta configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "ExtraTrees Model Performance Metrics\n",
            "Configuration Name:  Boruta\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.91      0.88    0.86       0.90     0.91\n",
            "    Test      0.97      0.94    0.97       0.92     0.97\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'random_state': 57, 'oob_score': True, 'n_estimators': 373, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': np.int64(3), 'min_samples_leaf': np.int64(6), 'min_impurity_decrease': 0.0, 'max_leaf_nodes': 90, 'max_features': 'log2', 'max_depth': 26, 'criterion': 'gini', 'class_weight': None, 'ccp_alpha': np.float64(0.1), 'bootstrap': True}\n",
            "\n",
            "Running Extra Trees with Autoencoder configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "ExtraTrees Model Performance Metrics\n",
            "Configuration Name:  Autoencoder\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.93      0.90    0.89       0.92     0.98\n",
            "    Test      0.95      0.90    0.95       0.88     0.97\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'random_state': 52, 'oob_score': False, 'n_estimators': 295, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': np.int64(7), 'min_samples_leaf': np.int64(1), 'min_impurity_decrease': 0.0, 'max_leaf_nodes': 58, 'max_features': 'log2', 'max_depth': 23, 'criterion': 'log_loss', 'class_weight': None, 'ccp_alpha': np.float64(0.0001), 'bootstrap': False}\n",
            "\n",
            "Running Extra Trees with SMOTETomek configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "ExtraTrees Model Performance Metrics\n",
            "Configuration Name:  SMOTETomek\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.91      0.91    0.91       0.92     0.98\n",
            "    Test      0.95      0.90    0.95       0.88     0.97\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'random_state': 37, 'oob_score': True, 'n_estimators': 295, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': np.int64(4), 'min_samples_leaf': np.int64(1), 'min_impurity_decrease': 0.0, 'max_leaf_nodes': 26, 'max_features': 'log2', 'max_depth': 17, 'criterion': 'log_loss', 'class_weight': None, 'ccp_alpha': np.float64(0.02008), 'bootstrap': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ADABoost"
      ],
      "metadata": {
        "id": "gSVf2NIHes4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "configurations = []\n",
        "configurations.append(('Original Data', X_train, X_test, y_train))\n",
        "\n",
        "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
        "configurations.append(('MI', X_train_mi, X_test_mi, y_train))\n",
        "configurations.append(('LDA', X_train_lda, X_test_lda, y_train))\n",
        "\n",
        "configurations.append(('Boruta', X_train_boruta, X_test_boruta, y_train))\n",
        "configurations.append(('Autoencoder', X_train_encoded, X_test_encoded, y_train))\n",
        "\n",
        "selected_features = boruta_selector.support_\n",
        "optimal_features = sum(selected_features)\n",
        "print(f\"Optimal number of features to select using Boruta: {optimal_features}\")\n",
        "\n",
        "configurations.append(('SMOTETomek', X_train_resample, X_test_normalized, y_train_resample))\n",
        "\n",
        "# Step 4: Extra Trees + GridSearchCV\n",
        "print(\"\\n=== AdaBoost Model Performance with Hyperparameter Tuning ===\")\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': range(300, 450, 13), #[50, 150],\n",
        "    'learning_rate': np.linspace(0.01, 0.05, 3), #[0.005, 0.5, 0.03, 0.003],\n",
        "    'estimator__max_depth': range(2, 10, 3), #[5, 20],\n",
        "    'estimator__min_samples_split': range(1, 5, 1), #[8],\n",
        "    'random_state': range(20, 60) #[42, 1234]\n",
        "}\n",
        "\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning AdaBoost with {name} configuration...\")\n",
        "    adb = RandomizedSearchCV(AdaBoostClassifier(estimator=DecisionTreeClassifier()), param_grid, cv=10, n_iter=50, n_jobs=-1,\n",
        "                             scoring=['accuracy', 'f1_macro'], refit='accuracy', verbose=2)\n",
        "    adb.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_ad = adb.predict(X_train_cfg)\n",
        "    y_test_ad = adb.predict(X_test_cfg)\n",
        "    y_train_ad_proba = adb.predict_proba(X_train_cfg)\n",
        "    y_test_ad_proba = adb.predict_proba(X_test_cfg)\n",
        "\n",
        "    metrics_dict = {\n",
        "          \"Dataset\": [\"Training\", \"Test\"],\n",
        "          \"Accuracy\": [\n",
        "              metrics.accuracy_score(y_train_cfg, y_train_ad),\n",
        "              metrics.accuracy_score(y_test, y_test_ad),\n",
        "          ],\n",
        "          \"F1 Score\": [\n",
        "              metrics.f1_score(y_train_cfg, y_train_ad, average='macro'),\n",
        "              metrics.f1_score(y_test, y_test_ad, average='macro'),\n",
        "          ],\n",
        "          \"Recall\": [\n",
        "              metrics.recall_score(y_train_cfg, y_train_ad, average='macro'),\n",
        "              metrics.recall_score(y_test, y_test_ad, average='macro'),\n",
        "          ],\n",
        "          \"Precision\": [\n",
        "              metrics.precision_score(y_train_cfg, y_train_ad, average='macro'),\n",
        "              metrics.precision_score(y_test, y_test_ad, average='macro'),\n",
        "          ],\n",
        "          \"AUC-ROC\": [\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_ad_proba, multi_class='ovr', average='macro'),\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_test), y_test_ad_proba, multi_class='ovr', average='macro'),\n",
        "          ]\n",
        "      }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nAdaBoost Model Performance Metrics\")\n",
        "    print(\"Configuration Name: \", name)\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_ad_proba, multi_class='ovr', average='macro')\n",
        "    storeResults(\n",
        "          'AdaBoost',\n",
        "          name,\n",
        "          metrics.accuracy_score(y_test, y_test_ad),\n",
        "          metrics.f1_score(y_test, y_test_ad, average='macro'),\n",
        "          metrics.recall_score(y_test, y_test_ad, average='macro'),\n",
        "          metrics.precision_score(y_test, y_test_ad, average='macro'),\n",
        "          auc_score\n",
        "      )\n",
        "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
        "    print(adb.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZClculzemGl",
        "outputId": "9cc3a09a-616a-408e-d32a-04881ab6f321"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal number of features to select using Boruta: 10\n",
            "\n",
            "=== AdaBoost Model Performance with Hyperparameter Tuning ===\n",
            "\n",
            "Running AdaBoost with Original Data configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "AdaBoost Model Performance Metrics\n",
            "Configuration Name:  Original Data\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.92      0.90    0.89       0.91     0.97\n",
            "    Test      0.95      0.90    0.95       0.88     0.98\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'random_state': 28, 'n_estimators': 443, 'learning_rate': np.float64(0.03), 'estimator__min_samples_split': 4, 'estimator__max_depth': 2}\n",
            "\n",
            "Running AdaBoost with Normalized Data configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "AdaBoost Model Performance Metrics\n",
            "Configuration Name:  Normalized Data\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.92      0.90    0.89       0.91     0.97\n",
            "    Test      0.97      0.94    0.97       0.92     0.97\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'random_state': 24, 'n_estimators': 404, 'learning_rate': np.float64(0.05), 'estimator__min_samples_split': 3, 'estimator__max_depth': 2}\n",
            "\n",
            "Running AdaBoost with MI configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "AdaBoost Model Performance Metrics\n",
            "Configuration Name:  MI\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.92      0.90    0.89       0.91     0.98\n",
            "    Test      0.95      0.90    0.95       0.88     0.95\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'random_state': 25, 'n_estimators': 417, 'learning_rate': np.float64(0.03), 'estimator__min_samples_split': 2, 'estimator__max_depth': 5}\n",
            "\n",
            "Running AdaBoost with LDA configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "AdaBoost Model Performance Metrics\n",
            "Configuration Name:  LDA\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.92      0.90    0.88       0.92     0.98\n",
            "    Test      0.93      0.89    0.94       0.86     0.94\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'random_state': 58, 'n_estimators': 443, 'learning_rate': np.float64(0.05), 'estimator__min_samples_split': 4, 'estimator__max_depth': 5}\n",
            "\n",
            "Running AdaBoost with Boruta configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "AdaBoost Model Performance Metrics\n",
            "Configuration Name:  Boruta\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.92      0.90    0.89       0.91     0.98\n",
            "    Test      0.95      0.90    0.95       0.88     0.97\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'random_state': 51, 'n_estimators': 326, 'learning_rate': np.float64(0.01), 'estimator__min_samples_split': 4, 'estimator__max_depth': 5}\n",
            "\n",
            "Running AdaBoost with Autoencoder configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "AdaBoost Model Performance Metrics\n",
            "Configuration Name:  Autoencoder\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.93      0.90    0.90       0.91     0.98\n",
            "    Test      0.93      0.87    0.94       0.85     0.95\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'random_state': 51, 'n_estimators': 339, 'learning_rate': np.float64(0.05), 'estimator__min_samples_split': 4, 'estimator__max_depth': 5}\n",
            "\n",
            "Running AdaBoost with SMOTETomek configuration...\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
            "\n",
            "AdaBoost Model Performance Metrics\n",
            "Configuration Name:  SMOTETomek\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.96      0.96    0.96       0.96     1.00\n",
            "    Test      0.91      0.90    0.93       0.88     0.98\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'random_state': 46, 'n_estimators': 443, 'learning_rate': np.float64(0.03), 'estimator__min_samples_split': 4, 'estimator__max_depth': 8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP"
      ],
      "metadata": {
        "id": "_TA7vnOBtT3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "configurations = []\n",
        "configurations.append(('Original Data', X_train, X_test, y_train))\n",
        "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
        "configurations.append(('MI', X_train_mi, X_test_mi, y_train))\n",
        "configurations.append(('LDA', X_train_lda, X_test_lda, y_train))\n",
        "configurations.append(('Boruta', X_train_boruta, X_test_boruta, y_train))\n",
        "configurations.append(('Autoencoder', X_train_encoded, X_test_encoded, y_train))\n",
        "configurations.append(('SMOTETomek', X_train_resample, X_test_normalized, y_train_resample))\n",
        "\n",
        "mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(100, 50),\n",
        "    activation='relu',\n",
        "    solver='sgd',\n",
        "    alpha=0.01,\n",
        "    batch_size='auto',\n",
        "    learning_rate='constant',\n",
        "    max_iter=1000,\n",
        "    random_state=42)\n",
        "\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning MLP Classifier with {name} configuration...\")\n",
        "    mlp.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_mlp = mlp.predict(X_train_cfg)\n",
        "    y_test_mlp = mlp.predict(X_test_cfg)\n",
        "    y_train_mlp_proba = mlp.predict_proba(X_train_cfg)\n",
        "    y_test_mlp_proba = mlp.predict_proba(X_test_cfg)\n",
        "\n",
        "    metrics_dict = {\n",
        "          \"Dataset\": [\"Training\", \"Test\"],\n",
        "          \"Accuracy\": [\n",
        "              metrics.accuracy_score(y_train_cfg, y_train_mlp),\n",
        "              metrics.accuracy_score(y_test, y_test_mlp),\n",
        "          ],\n",
        "          \"F1 Score\": [\n",
        "              metrics.f1_score(y_train_cfg, y_train_mlp, average='macro'),\n",
        "              metrics.f1_score(y_test, y_test_mlp, average='macro'),\n",
        "          ],\n",
        "          \"Recall\": [\n",
        "              metrics.recall_score(y_train_cfg, y_train_mlp, average='macro'),\n",
        "              metrics.recall_score(y_test, y_test_mlp, average='macro'),\n",
        "          ],\n",
        "          \"Precision\": [\n",
        "              metrics.precision_score(y_train_cfg, y_train_mlp, average='macro'),\n",
        "              metrics.precision_score(y_test, y_test_mlp, average='macro'),\n",
        "          ],\n",
        "          \"AUC-ROC\": [\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_mlp_proba, multi_class='ovr', average='macro'),\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_test), y_test_mlp_proba, multi_class='ovr', average='macro'),\n",
        "          ]\n",
        "      }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\MLP Model Performance Metrics\")\n",
        "    print(\"Configuration Name: \", name)\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_mlp_proba, multi_class='ovr', average='macro')\n",
        "    storeResults(\n",
        "          'MLP Classifier',\n",
        "          name,\n",
        "          metrics.accuracy_score(y_test, y_test_mlp),\n",
        "          metrics.f1_score(y_test, y_test_mlp, average='macro'),\n",
        "          metrics.recall_score(y_test, y_test_mlp, average='macro'),\n",
        "          metrics.precision_score(y_test, y_test_mlp, average='macro'),\n",
        "          auc_score\n",
        "      )\n",
        "    # print(\"Best hyperparameters found by GridSearchCV:\")\n",
        "    # print(mlp.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScDMjCP9tTZH",
        "outputId": "fd47db74-4a06-442e-d2cc-b1fd498dba3a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running MLP Classifier with Original Data configuration...\n",
            "\\MLP Model Performance Metrics\n",
            "Configuration Name:  Original Data\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.64      0.26    0.33       0.21     0.50\n",
            "    Test      0.64      0.26    0.33       0.21     0.50\n",
            "\n",
            "Running MLP Classifier with Normalized Data configuration...\n",
            "\\MLP Model Performance Metrics\n",
            "Configuration Name:  Normalized Data\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.92      0.89    0.88       0.92     0.94\n",
            "    Test      0.95      0.93    0.95       0.92     0.98\n",
            "\n",
            "Running MLP Classifier with MI configuration...\n",
            "\\MLP Model Performance Metrics\n",
            "Configuration Name:  MI\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.89      0.86    0.84       0.89     0.92\n",
            "    Test      0.95      0.93    0.95       0.92     0.96\n",
            "\n",
            "Running MLP Classifier with LDA configuration...\n",
            "\\MLP Model Performance Metrics\n",
            "Configuration Name:  LDA\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.87      0.84    0.82       0.87     0.91\n",
            "    Test      0.93      0.91    0.93       0.91     0.96\n",
            "\n",
            "Running MLP Classifier with Boruta configuration...\n",
            "\\MLP Model Performance Metrics\n",
            "Configuration Name:  Boruta\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.91      0.88    0.86       0.91     0.93\n",
            "    Test      0.97      0.94    0.97       0.92     0.99\n",
            "\n",
            "Running MLP Classifier with Autoencoder configuration...\n",
            "\\MLP Model Performance Metrics\n",
            "Configuration Name:  Autoencoder\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.89      0.86    0.85       0.89     0.92\n",
            "    Test      0.95      0.93    0.95       0.92     0.99\n",
            "\n",
            "Running MLP Classifier with SMOTETomek configuration...\n",
            "\\MLP Model Performance Metrics\n",
            "Configuration Name:  SMOTETomek\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training      0.90      0.90    0.90       0.90     0.97\n",
            "    Test      0.95      0.90    0.95       0.88     0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Result\n"
      ],
      "metadata": {
        "id": "DZjM6oTkrnlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the dataframe\n",
        "result = pd.DataFrame({\n",
        "    'ML Model': ML_Model,\n",
        "    'Configuration': ML_Config,\n",
        "    'Accuracy': [f\"{acc * 100:.3f}%\" for acc in accuracy],\n",
        "    'F1 Score': [f\"{f1 * 100:.3f}%\" for f1 in f1_score],\n",
        "    'Recall': [f\"{rec * 100:.3f}%\" for rec in recall],\n",
        "    'Precision': [f\"{prec * 100:.3f}%\" for prec in precision],\n",
        "    'ROC_AUC': [f\"{roc * 100:.3f}%\" for roc in auc_roc],\n",
        "})\n",
        "\n",
        "# Remove duplicates based on model and configuration\n",
        "result.drop_duplicates(subset=[\"ML Model\", \"Configuration\"], inplace=True)\n",
        "\n",
        "# Display the result\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"MODEL PERFORMANCE RESULTS\")\n",
        "print(\"=\" * 100)\n",
        "print(result.to_string(index=False))\n",
        "\n",
        "# Save the result to a CSV file\n",
        "# result.to_csv('final_results/model_results.csv', index=False)\n",
        "# print(\"\\nResults saved to model_results.csv\")\n",
        "\n",
        "# Sort by Accuracy and F1 Score\n",
        "sorted_result = result.sort_values(by=['Accuracy', 'F1 Score'], ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Display the sorted result\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"SORTED MODEL PERFORMANCE RESULTS (by Accuracy and F1 Score)\")\n",
        "print(\"=\" * 100)\n",
        "print(sorted_result.to_string(index=False))\n",
        "\n",
        "# Save the sorted result\n",
        "# sorted_result.to_csv('final_results/sorted_model_results.csv', index=False)\n",
        "# print(\"\\nSorted results saved to sorted_model_results.csv\")\n",
        "\n",
        "# Extract top configuration per ML model\n",
        "top_per_model = sorted_result.groupby('ML Model', as_index=False).first()\n",
        "\n",
        "# Display and save the top configuration table\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"TOP CONFIGURATION PER MODEL\")\n",
        "print(\"=\" * 100)\n",
        "print(top_per_model.to_string(index=False))\n",
        "\n",
        "# top_per_model.to_csv('final_results/top_configurations.csv', index=False)\n",
        "# print(\"\\nTop configuration per model saved to top_configurations.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jgo6Ddxwr281",
        "outputId": "6eeda352-b7e9-47ad-f0e8-9f9bbfa376ce"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================================================================================================\n",
            "MODEL PERFORMANCE RESULTS\n",
            "====================================================================================================\n",
            "           ML Model   Configuration Accuracy F1 Score  Recall Precision ROC_AUC\n",
            "Logistic Regression   Original Data  94.828%  92.880% 95.178%   91.579% 94.328%\n",
            "Logistic Regression      SMOTETomek  89.655%  85.101% 92.475%   81.726% 95.248%\n",
            "Logistic Regression Normalized Data  94.828%  92.880% 95.178%   91.579% 97.787%\n",
            "Logistic Regression              MI  93.103%  91.316% 93.217%   90.769% 95.526%\n",
            "Logistic Regression             LDA  86.207%  84.321% 85.374%   87.907% 95.322%\n",
            "Logistic Regression          Boruta  94.828%  90.349% 95.178%   87.988% 98.035%\n",
            "Logistic Regression     Autoencoder  94.828%  92.880% 95.178%   91.579% 97.786%\n",
            "      Random Forest   Original Data  94.828%  90.349% 95.178%   87.988% 97.929%\n",
            "      Random Forest      SMOTETomek  94.828%  90.349% 95.178%   87.988% 97.140%\n",
            "      Random Forest Normalized Data  96.552%  94.385% 97.138%   92.432% 96.515%\n",
            "      Random Forest              MI  93.103%  87.456% 94.277%   84.788% 96.294%\n",
            "      Random Forest             LDA  91.379%  84.954% 93.376%   82.381% 95.106%\n",
            "      Random Forest          Boruta  96.552%  94.385% 97.138%   92.432% 98.108%\n",
            "      Random Forest     Autoencoder  94.828%  90.349% 95.178%   87.988% 96.805%\n",
            "           AdaBoost   Original Data  94.828%  90.349% 95.178%   87.988% 97.882%\n",
            "           AdaBoost Normalized Data  96.552%  94.385% 97.138%   92.432% 97.496%\n",
            "           AdaBoost              MI  94.828%  90.349% 95.178%   87.988% 95.341%\n",
            "           AdaBoost             LDA  93.103%  88.933% 94.277%   85.880% 93.840%\n",
            "           AdaBoost          Boruta  94.828%  90.349% 95.178%   87.988% 96.921%\n",
            "           AdaBoost     Autoencoder  93.103%  87.456% 94.277%   84.788% 95.056%\n",
            "           AdaBoost      SMOTETomek  91.379%  90.092% 93.376%   87.560% 97.599%\n",
            "      XGBoost Model   Original Data  94.828%  92.880% 95.178%   91.579% 97.848%\n",
            "      XGBoost Model      SMOTETomek  89.655%  82.749% 92.475%   80.501% 96.753%\n",
            "      XGBoost Model Normalized Data  96.552%  94.385% 97.138%   92.432% 97.319%\n",
            "      XGBoost Model              MI  94.828%  90.349% 95.178%   87.988% 96.318%\n",
            "      XGBoost Model             LDA  94.828%  90.349% 95.178%   87.988% 95.364%\n",
            "      XGBoost Model          Boruta  96.552%  94.385% 97.138%   92.432% 97.672%\n",
            "      XGBoost Model     Autoencoder  94.828%  92.880% 95.178%   91.579% 96.165%\n",
            "  Gradient Boosting   Original Data  96.552%  94.385% 97.138%   92.432% 97.261%\n",
            "  Gradient Boosting Normalized Data  96.552%  94.385% 97.138%   92.432% 97.261%\n",
            "  Gradient Boosting              MI  94.828%  90.349% 95.178%   87.988% 96.323%\n",
            "  Gradient Boosting             LDA  93.103%  87.456% 94.277%   84.788% 95.574%\n",
            "  Gradient Boosting          Boruta  96.552%  94.385% 97.138%   92.432% 97.337%\n",
            "  Gradient Boosting     Autoencoder  94.828%  90.349% 95.178%   87.988% 95.648%\n",
            "  Gradient Boosting      SMOTETomek  96.552%  94.385% 97.138%   92.432% 96.722%\n",
            "        Extra Trees   Original Data  94.828%  90.349% 95.178%   87.988% 97.109%\n",
            "        Extra Trees Normalized Data  94.828%  90.349% 95.178%   87.988% 96.797%\n",
            "        Extra Trees              MI  93.103%  89.448% 94.277%   87.087% 96.288%\n",
            "        Extra Trees             LDA  93.103%  91.316% 93.217%   90.769% 94.987%\n",
            "        Extra Trees          Boruta  96.552%  94.385% 97.138%   92.432% 97.205%\n",
            "        Extra Trees     Autoencoder  94.828%  90.349% 95.178%   87.988% 96.508%\n",
            "        Extra Trees      SMOTETomek  94.828%  90.349% 95.178%   87.988% 97.198%\n",
            "     MLP Classifier   Original Data  63.793%  25.965% 33.333%   21.264% 50.000%\n",
            "     MLP Classifier Normalized Data  94.828%  92.880% 95.178%   91.579% 98.417%\n",
            "     MLP Classifier              MI  94.828%  92.880% 95.178%   91.579% 95.771%\n",
            "     MLP Classifier             LDA  93.103%  91.316% 93.217%   90.769% 95.536%\n",
            "     MLP Classifier          Boruta  96.552%  94.385% 97.138%   92.432% 98.770%\n",
            "     MLP Classifier     Autoencoder  94.828%  92.880% 95.178%   91.579% 98.632%\n",
            "     MLP Classifier      SMOTETomek  94.828%  90.349% 95.178%   87.988% 97.217%\n",
            "K-Nearest Neighbors   Original Data  94.828%  90.349% 95.178%   87.988% 96.536%\n",
            "K-Nearest Neighbors      SMOTETomek  94.828%  90.349% 95.178%   87.988% 97.368%\n",
            "K-Nearest Neighbors Normalized Data  94.828%  92.880% 95.178%   91.579% 98.174%\n",
            "K-Nearest Neighbors              MI  94.828%  90.349% 95.178%   87.988% 95.390%\n",
            "K-Nearest Neighbors             LDA  94.828%  90.349% 95.178%   87.988% 95.390%\n",
            "K-Nearest Neighbors          Boruta  94.828%  90.349% 95.178%   87.988% 97.135%\n",
            "K-Nearest Neighbors     Autoencoder  94.828%  90.349% 95.178%   87.988% 96.944%\n",
            "\n",
            "====================================================================================================\n",
            "SORTED MODEL PERFORMANCE RESULTS (by Accuracy and F1 Score)\n",
            "====================================================================================================\n",
            "           ML Model   Configuration Accuracy F1 Score  Recall Precision ROC_AUC\n",
            "      Random Forest Normalized Data  96.552%  94.385% 97.138%   92.432% 96.515%\n",
            "      Random Forest          Boruta  96.552%  94.385% 97.138%   92.432% 98.108%\n",
            "           AdaBoost Normalized Data  96.552%  94.385% 97.138%   92.432% 97.496%\n",
            "      XGBoost Model Normalized Data  96.552%  94.385% 97.138%   92.432% 97.319%\n",
            "      XGBoost Model          Boruta  96.552%  94.385% 97.138%   92.432% 97.672%\n",
            "  Gradient Boosting   Original Data  96.552%  94.385% 97.138%   92.432% 97.261%\n",
            "  Gradient Boosting Normalized Data  96.552%  94.385% 97.138%   92.432% 97.261%\n",
            "  Gradient Boosting          Boruta  96.552%  94.385% 97.138%   92.432% 97.337%\n",
            "  Gradient Boosting      SMOTETomek  96.552%  94.385% 97.138%   92.432% 96.722%\n",
            "        Extra Trees          Boruta  96.552%  94.385% 97.138%   92.432% 97.205%\n",
            "     MLP Classifier          Boruta  96.552%  94.385% 97.138%   92.432% 98.770%\n",
            "Logistic Regression   Original Data  94.828%  92.880% 95.178%   91.579% 94.328%\n",
            "Logistic Regression Normalized Data  94.828%  92.880% 95.178%   91.579% 97.787%\n",
            "Logistic Regression     Autoencoder  94.828%  92.880% 95.178%   91.579% 97.786%\n",
            "      XGBoost Model   Original Data  94.828%  92.880% 95.178%   91.579% 97.848%\n",
            "      XGBoost Model     Autoencoder  94.828%  92.880% 95.178%   91.579% 96.165%\n",
            "     MLP Classifier Normalized Data  94.828%  92.880% 95.178%   91.579% 98.417%\n",
            "     MLP Classifier              MI  94.828%  92.880% 95.178%   91.579% 95.771%\n",
            "     MLP Classifier     Autoencoder  94.828%  92.880% 95.178%   91.579% 98.632%\n",
            "K-Nearest Neighbors Normalized Data  94.828%  92.880% 95.178%   91.579% 98.174%\n",
            "Logistic Regression          Boruta  94.828%  90.349% 95.178%   87.988% 98.035%\n",
            "      Random Forest   Original Data  94.828%  90.349% 95.178%   87.988% 97.929%\n",
            "      Random Forest      SMOTETomek  94.828%  90.349% 95.178%   87.988% 97.140%\n",
            "      Random Forest     Autoencoder  94.828%  90.349% 95.178%   87.988% 96.805%\n",
            "           AdaBoost   Original Data  94.828%  90.349% 95.178%   87.988% 97.882%\n",
            "           AdaBoost              MI  94.828%  90.349% 95.178%   87.988% 95.341%\n",
            "           AdaBoost          Boruta  94.828%  90.349% 95.178%   87.988% 96.921%\n",
            "      XGBoost Model              MI  94.828%  90.349% 95.178%   87.988% 96.318%\n",
            "      XGBoost Model             LDA  94.828%  90.349% 95.178%   87.988% 95.364%\n",
            "  Gradient Boosting              MI  94.828%  90.349% 95.178%   87.988% 96.323%\n",
            "  Gradient Boosting     Autoencoder  94.828%  90.349% 95.178%   87.988% 95.648%\n",
            "        Extra Trees   Original Data  94.828%  90.349% 95.178%   87.988% 97.109%\n",
            "        Extra Trees Normalized Data  94.828%  90.349% 95.178%   87.988% 96.797%\n",
            "        Extra Trees     Autoencoder  94.828%  90.349% 95.178%   87.988% 96.508%\n",
            "        Extra Trees      SMOTETomek  94.828%  90.349% 95.178%   87.988% 97.198%\n",
            "     MLP Classifier      SMOTETomek  94.828%  90.349% 95.178%   87.988% 97.217%\n",
            "K-Nearest Neighbors   Original Data  94.828%  90.349% 95.178%   87.988% 96.536%\n",
            "K-Nearest Neighbors      SMOTETomek  94.828%  90.349% 95.178%   87.988% 97.368%\n",
            "K-Nearest Neighbors              MI  94.828%  90.349% 95.178%   87.988% 95.390%\n",
            "K-Nearest Neighbors             LDA  94.828%  90.349% 95.178%   87.988% 95.390%\n",
            "K-Nearest Neighbors          Boruta  94.828%  90.349% 95.178%   87.988% 97.135%\n",
            "K-Nearest Neighbors     Autoencoder  94.828%  90.349% 95.178%   87.988% 96.944%\n",
            "Logistic Regression              MI  93.103%  91.316% 93.217%   90.769% 95.526%\n",
            "        Extra Trees             LDA  93.103%  91.316% 93.217%   90.769% 94.987%\n",
            "     MLP Classifier             LDA  93.103%  91.316% 93.217%   90.769% 95.536%\n",
            "        Extra Trees              MI  93.103%  89.448% 94.277%   87.087% 96.288%\n",
            "           AdaBoost             LDA  93.103%  88.933% 94.277%   85.880% 93.840%\n",
            "      Random Forest              MI  93.103%  87.456% 94.277%   84.788% 96.294%\n",
            "           AdaBoost     Autoencoder  93.103%  87.456% 94.277%   84.788% 95.056%\n",
            "  Gradient Boosting             LDA  93.103%  87.456% 94.277%   84.788% 95.574%\n",
            "           AdaBoost      SMOTETomek  91.379%  90.092% 93.376%   87.560% 97.599%\n",
            "      Random Forest             LDA  91.379%  84.954% 93.376%   82.381% 95.106%\n",
            "Logistic Regression      SMOTETomek  89.655%  85.101% 92.475%   81.726% 95.248%\n",
            "      XGBoost Model      SMOTETomek  89.655%  82.749% 92.475%   80.501% 96.753%\n",
            "Logistic Regression             LDA  86.207%  84.321% 85.374%   87.907% 95.322%\n",
            "     MLP Classifier   Original Data  63.793%  25.965% 33.333%   21.264% 50.000%\n",
            "\n",
            "====================================================================================================\n",
            "TOP CONFIGURATION PER MODEL\n",
            "====================================================================================================\n",
            "           ML Model   Configuration Accuracy F1 Score  Recall Precision ROC_AUC\n",
            "           AdaBoost Normalized Data  96.552%  94.385% 97.138%   92.432% 97.496%\n",
            "        Extra Trees          Boruta  96.552%  94.385% 97.138%   92.432% 97.205%\n",
            "  Gradient Boosting   Original Data  96.552%  94.385% 97.138%   92.432% 97.261%\n",
            "K-Nearest Neighbors Normalized Data  94.828%  92.880% 95.178%   91.579% 98.174%\n",
            "Logistic Regression   Original Data  94.828%  92.880% 95.178%   91.579% 94.328%\n",
            "     MLP Classifier          Boruta  96.552%  94.385% 97.138%   92.432% 98.770%\n",
            "      Random Forest Normalized Data  96.552%  94.385% 97.138%   92.432% 96.515%\n",
            "      XGBoost Model Normalized Data  96.552%  94.385% 97.138%   92.432% 97.319%\n"
          ]
        }
      ]
    }
  ]
}