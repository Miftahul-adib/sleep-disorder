{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTPn2tKYYU6pdOdeRP6zE3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Importing Libraries\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Yb1fHaSMPI85"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "diEVyiMoJ4Cm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, RobustScaler\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler\n",
        "from imblearn.under_sampling import CondensedNearestNeighbour, TomekLinks, RandomUnderSampler\n",
        "from boruta import BorutaPy\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Load & Preprocessing"
      ],
      "metadata": {
        "id": "unb2qil8SoTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/Sleep_health_and_lifestyle_dataset.csv\")\n",
        "df.fillna(\"None\", inplace=True)\n",
        "df[['Systolic BP', 'Diastolic BP']] = df['Blood Pressure'].str.split('/', expand=True).astype(int)\n",
        "df.drop(['Person ID', 'Blood Pressure'], axis=1, inplace=True)\n",
        "df = pd.get_dummies(df, columns=['Occupation', 'BMI Category'], drop_first=False)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df['Gender'] = label_encoder.fit_transform(df['Gender'])\n",
        "\n",
        "X = df.drop('Sleep Disorder', axis=1)\n",
        "y = df['Sleep Disorder']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n"
      ],
      "metadata": {
        "id": "R6yAAGUeSsn3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Model Result Storage"
      ],
      "metadata": {
        "id": "TpEDoK5Ihy4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ML_Model = []\n",
        "ML_Config = []\n",
        "accuracy = []\n",
        "f1_score = []\n",
        "recall = []\n",
        "precision = []\n",
        "auc_roc = []  # Adding a holder for AUC-ROC\n",
        "\n",
        "# Function to call for storing the results\n",
        "def storeResults(model, config, a, b, c, d, e):\n",
        "    \"\"\"\n",
        "    Store model performance results\n",
        "\n",
        "    Parameters:\n",
        "    model: Name of the ML model\n",
        "    config: Configuration name (preprocessing steps applied)\n",
        "    a: Accuracy score\n",
        "    b: F1 score\n",
        "    c: Recall score\n",
        "    d: Precision score\n",
        "    e: AUC-ROC score\n",
        "    \"\"\"\n",
        "    ML_Model.append(model)\n",
        "    ML_Config.append(config)\n",
        "    accuracy.append(round(a, 6))\n",
        "    f1_score.append(round(b, 6))\n",
        "    recall.append(round(c, 6))\n",
        "    precision.append(round(d, 6))\n",
        "    auc_roc.append(round(e, 6))"
      ],
      "metadata": {
        "id": "tlV5BgjKhyYX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest with K-Fold, Oversampling, Undersampling, Randomsampling"
      ],
      "metadata": {
        "id": "SHcHbkJxdLvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "configurations = []\n",
        "configurations.append(('Original Data', X_train, X_test, y_train))\n",
        "\n",
        "# Step 2: Normalize the data\n",
        "scaler = MinMaxScaler()\n",
        "X_train_normalized = scaler.fit_transform(X_train)\n",
        "X_test_normalized = scaler.transform(X_test)\n",
        "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
        "\n",
        "# RandomForest classifier\n",
        "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Applying Boruta Feature Selection\n",
        "boruta_selector = BorutaPy(rfc, n_estimators='auto', verbose=0, random_state=42)\n",
        "X_train_boruta = boruta_selector.fit_transform(X_train_normalized, y_train)\n",
        "X_test_boruta = boruta_selector.transform(X_test_normalized)\n",
        "configurations.append(('Boruta', X_train_boruta, X_test_boruta, y_train))\n",
        "\n",
        "selected_features = boruta_selector.support_\n",
        "optimal_features = sum(selected_features)\n",
        "print(f\"Optimal number of features to select using Boruta: {optimal_features}\")\n",
        "\n",
        "# applying Autoencoder\n",
        "n_features = X_train_boruta.shape[1]\n",
        "input_layer = Input(shape=(n_features,))\n",
        "encoded = Dense(32, activation='relu')(input_layer)\n",
        "bottleneck = Dense(16, activation='relu')(encoded)\n",
        "decoded = Dense(32, activation='relu')(encoded)\n",
        "decoded = Dense(n_features, activation='sigmoid')(encoded)\n",
        "\n",
        "autoencoder = Model(input_layer, decoded)\n",
        "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "autoencoder.fit(X_train_boruta, X_train_boruta, epochs=10, batch_size=32, verbose=0)\n",
        "encoder = Model(input_layer, bottleneck)\n",
        "X_train_encoded = encoder.predict(X_train_boruta)\n",
        "X_test_encoded = encoder.predict(X_test_boruta)\n",
        "configurations.append(('Autoencoder', X_train_encoded, X_test_encoded, y_train))\n",
        "\n",
        "# applying oversampling Smote & ADASYN\n",
        "smote = SMOTE(random_state=42)\n",
        "adasyn = ADASYN(random_state=42)\n",
        "X_train_resample_smote, y_train_resample_smote = smote.fit_resample(X_train_normalized, y_train)\n",
        "X_train_resample_adasyn, y_train_resample_adasyn = adasyn.fit_resample(X_train_normalized, y_train)\n",
        "configurations.append(('SMOTE', X_train_resample_smote, X_test_normalized, y_train_resample_smote))\n",
        "configurations.append(('ADASYN', X_train_resample_adasyn, X_test_normalized, y_train_resample_adasyn))\n",
        "\n",
        "# applying undersampling CNN & Tomek Links\n",
        "cnn = CondensedNearestNeighbour(random_state=42)\n",
        "tomek = TomekLinks()\n",
        "X_train_resample_cnn, y_train_resample_cnn = cnn.fit_resample(X_train_normalized, y_train)\n",
        "X_train_resample_tomek, y_train_resample_tomek = tomek.fit_resample(X_train_normalized, y_train)\n",
        "configurations.append(('CondensedNN', X_train_resample_cnn, X_test_normalized, y_train_resample_cnn))\n",
        "configurations.append(('Tomek Links', X_train_resample_tomek, X_test_normalized, y_train_resample_tomek))\n",
        "\n",
        "# applying randomsampling Randomoversampling & Randomundersampling\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_train_resample_ros, y_train_resample_ros = ros.fit_resample(X_train_normalized, y_train)\n",
        "X_train_resample_rus, y_train_resample_rus = rus.fit_resample(X_train_normalized, y_train)\n",
        "configurations.append(('Random Oversampling', X_train_resample_ros, X_test_normalized, y_train_resample_ros))\n",
        "configurations.append(('Random Undersampling', X_train_resample_rus, X_test_normalized, y_train_resample_rus))\n",
        "\n",
        "# Step 4: Random Forest + GridSearchCV\n",
        "print(\"\\n=== Random Forest Model Performance with Hyperparameter Tuning ===\")\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [120],\n",
        "    'max_depth': [20],\n",
        "    'min_samples_split': [2],\n",
        "    'min_samples_leaf': [1],\n",
        "    'max_features': ['sqrt'],\n",
        "    'bootstrap': [True],\n",
        "    'criterion': ['entropy']\n",
        "}\n",
        "\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning Random Forest with {name} configuration...\")\n",
        "    rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=10, n_jobs=-1, verbose=2)\n",
        "    rf.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_rf = rf.predict(X_train_cfg)\n",
        "    y_test_rf = rf.predict(X_test_cfg)\n",
        "    y_train_rf_proba = rf.predict_proba(X_train_cfg)\n",
        "    y_test_rf_proba = rf.predict_proba(X_test_cfg)\n",
        "\n",
        "    metrics_dict = {\n",
        "          \"Dataset\": [\"Training\", \"Test\"],\n",
        "          \"Accuracy\": [\n",
        "              metrics.accuracy_score(y_train_cfg, y_train_rf),\n",
        "              metrics.accuracy_score(y_test, y_test_rf),\n",
        "          ],\n",
        "          \"F1 Score\": [\n",
        "              metrics.f1_score(y_train_cfg, y_train_rf, average='macro'),\n",
        "              metrics.f1_score(y_test, y_test_rf, average='macro'),\n",
        "          ],\n",
        "          \"Recall\": [\n",
        "              metrics.recall_score(y_train_cfg, y_train_rf, average='macro'),\n",
        "              metrics.recall_score(y_test, y_test_rf, average='macro'),\n",
        "          ],\n",
        "          \"Precision\": [\n",
        "              metrics.precision_score(y_train_cfg, y_train_rf, average='macro'),\n",
        "              metrics.precision_score(y_test, y_test_rf, average='macro'),\n",
        "          ],\n",
        "          \"AUC-ROC\": [\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_rf_proba, multi_class='ovr', average='macro'),\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_test), y_test_rf_proba, multi_class='ovr', average='macro'),\n",
        "          ]\n",
        "      }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nRandom Forest Model Performance Metrics\")\n",
        "    print(\"Configuration Name: \", name)\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_rf_proba, multi_class='ovr', average='macro')\n",
        "    storeResults(\n",
        "          'Random Forest',\n",
        "          name,\n",
        "          metrics.accuracy_score(y_test, y_test_rf),\n",
        "          metrics.f1_score(y_test, y_test_rf, average='macro'),\n",
        "          metrics.recall_score(y_test, y_test_rf, average='macro'),\n",
        "          metrics.precision_score(y_test, y_test_rf, average='macro'),\n",
        "          auc_score\n",
        "      )\n",
        "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
        "    print(rf.best_params_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7NTek4OUXv5",
        "outputId": "232db945-b547-4beb-8ed4-42e243d628d5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal number of features to select using Boruta: 10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\n",
            "=== Random Forest Model Performance with Hyperparameter Tuning ===\n",
            "\n",
            "Running Random Forest with Original Data configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "Random Forest Model Performance Metrics\n",
            "Configuration Name:  Original Data\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.932143  0.923808 0.920645   0.927169 0.990293\n",
            "    Test  0.893617  0.843243 0.844207   0.851058 0.931928\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'bootstrap': True, 'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 120}\n",
            "\n",
            "Running Random Forest with Normalized Data configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "Random Forest Model Performance Metrics\n",
            "Configuration Name:  Normalized Data\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.932143  0.923808 0.920645   0.927169 0.990293\n",
            "    Test  0.893617  0.843243 0.844207   0.851058 0.931773\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'bootstrap': True, 'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 120}\n",
            "\n",
            "Running Random Forest with Boruta configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "Random Forest Model Performance Metrics\n",
            "Configuration Name:  Boruta\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.932143  0.923808 0.920645   0.927169 0.990098\n",
            "    Test  0.893617  0.843243 0.844207   0.851058 0.932706\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'bootstrap': True, 'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 120}\n",
            "\n",
            "Running Random Forest with Autoencoder configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "Random Forest Model Performance Metrics\n",
            "Configuration Name:  Autoencoder\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.932143  0.923808 0.920645   0.927169 0.989809\n",
            "    Test  0.893617  0.851917 0.844207   0.865900 0.928169\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'bootstrap': True, 'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 120}\n",
            "\n",
            "Running Random Forest with SMOTE configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "Random Forest Model Performance Metrics\n",
            "Configuration Name:  SMOTE\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.961382  0.961429 0.961382   0.961621 0.995185\n",
            "    Test  0.893617  0.843243 0.844207   0.851058 0.931431\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'bootstrap': True, 'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 120}\n",
            "\n",
            "Running Random Forest with ADASYN configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "Random Forest Model Performance Metrics\n",
            "Configuration Name:  ADASYN\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.959184  0.959246 0.959085   0.959517 0.994810\n",
            "    Test  0.893617  0.843243 0.844207   0.851058 0.920455\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'bootstrap': True, 'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 120}\n",
            "\n",
            "Running Random Forest with CondensedNN configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "Random Forest Model Performance Metrics\n",
            "Configuration Name:  CondensedNN\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training  0.890000  0.875327 0.85119   0.911937 0.975129\n",
            "    Test  0.861702  0.801201 0.79893   0.811772 0.883450\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'bootstrap': True, 'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 120}\n",
            "\n",
            "Running Random Forest with Tomek Links configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "Random Forest Model Performance Metrics\n",
            "Configuration Name:  Tomek Links\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.931900  0.923321 0.920080   0.926766 0.989960\n",
            "    Test  0.893617  0.843243 0.844207   0.851058 0.932156\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'bootstrap': True, 'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 120}\n",
            "\n",
            "Running Random Forest with Random Oversampling configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "Random Forest Model Performance Metrics\n",
            "Configuration Name:  Random Oversampling\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.934959  0.934635 0.934959   0.934878 0.992787\n",
            "    Test  0.808511  0.768019 0.795722   0.760079 0.922159\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'bootstrap': True, 'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 120}\n",
            "\n",
            "Running Random Forest with Random Undersampling configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "Random Forest Model Performance Metrics\n",
            "Configuration Name:  Random Undersampling\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.934524  0.934953 0.934524    0.93664 0.994898\n",
            "    Test  0.861702  0.814815 0.826025    0.81995 0.914786\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'bootstrap': True, 'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 120}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree with K-Fold, Oversampling, Undersampling, Randomsampling"
      ],
      "metadata": {
        "id": "X_F-w3vDsLHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "configurations = []\n",
        "configurations.append(('Original Data', X_train, X_test, y_train))\n",
        "\n",
        "# Step 2: Normalize the data\n",
        "scaler = MinMaxScaler()\n",
        "X_train_normalized = scaler.fit_transform(X_train)\n",
        "X_test_normalized = scaler.transform(X_test)\n",
        "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
        "\n",
        "# # DecisionTree classifier\n",
        "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Applying Boruta Feature Selection\n",
        "boruta_selector = BorutaPy(rfc, n_estimators='auto', verbose=0, random_state=42)\n",
        "X_train_boruta = boruta_selector.fit_transform(X_train_normalized, y_train)\n",
        "X_test_boruta = boruta_selector.transform(X_test_normalized)\n",
        "configurations.append(('Boruta', X_train_boruta, X_test_boruta, y_train))\n",
        "\n",
        "selected_features = boruta_selector.support_\n",
        "optimal_features = sum(selected_features)\n",
        "print(f\"Optimal number of features to select using Boruta: {optimal_features}\")\n",
        "\n",
        "# applying oversampling Smote & ADASYN\n",
        "smote = SMOTE(random_state=42)\n",
        "adasyn = ADASYN(random_state=42)\n",
        "X_train_resample_smote, y_train_resample_smote = smote.fit_resample(X_train_normalized, y_train)\n",
        "X_train_resample_adasyn, y_train_resample_adasyn = adasyn.fit_resample(X_train_normalized, y_train)\n",
        "configurations.append(('SMOTE', X_train_resample_smote, X_test_normalized, y_train_resample_smote))\n",
        "configurations.append(('ADASYN', X_train_resample_adasyn, X_test_normalized, y_train_resample_adasyn))\n",
        "\n",
        "# applying undersampling CNN & Tomek Links\n",
        "cnn = CondensedNearestNeighbour(random_state=42)\n",
        "tomek = TomekLinks()\n",
        "X_train_resample_cnn, y_train_resample_cnn = cnn.fit_resample(X_train_normalized, y_train)\n",
        "X_train_resample_tomek, y_train_resample_tomek = tomek.fit_resample(X_train_normalized, y_train)\n",
        "configurations.append(('CondensedNN', X_train_resample_cnn, X_test_normalized, y_train_resample_cnn))\n",
        "configurations.append(('Tomek Links', X_train_resample_tomek, X_test_normalized, y_train_resample_tomek))\n",
        "\n",
        "# applying randomsampling Randomoversampling & Randomundersampling\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_train_resample_ros, y_train_resample_ros = ros.fit_resample(X_train_normalized, y_train)\n",
        "X_train_resample_rus, y_train_resample_rus = rus.fit_resample(X_train_normalized, y_train)\n",
        "configurations.append(('Random Oversampling', X_train_resample_ros, X_test_normalized, y_train_resample_ros))\n",
        "configurations.append(('Random Undersampling', X_train_resample_rus, X_test_normalized, y_train_resample_rus))\n",
        "\n",
        "# Step 4: Decision Tree + GridSearchCV\n",
        "print(\"\\n=== Decision Tree Model Performance with Hyperparameter Tuning ===\")\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [20],\n",
        "    'min_samples_split': [2],\n",
        "    'min_samples_leaf': [3],\n",
        "    'max_features': ['sqrt'],\n",
        "    'criterion': ['entropy']\n",
        "}\n",
        "\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning Decision Tree with {name} configuration...\")\n",
        "    dt = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=10, n_jobs=-1, verbose=2)\n",
        "    dt.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_dt = dt.predict(X_train_cfg)\n",
        "    y_test_dt = dt.predict(X_test_cfg)\n",
        "    y_train_dt_proba = dt.predict_proba(X_train_cfg)\n",
        "    y_test_dt_proba = dt.predict_proba(X_test_cfg)\n",
        "\n",
        "    metrics_dict = {\n",
        "          \"Dataset\": [\"Training\", \"Test\"],\n",
        "          \"Accuracy\": [\n",
        "              metrics.accuracy_score(y_train_cfg, y_train_dt),\n",
        "              metrics.accuracy_score(y_test, y_test_dt),\n",
        "          ],\n",
        "          \"F1 Score\": [\n",
        "              metrics.f1_score(y_train_cfg, y_train_dt, average='macro'),\n",
        "              metrics.f1_score(y_test, y_test_dt, average='macro'),\n",
        "          ],\n",
        "          \"Recall\": [\n",
        "              metrics.recall_score(y_train_cfg, y_train_dt, average='macro'),\n",
        "              metrics.recall_score(y_test, y_test_dt, average='macro'),\n",
        "          ],\n",
        "          \"Precision\": [\n",
        "              metrics.precision_score(y_train_cfg, y_train_dt, average='macro'),\n",
        "              metrics.precision_score(y_test, y_test_dt, average='macro'),\n",
        "          ],\n",
        "          \"AUC-ROC\": [\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_dt_proba, multi_class='ovr', average='macro'),\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_test), y_test_dt_proba, multi_class='ovr', average='macro'),\n",
        "          ]\n",
        "      }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nDeicion Tree Model Performance Metrics\")\n",
        "    print(\"Configuration Name: \", name)\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_dt_proba, multi_class='ovr', average='macro')\n",
        "    storeResults(\n",
        "        'Decision Tree',\n",
        "        name,\n",
        "        metrics.accuracy_score(y_test, y_test_dt),\n",
        "        metrics.f1_score(y_test, y_test_dt, average='macro'),\n",
        "        metrics.recall_score(y_test, y_test_dt, average='macro'),\n",
        "        metrics.precision_score(y_test, y_test_dt, average='macro'),\n",
        "        auc_score\n",
        "    )\n",
        "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
        "    print(dt.best_params_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gFupPmNsedu",
        "outputId": "77aeefe3-3926-44a4-da5a-979517520c2b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal number of features to select using Boruta: 10\n",
            "\n",
            "=== Decision Tree Model Performance with Hyperparameter Tuning ===\n",
            "\n",
            "Running Decision Tree with Original Data configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "Deicion Tree Model Performance Metrics\n",
            "Configuration Name:  Original Data\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.917857  0.902743 0.900755   0.905341 0.984165\n",
            "    Test  0.893617  0.852012 0.848663   0.872845 0.890642\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 2}\n",
            "\n",
            "Running Decision Tree with Normalized Data configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "Deicion Tree Model Performance Metrics\n",
            "Configuration Name:  Normalized Data\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.917857  0.902743 0.900755   0.905341 0.984165\n",
            "    Test  0.893617  0.852012 0.848663   0.872845 0.890642\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 2}\n",
            "\n",
            "Running Decision Tree with Boruta configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "Deicion Tree Model Performance Metrics\n",
            "Configuration Name:  Boruta\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.917857  0.900505 0.897232   0.903994 0.976897\n",
            "    Test  0.914894  0.877928 0.878966   0.886243 0.940308\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 2}\n",
            "\n",
            "Running Decision Tree with SMOTE configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "Deicion Tree Model Performance Metrics\n",
            "Configuration Name:  SMOTE\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.928862  0.929005 0.928862   0.930160 0.987879\n",
            "    Test  0.893617  0.843243 0.844207   0.851058 0.902484\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 2}\n",
            "\n",
            "Running Decision Tree with ADASYN configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "Deicion Tree Model Performance Metrics\n",
            "Configuration Name:  ADASYN\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.936735  0.936864 0.936490   0.937932 0.991729\n",
            "    Test  0.882979  0.834033 0.842602   0.842602 0.892351\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 2}\n",
            "\n",
            "Running Decision Tree with CondensedNN configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "Deicion Tree Model Performance Metrics\n",
            "Configuration Name:  CondensedNN\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.840000  0.785733 0.768849   0.812407 0.947782\n",
            "    Test  0.882979  0.834370 0.842602   0.851103 0.864650\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 2}\n",
            "\n",
            "Running Decision Tree with Tomek Links configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "Deicion Tree Model Performance Metrics\n",
            "Configuration Name:  Tomek Links\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.921147  0.907981 0.906748   0.909806 0.984583\n",
            "    Test  0.893617  0.851917 0.844207   0.865900 0.891429\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 2}\n",
            "\n",
            "Running Decision Tree with Random Oversampling configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "Deicion Tree Model Performance Metrics\n",
            "Configuration Name:  Random Oversampling\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.920732  0.920710 0.920732   0.920769 0.988734\n",
            "    Test  0.861702  0.821911 0.830481   0.830481 0.891618\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 2}\n",
            "\n",
            "Running Decision Tree with Random Undersampling configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "Deicion Tree Model Performance Metrics\n",
            "Configuration Name:  Random Undersampling\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.916667  0.916951 0.916667   0.919014 0.986660\n",
            "    Test  0.861702  0.826049 0.839572   0.836524 0.877771\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Boosting with K-Fold, Oversampling, Undersampling, Randomsampling"
      ],
      "metadata": {
        "id": "RQWUpTh_uU8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "configurations = []\n",
        "configurations.append(('Original Data', X_train, X_test, y_train))\n",
        "\n",
        "# Step 2: Normalize the data\n",
        "scaler = MinMaxScaler()\n",
        "X_train_normalized = scaler.fit_transform(X_train)\n",
        "X_test_normalized = scaler.transform(X_test)\n",
        "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
        "\n",
        "# GradientBoosting classifier\n",
        "gb = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Applying Boruta Feature Selection\n",
        "boruta_selector = BorutaPy(gb, n_estimators='auto', verbose=0, random_state=42)\n",
        "X_train_boruta = boruta_selector.fit_transform(X_train_normalized, y_train)\n",
        "X_test_boruta = boruta_selector.transform(X_test_normalized)\n",
        "configurations.append(('Boruta', X_train_boruta, X_test_boruta, y_train))\n",
        "\n",
        "selected_features = boruta_selector.support_\n",
        "optimal_features = sum(selected_features)\n",
        "print(f\"Optimal number of features to select using Boruta: {optimal_features}\")\n",
        "\n",
        "# applying oversampling Smote & ADASYN\n",
        "smote = SMOTE(random_state=42)\n",
        "adasyn = ADASYN(random_state=42)\n",
        "X_train_resample_smote, y_train_resample_smote = smote.fit_resample(X_train_normalized, y_train)\n",
        "X_train_resample_adasyn, y_train_resample_adasyn = adasyn.fit_resample(X_train_normalized, y_train)\n",
        "configurations.append(('SMOTE', X_train_resample_smote, X_test_normalized, y_train_resample_smote))\n",
        "configurations.append(('ADASYN', X_train_resample_adasyn, X_test_normalized, y_train_resample_adasyn))\n",
        "\n",
        "# applying undersampling CNN & Tomek Links\n",
        "cnn = CondensedNearestNeighbour(random_state=42)\n",
        "tomek = TomekLinks()\n",
        "X_train_resample_cnn, y_train_resample_cnn = cnn.fit_resample(X_train_normalized, y_train)\n",
        "X_train_resample_tomek, y_train_resample_tomek = tomek.fit_resample(X_train_normalized, y_train)\n",
        "configurations.append(('CondensedNN', X_train_resample_cnn, X_test_normalized, y_train_resample_cnn))\n",
        "configurations.append(('Tomek Links', X_train_resample_tomek, X_test_normalized, y_train_resample_tomek))\n",
        "\n",
        "# applying randomsampling Randomoversampling & Randomundersampling\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_train_resample_ros, y_train_resample_ros = ros.fit_resample(X_train_normalized, y_train)\n",
        "X_train_resample_rus, y_train_resample_rus = rus.fit_resample(X_train_normalized, y_train)\n",
        "configurations.append(('Random Oversampling', X_train_resample_ros, X_test_normalized, y_train_resample_ros))\n",
        "configurations.append(('Random Undersampling', X_train_resample_rus, X_test_normalized, y_train_resample_rus))\n",
        "\n",
        "# Step 4: Gradient Boosting + GridSearchCV\n",
        "print(\"\\n=== Gradient Boosting Model Performance with Hyperparameter Tuning ===\")\n",
        "\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01],\n",
        "    'n_estimators': [150],\n",
        "    'max_depth': [20],\n",
        "    'min_samples_split': [2],\n",
        "    'min_samples_leaf': [1],\n",
        "    'max_features': ['sqrt'],\n",
        "    'subsample': [0.8]\n",
        "}\n",
        "\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning Gradient Boosting with {name} configuration...\")\n",
        "    gbc = GridSearchCV(GradientBoostingClassifier(random_state=42), param_grid, cv=10, n_jobs=-1, verbose=2)\n",
        "    gbc.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_gb = gbc.predict(X_train_cfg)\n",
        "    y_test_gb = gbc.predict(X_test_cfg)\n",
        "    y_train_gb_proba = gbc.predict_proba(X_train_cfg)\n",
        "    y_test_gb_proba = gbc.predict_proba(X_test_cfg)\n",
        "\n",
        "    metrics_dict = {\n",
        "          \"Dataset\": [\"Training\", \"Test\"],\n",
        "          \"Accuracy\": [\n",
        "              metrics.accuracy_score(y_train_cfg, y_train_gb),\n",
        "              metrics.accuracy_score(y_test, y_test_gb),\n",
        "          ],\n",
        "          \"F1 Score\": [\n",
        "              metrics.f1_score(y_train_cfg, y_train_gb, average='macro'),\n",
        "              metrics.f1_score(y_test, y_test_gb, average='macro'),\n",
        "          ],\n",
        "          \"Recall\": [\n",
        "              metrics.recall_score(y_train_cfg, y_train_gb, average='macro'),\n",
        "              metrics.recall_score(y_test, y_test_gb, average='macro'),\n",
        "          ],\n",
        "          \"Precision\": [\n",
        "              metrics.precision_score(y_train_cfg, y_train_gb, average='macro'),\n",
        "              metrics.precision_score(y_test, y_test_gb, average='macro'),\n",
        "          ],\n",
        "          \"AUC-ROC\": [\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_gb_proba, multi_class='ovr', average='macro'),\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_test), y_test_gb_proba, multi_class='ovr', average='macro'),\n",
        "          ]\n",
        "      }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nGradien Boosting Model Performance Metrics\")\n",
        "    print(\"Configuration Name: \", name)\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_gb_proba, multi_class='ovr', average='macro')\n",
        "    storeResults(\n",
        "          'Gradient Boosting',\n",
        "          name,\n",
        "          metrics.accuracy_score(y_test, y_test_gb),\n",
        "          metrics.f1_score(y_test, y_test_gb, average='macro'),\n",
        "          metrics.recall_score(y_test, y_test_gb, average='macro'),\n",
        "          metrics.precision_score(y_test, y_test_gb, average='macro'),\n",
        "          auc_score\n",
        "      )\n",
        "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
        "    print(gbc.best_params_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XoccNFtuYDy",
        "outputId": "aae9a6b2-e011-4171-9d3d-3a2dc621c661"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal number of features to select using Boruta: 6\n",
            "\n",
            "=== Gradient Boosting Model Performance with Hyperparameter Tuning ===\n",
            "\n",
            "Running Gradient Boosting with Original Data configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "Gradien Boosting Model Performance Metrics\n",
            "Configuration Name:  Original Data\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.932143  0.923808 0.920645   0.927169 0.990717\n",
            "    Test  0.893617  0.843243 0.844207   0.851058 0.940176\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'learning_rate': 0.01, 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 0.8}\n",
            "\n",
            "Running Gradient Boosting with Normalized Data configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "Gradien Boosting Model Performance Metrics\n",
            "Configuration Name:  Normalized Data\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.932143  0.923808 0.920645   0.927169 0.990717\n",
            "    Test  0.893617  0.843243 0.844207   0.851058 0.940785\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'learning_rate': 0.01, 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 0.8}\n",
            "\n",
            "Running Gradient Boosting with Boruta configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "Gradien Boosting Model Performance Metrics\n",
            "Configuration Name:  Boruta\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.928571  0.918031 0.915089   0.921440 0.972972\n",
            "    Test  0.893617  0.843243 0.844207   0.851058 0.910716\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'learning_rate': 0.01, 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 0.8}\n",
            "\n",
            "Running Gradient Boosting with SMOTE configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "Gradien Boosting Model Performance Metrics\n",
            "Configuration Name:  SMOTE\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.961382  0.961431 0.961382   0.961527 0.996455\n",
            "    Test  0.893617  0.843243 0.844207   0.851058 0.918870\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'learning_rate': 0.01, 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 0.8}\n",
            "\n",
            "Running Gradient Boosting with ADASYN configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "Gradien Boosting Model Performance Metrics\n",
            "Configuration Name:  ADASYN\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.959184  0.959246 0.959085   0.959517 0.996284\n",
            "    Test  0.893617  0.843243 0.844207   0.851058 0.925711\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'learning_rate': 0.01, 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 0.8}\n",
            "\n",
            "Running Gradient Boosting with CondensedNN configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "Gradien Boosting Model Performance Metrics\n",
            "Configuration Name:  CondensedNN\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training  0.890000  0.875327 0.85119   0.911937 0.978221\n",
            "    Test  0.861702  0.801201 0.79893   0.811772 0.868205\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'learning_rate': 0.01, 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 0.8}\n",
            "\n",
            "Running Gradient Boosting with Tomek Links configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "Gradien Boosting Model Performance Metrics\n",
            "Configuration Name:  Tomek Links\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.931900  0.923339 0.920382   0.927255 0.990627\n",
            "    Test  0.893617  0.843243 0.844207   0.851058 0.932524\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'learning_rate': 0.01, 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 0.8}\n",
            "\n",
            "Running Gradient Boosting with Random Oversampling configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "Gradien Boosting Model Performance Metrics\n",
            "Configuration Name:  Random Oversampling\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.934959  0.934825 0.934959   0.934796 0.992818\n",
            "    Test  0.851064  0.804811 0.819964   0.799301 0.923841\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'learning_rate': 0.01, 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 0.8}\n",
            "\n",
            "Running Gradient Boosting with Random Undersampling configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "Gradien Boosting Model Performance Metrics\n",
            "Configuration Name:  Random Undersampling\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.934524  0.934930 0.934524   0.938332 0.995057\n",
            "    Test  0.840426  0.795478 0.813904   0.789827 0.924129\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'learning_rate': 0.01, 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 0.8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra Trees with K-Fold, Oversampling, Undersampling, Randomsampling"
      ],
      "metadata": {
        "id": "EtjUT-NQw64A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "configurations = []\n",
        "configurations.append(('Original Data', X_train, X_test, y_train))\n",
        "\n",
        "# Step 2: Normalize the data\n",
        "scaler = MinMaxScaler()\n",
        "X_train_normalized = scaler.fit_transform(X_train)\n",
        "X_test_normalized = scaler.transform(X_test)\n",
        "\n",
        "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
        "\n",
        "# ExtraTrees classifier\n",
        "et = ExtraTreesClassifier(random_state=42)\n",
        "\n",
        "# Applying Boruta Feature Selection\n",
        "boruta_selector = BorutaPy(et, n_estimators='auto', verbose=0, random_state=42)\n",
        "X_train_boruta = boruta_selector.fit_transform(X_train_normalized, y_train)\n",
        "X_test_boruta = boruta_selector.transform(X_test_normalized)\n",
        "configurations.append(('Boruta', X_train_boruta, X_test_boruta, y_train))\n",
        "\n",
        "selected_features = boruta_selector.support_\n",
        "optimal_features = sum(selected_features)\n",
        "print(f\"Optimal number of features to select using Boruta: {optimal_features}\")\n",
        "\n",
        "# applying oversampling Smote & ADASYN\n",
        "smote = SMOTE(random_state=42)\n",
        "adasyn = ADASYN(random_state=42)\n",
        "X_train_resample_smote, y_train_resample_smote = smote.fit_resample(X_train_normalized, y_train)\n",
        "X_train_resample_adasyn, y_train_resample_adasyn = adasyn.fit_resample(X_train_normalized, y_train)\n",
        "configurations.append(('SMOTE', X_train_resample_smote, X_test_normalized, y_train_resample_smote))\n",
        "configurations.append(('ADASYN', X_train_resample_adasyn, X_test_normalized, y_train_resample_adasyn))\n",
        "\n",
        "# applying undersampling CNN & Tomek Links\n",
        "cnn = CondensedNearestNeighbour(random_state=42)\n",
        "tomek = TomekLinks()\n",
        "X_train_resample_cnn, y_train_resample_cnn = cnn.fit_resample(X_train_normalized, y_train)\n",
        "X_train_resample_tomek, y_train_resample_tomek = tomek.fit_resample(X_train_normalized, y_train)\n",
        "configurations.append(('CondensedNN', X_train_resample_cnn, X_test_normalized, y_train_resample_cnn))\n",
        "configurations.append(('Tomek Links', X_train_resample_tomek, X_test_normalized, y_train_resample_tomek))\n",
        "\n",
        "# applying randomsampling Randomoversampling & Randomundersampling\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_train_resample_ros, y_train_resample_ros = ros.fit_resample(X_train_normalized, y_train)\n",
        "X_train_resample_rus, y_train_resample_rus = rus.fit_resample(X_train_normalized, y_train)\n",
        "configurations.append(('Random Oversampling', X_train_resample_ros, X_test_normalized, y_train_resample_ros))\n",
        "configurations.append(('Random Undersampling', X_train_resample_rus, X_test_normalized, y_train_resample_rus))\n",
        "\n",
        "# Step 4: Extra Trees + GridSearchCV\n",
        "print(\"\\n=== Extra Trees Model Performance with Hyperparameter Tuning ===\")\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100],\n",
        "    'max_depth': [20],\n",
        "    'min_samples_split': [2],\n",
        "    'min_samples_leaf': [1],\n",
        "    'max_features': ['sqrt'],\n",
        "    'bootstrap': [True],\n",
        "    'criterion': ['entropy']\n",
        "}\n",
        "\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning Extra Trees with {name} configuration...\")\n",
        "    etc = GridSearchCV(ExtraTreesClassifier(random_state=42), param_grid, cv=10, n_jobs=-1, verbose=2)\n",
        "    etc.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_et = etc.predict(X_train_cfg)\n",
        "    y_test_et = etc.predict(X_test_cfg)\n",
        "    y_train_et_proba = etc.predict_proba(X_train_cfg)\n",
        "    y_test_et_proba = etc.predict_proba(X_test_cfg)\n",
        "\n",
        "    metrics_dict = {\n",
        "          \"Dataset\": [\"Training\", \"Test\"],\n",
        "          \"Accuracy\": [\n",
        "              metrics.accuracy_score(y_train_cfg, y_train_et),\n",
        "              metrics.accuracy_score(y_test, y_test_et),\n",
        "          ],\n",
        "          \"F1 Score\": [\n",
        "              metrics.f1_score(y_train_cfg, y_train_et, average='macro'),\n",
        "              metrics.f1_score(y_test, y_test_et, average='macro'),\n",
        "          ],\n",
        "          \"Recall\": [\n",
        "              metrics.recall_score(y_train_cfg, y_train_et, average='macro'),\n",
        "              metrics.recall_score(y_test, y_test_et, average='macro'),\n",
        "          ],\n",
        "          \"Precision\": [\n",
        "              metrics.precision_score(y_train_cfg, y_train_et, average='macro'),\n",
        "              metrics.precision_score(y_test, y_test_et, average='macro'),\n",
        "          ],\n",
        "          \"AUC-ROC\": [\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_et_proba, multi_class='ovr', average='macro'),\n",
        "              metrics.roc_auc_score(pd.get_dummies(y_test), y_test_et_proba, multi_class='ovr', average='macro'),\n",
        "          ]\n",
        "      }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nExtraTrees Model Performance Metrics\")\n",
        "    print(\"Configuration Name: \", name)\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_et_proba, multi_class='ovr', average='macro')\n",
        "    storeResults(\n",
        "          'Extra Trees',\n",
        "          name,\n",
        "          metrics.accuracy_score(y_test, y_test_et),\n",
        "          metrics.f1_score(y_test, y_test_et, average='macro'),\n",
        "          metrics.recall_score(y_test, y_test_et, average='macro'),\n",
        "          metrics.precision_score(y_test, y_test_et, average='macro'),\n",
        "          auc_score\n",
        "      )\n",
        "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
        "    print(etc.best_params_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01gypP_5xEKE",
        "outputId": "1a45bf4c-52f6-4afb-d7ce-62702f8290b3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal number of features to select using Boruta: 14\n",
            "\n",
            "=== Extra Trees Model Performance with Hyperparameter Tuning ===\n",
            "\n",
            "Running Extra Trees with Original Data configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "ExtraTrees Model Performance Metrics\n",
            "Configuration Name:  Original Data\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.932143  0.923843 0.921041   0.927580 0.990362\n",
            "    Test  0.893617  0.843243 0.844207   0.851058 0.931017\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'bootstrap': True, 'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "\n",
            "Running Extra Trees with Normalized Data configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "ExtraTrees Model Performance Metrics\n",
            "Configuration Name:  Normalized Data\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.932143  0.923843 0.921041   0.927580 0.990362\n",
            "    Test  0.893617  0.843243 0.844207   0.851058 0.931017\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'bootstrap': True, 'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "\n",
            "Running Extra Trees with Boruta configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "ExtraTrees Model Performance Metrics\n",
            "Configuration Name:  Boruta\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.932143  0.923843 0.921041   0.927580 0.990372\n",
            "    Test  0.893617  0.843243 0.844207   0.851058 0.929544\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'bootstrap': True, 'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "\n",
            "Running Extra Trees with SMOTE configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "ExtraTrees Model Performance Metrics\n",
            "Configuration Name:  SMOTE\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.961382  0.961429 0.961382   0.961621  0.99508\n",
            "    Test  0.904255  0.860006 0.859358   0.865288  0.92983\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'bootstrap': True, 'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "\n",
            "Running Extra Trees with ADASYN configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "ExtraTrees Model Performance Metrics\n",
            "Configuration Name:  ADASYN\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.959184  0.959231 0.958984   0.959716 0.994704\n",
            "    Test  0.893617  0.843243 0.844207   0.851058 0.924974\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'bootstrap': True, 'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "\n",
            "Running Extra Trees with CondensedNN configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "ExtraTrees Model Performance Metrics\n",
            "Configuration Name:  CondensedNN\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.890000  0.878990 0.873016   0.886631 0.974652\n",
            "    Test  0.851064  0.792344 0.792870   0.801924 0.896418\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'bootstrap': True, 'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "\n",
            "Running Extra Trees with Tomek Links configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "ExtraTrees Model Performance Metrics\n",
            "Configuration Name:  Tomek Links\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.931900  0.923321 0.920080   0.926766 0.989905\n",
            "    Test  0.893617  0.843243 0.844207   0.851058 0.931222\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'bootstrap': True, 'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "\n",
            "Running Extra Trees with Random Oversampling configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "ExtraTrees Model Performance Metrics\n",
            "Configuration Name:  Random Oversampling\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.934959  0.934635 0.934959   0.934878 0.992775\n",
            "    Test  0.808511  0.768019 0.795722   0.760079 0.920512\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'bootstrap': True, 'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "\n",
            "Running Extra Trees with Random Undersampling configuration...\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "ExtraTrees Model Performance Metrics\n",
            "Configuration Name:  Random Undersampling\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.934524  0.935039 0.934524   0.937719 0.994951\n",
            "    Test  0.861702  0.812066 0.826025   0.817664 0.918337\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'bootstrap': True, 'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Result\n"
      ],
      "metadata": {
        "id": "DZjM6oTkrnlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the dataframe\n",
        "result = pd.DataFrame({\n",
        "    'ML Model': ML_Model,\n",
        "    'Configuration': ML_Config,\n",
        "    'Accuracy': [f\"{acc * 100:.3f}%\" for acc in accuracy],\n",
        "    'F1 Score': [f\"{f1 * 100:.3f}%\" for f1 in f1_score],\n",
        "    'Recall': [f\"{rec * 100:.3f}%\" for rec in recall],\n",
        "    'Precision': [f\"{prec * 100:.3f}%\" for prec in precision],\n",
        "    'ROC_AUC': [f\"{roc * 100:.3f}%\" for roc in auc_roc],\n",
        "})\n",
        "\n",
        "# Remove duplicates based on model and configuration\n",
        "result.drop_duplicates(subset=[\"ML Model\", \"Configuration\"], inplace=True)\n",
        "\n",
        "# Display the result\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"MODEL PERFORMANCE RESULTS\")\n",
        "print(\"=\" * 100)\n",
        "print(result.to_string(index=False))\n",
        "\n",
        "# Save the result to a CSV file\n",
        "# result.to_csv('final_results/model_results.csv', index=False)\n",
        "# print(\"\\nResults saved to model_results.csv\")\n",
        "\n",
        "# Sort by Accuracy and F1 Score\n",
        "sorted_result = result.sort_values(by=['Accuracy', 'F1 Score'], ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Display the sorted result\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"SORTED MODEL PERFORMANCE RESULTS (by Accuracy and F1 Score)\")\n",
        "print(\"=\" * 100)\n",
        "print(sorted_result.to_string(index=False))\n",
        "\n",
        "# Save the sorted result\n",
        "# sorted_result.to_csv('final_results/sorted_model_results.csv', index=False)\n",
        "# print(\"\\nSorted results saved to sorted_model_results.csv\")\n",
        "\n",
        "# Extract top configuration per ML model\n",
        "top_per_model = sorted_result.groupby('ML Model', as_index=False).first()\n",
        "\n",
        "# Display and save the top configuration table\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"TOP CONFIGURATION PER MODEL\")\n",
        "print(\"=\" * 100)\n",
        "print(top_per_model.to_string(index=False))\n",
        "\n",
        "# top_per_model.to_csv('final_results/top_configurations.csv', index=False)\n",
        "# print(\"\\nTop configuration per model saved to top_configurations.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jgo6Ddxwr281",
        "outputId": "08e7cdeb-ccaa-47f2-c5a0-ace2dcb29036"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================================================================================================\n",
            "MODEL PERFORMANCE RESULTS\n",
            "====================================================================================================\n",
            "         ML Model        Configuration Accuracy F1 Score  Recall Precision ROC_AUC\n",
            "    Random Forest        Original Data  89.362%  84.324% 84.421%   85.106% 93.193%\n",
            "    Random Forest      Normalized Data  89.362%  84.324% 84.421%   85.106% 93.177%\n",
            "    Random Forest               Boruta  89.362%  84.324% 84.421%   85.106% 93.271%\n",
            "    Random Forest                SMOTE  89.362%  84.324% 84.421%   85.106% 93.143%\n",
            "    Random Forest               ADASYN  89.362%  84.324% 84.421%   85.106% 92.046%\n",
            "    Random Forest          CondensedNN  86.170%  80.120% 79.893%   81.177% 88.345%\n",
            "    Random Forest          Tomek Links  89.362%  84.324% 84.421%   85.106% 93.216%\n",
            "    Random Forest  Random Oversampling  80.851%  76.802% 79.572%   76.008% 92.216%\n",
            "    Random Forest Random Undersampling  86.170%  81.481% 82.603%   81.995% 91.479%\n",
            "    Decision Tree        Original Data  89.362%  85.201% 84.866%   87.284% 89.064%\n",
            "    Decision Tree      Normalized Data  89.362%  85.201% 84.866%   87.284% 89.064%\n",
            "    Decision Tree               Boruta  91.489%  87.793% 87.897%   88.624% 94.031%\n",
            "    Decision Tree                SMOTE  89.362%  84.324% 84.421%   85.106% 90.248%\n",
            "    Decision Tree               ADASYN  88.298%  83.403% 84.260%   84.260% 89.235%\n",
            "    Decision Tree          CondensedNN  88.298%  83.437% 84.260%   85.110% 86.465%\n",
            "    Decision Tree          Tomek Links  89.362%  85.192% 84.421%   86.590% 89.143%\n",
            "    Decision Tree  Random Oversampling  86.170%  82.191% 83.048%   83.048% 89.162%\n",
            "    Decision Tree Random Undersampling  86.170%  82.605% 83.957%   83.652% 87.777%\n",
            "Gradient Boosting        Original Data  89.362%  84.324% 84.421%   85.106% 94.018%\n",
            "Gradient Boosting      Normalized Data  89.362%  84.324% 84.421%   85.106% 94.078%\n",
            "Gradient Boosting               Boruta  89.362%  84.324% 84.421%   85.106% 91.072%\n",
            "Gradient Boosting                SMOTE  89.362%  84.324% 84.421%   85.106% 91.887%\n",
            "Gradient Boosting               ADASYN  89.362%  84.324% 84.421%   85.106% 92.571%\n",
            "Gradient Boosting          CondensedNN  86.170%  80.120% 79.893%   81.177% 86.820%\n",
            "Gradient Boosting          Tomek Links  89.362%  84.324% 84.421%   85.106% 93.252%\n",
            "Gradient Boosting  Random Oversampling  85.106%  80.481% 81.996%   79.930% 92.384%\n",
            "Gradient Boosting Random Undersampling  84.043%  79.548% 81.390%   78.983% 92.413%\n",
            "      Extra Trees        Original Data  89.362%  84.324% 84.421%   85.106% 93.102%\n",
            "      Extra Trees      Normalized Data  89.362%  84.324% 84.421%   85.106% 93.102%\n",
            "      Extra Trees               Boruta  89.362%  84.324% 84.421%   85.106% 92.954%\n",
            "      Extra Trees                SMOTE  90.425%  86.001% 85.936%   86.529% 92.983%\n",
            "      Extra Trees               ADASYN  89.362%  84.324% 84.421%   85.106% 92.497%\n",
            "      Extra Trees          CondensedNN  85.106%  79.234% 79.287%   80.192% 89.642%\n",
            "      Extra Trees          Tomek Links  89.362%  84.324% 84.421%   85.106% 93.122%\n",
            "      Extra Trees  Random Oversampling  80.851%  76.802% 79.572%   76.008% 92.051%\n",
            "      Extra Trees Random Undersampling  86.170%  81.207% 82.603%   81.766% 91.834%\n",
            "\n",
            "====================================================================================================\n",
            "SORTED MODEL PERFORMANCE RESULTS (by Accuracy and F1 Score)\n",
            "====================================================================================================\n",
            "         ML Model        Configuration Accuracy F1 Score  Recall Precision ROC_AUC\n",
            "    Decision Tree               Boruta  91.489%  87.793% 87.897%   88.624% 94.031%\n",
            "      Extra Trees                SMOTE  90.425%  86.001% 85.936%   86.529% 92.983%\n",
            "    Decision Tree        Original Data  89.362%  85.201% 84.866%   87.284% 89.064%\n",
            "    Decision Tree      Normalized Data  89.362%  85.201% 84.866%   87.284% 89.064%\n",
            "    Decision Tree          Tomek Links  89.362%  85.192% 84.421%   86.590% 89.143%\n",
            "    Random Forest        Original Data  89.362%  84.324% 84.421%   85.106% 93.193%\n",
            "    Random Forest      Normalized Data  89.362%  84.324% 84.421%   85.106% 93.177%\n",
            "    Random Forest               Boruta  89.362%  84.324% 84.421%   85.106% 93.271%\n",
            "    Random Forest                SMOTE  89.362%  84.324% 84.421%   85.106% 93.143%\n",
            "    Random Forest               ADASYN  89.362%  84.324% 84.421%   85.106% 92.046%\n",
            "    Random Forest          Tomek Links  89.362%  84.324% 84.421%   85.106% 93.216%\n",
            "    Decision Tree                SMOTE  89.362%  84.324% 84.421%   85.106% 90.248%\n",
            "Gradient Boosting        Original Data  89.362%  84.324% 84.421%   85.106% 94.018%\n",
            "Gradient Boosting      Normalized Data  89.362%  84.324% 84.421%   85.106% 94.078%\n",
            "Gradient Boosting               Boruta  89.362%  84.324% 84.421%   85.106% 91.072%\n",
            "Gradient Boosting                SMOTE  89.362%  84.324% 84.421%   85.106% 91.887%\n",
            "Gradient Boosting               ADASYN  89.362%  84.324% 84.421%   85.106% 92.571%\n",
            "Gradient Boosting          Tomek Links  89.362%  84.324% 84.421%   85.106% 93.252%\n",
            "      Extra Trees        Original Data  89.362%  84.324% 84.421%   85.106% 93.102%\n",
            "      Extra Trees      Normalized Data  89.362%  84.324% 84.421%   85.106% 93.102%\n",
            "      Extra Trees               Boruta  89.362%  84.324% 84.421%   85.106% 92.954%\n",
            "      Extra Trees               ADASYN  89.362%  84.324% 84.421%   85.106% 92.497%\n",
            "      Extra Trees          Tomek Links  89.362%  84.324% 84.421%   85.106% 93.122%\n",
            "    Decision Tree          CondensedNN  88.298%  83.437% 84.260%   85.110% 86.465%\n",
            "    Decision Tree               ADASYN  88.298%  83.403% 84.260%   84.260% 89.235%\n",
            "    Decision Tree Random Undersampling  86.170%  82.605% 83.957%   83.652% 87.777%\n",
            "    Decision Tree  Random Oversampling  86.170%  82.191% 83.048%   83.048% 89.162%\n",
            "    Random Forest Random Undersampling  86.170%  81.481% 82.603%   81.995% 91.479%\n",
            "      Extra Trees Random Undersampling  86.170%  81.207% 82.603%   81.766% 91.834%\n",
            "    Random Forest          CondensedNN  86.170%  80.120% 79.893%   81.177% 88.345%\n",
            "Gradient Boosting          CondensedNN  86.170%  80.120% 79.893%   81.177% 86.820%\n",
            "Gradient Boosting  Random Oversampling  85.106%  80.481% 81.996%   79.930% 92.384%\n",
            "      Extra Trees          CondensedNN  85.106%  79.234% 79.287%   80.192% 89.642%\n",
            "Gradient Boosting Random Undersampling  84.043%  79.548% 81.390%   78.983% 92.413%\n",
            "    Random Forest  Random Oversampling  80.851%  76.802% 79.572%   76.008% 92.216%\n",
            "      Extra Trees  Random Oversampling  80.851%  76.802% 79.572%   76.008% 92.051%\n",
            "\n",
            "====================================================================================================\n",
            "TOP CONFIGURATION PER MODEL\n",
            "====================================================================================================\n",
            "         ML Model Configuration Accuracy F1 Score  Recall Precision ROC_AUC\n",
            "    Decision Tree        Boruta  91.489%  87.793% 87.897%   88.624% 94.031%\n",
            "      Extra Trees         SMOTE  90.425%  86.001% 85.936%   86.529% 92.983%\n",
            "Gradient Boosting Original Data  89.362%  84.324% 84.421%   85.106% 94.018%\n",
            "    Random Forest Original Data  89.362%  84.324% 84.421%   85.106% 93.193%\n"
          ]
        }
      ]
    }
  ]
}